{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction to GME The Gravity Modeling Environment (GME) package is a collection of tools written in Python to be used for gravity trade analysis. The package consists of tools to aid in the fast, flexible, and robust estimation of gravity models using modern, best practices. Future updates to the package will include tools for the simulation of general equilibrium gravity models and the preparation of data for estimation. The GME package offers several distinct advantages over alternative software choices for conducting gravity analysis. First, the package is written in Python, a flexible, powerful, and free programming language that can be readily used on a wide variety of computers with no cost. Second, unlike more general statistical software, which must cater to a broad number of needs, the GME package has been specifically designed to perform gravity analysis well. Third, because the tools are implemented in Python, users have access to an enormous and growing collection of third-party tools to incorporate into and expand their work. Visit the USITC gravity portal for more information about the Gravity Modeling Environment, which includes the GME package and a collection of data. Package Overview In the current release, the GME package consists of three key components: the EstimationData object, the EstimationModel object, and the estimate() function. The EstimationData object houses the data to be used for estimation as well as information about the data and a set of tools to facilitate descriptive analysis of the data. The EstimationModel object is used to set up the specification for the estimation of the model and store the eventual results and diagnostic information. Finally, the function estimate() runs a Poisson Pseudo-Maximum Likelihood (PPML) estimation according to the specification established in the EstimationModel. Latest Version: 1.3 To see what is new in version 1.3, see v1.3 updates","title":"Introduction"},{"location":"#introduction-to-gme","text":"The Gravity Modeling Environment (GME) package is a collection of tools written in Python to be used for gravity trade analysis. The package consists of tools to aid in the fast, flexible, and robust estimation of gravity models using modern, best practices. Future updates to the package will include tools for the simulation of general equilibrium gravity models and the preparation of data for estimation. The GME package offers several distinct advantages over alternative software choices for conducting gravity analysis. First, the package is written in Python, a flexible, powerful, and free programming language that can be readily used on a wide variety of computers with no cost. Second, unlike more general statistical software, which must cater to a broad number of needs, the GME package has been specifically designed to perform gravity analysis well. Third, because the tools are implemented in Python, users have access to an enormous and growing collection of third-party tools to incorporate into and expand their work. Visit the USITC gravity portal for more information about the Gravity Modeling Environment, which includes the GME package and a collection of data.","title":"Introduction to GME"},{"location":"#package-overview","text":"In the current release, the GME package consists of three key components: the EstimationData object, the EstimationModel object, and the estimate() function. The EstimationData object houses the data to be used for estimation as well as information about the data and a set of tools to facilitate descriptive analysis of the data. The EstimationModel object is used to set up the specification for the estimation of the model and store the eventual results and diagnostic information. Finally, the function estimate() runs a Poisson Pseudo-Maximum Likelihood (PPML) estimation according to the specification established in the EstimationModel.","title":"Package Overview"},{"location":"#latest-version-13","text":"To see what is new in version 1.3, see v1.3 updates","title":"Latest Version: 1.3"},{"location":"about/","text":"About the GME package The GME package was created by the Gravity Modeling Group at the United States International Trade Commission. The package has been primarily developed by Peter Herman and Saad Ahmad. Additional contributions have been made by Serge Shikher, Tamara Gurevich, Samantha Schreiber, Carlos Payan, Caroline Peters, Grace Kenneally, and Austin Drenski. Contact us at gravity@usitc.gov Additional information about the development team can be located at the project website at gravity.usitc.gov . Release information GME is free, open-source software Version: 1.3 Date: 07/09/2020","title":"About"},{"location":"about/#about-the-gme-package","text":"The GME package was created by the Gravity Modeling Group at the United States International Trade Commission. The package has been primarily developed by Peter Herman and Saad Ahmad. Additional contributions have been made by Serge Shikher, Tamara Gurevich, Samantha Schreiber, Carlos Payan, Caroline Peters, Grace Kenneally, and Austin Drenski. Contact us at gravity@usitc.gov Additional information about the development team can be located at the project website at gravity.usitc.gov .","title":"About the GME package"},{"location":"about/#release-information","text":"GME is free, open-source software Version: 1.3 Date: 07/09/2020","title":"Release information"},{"location":"estimate_technical/","text":"MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \"AMS\" } } }); Gravity Estimation Methodology Theory-based gravity equation The GME module estimates the structural gravity equation \\eqref{eq:gravity} using the Poisson Pseudo Maximum Likelihood (PPML) estimator, a special case of the the Generalized Linear Model (GLM) framework. \\begin{equation} X_{ij} =\\frac{Y_i E_j}{Y}\\left(\\frac{t_{ij}}{S_i P_j}\\right)^{1-\\sigma}. \\label{eq:gravity} \\end{equation} \\begin{equation} X_{ij} =\\frac{Y_i E_j}{Y}\\left(\\frac{t_{ij}}{S_i P_j}\\right)^{1-\\sigma}. \\label{eq:gravity} \\end{equation} In this equation, X_{ij} X_{ij} are the value of exports from country i i to country j j , Y_{i} Y_{i} is country i i 's domestic production, E_{j} E_{j} is country j j 's aggregate expenditure, Y Y is global production, t_{ij} t_{ij} is the bilateral cost of trading between country i i and country j j , and $\\sigma$ is the elasticity of substitution among goods from varying source countries. The structural terms S_{i} S_{i} and P_{j} P_{j} represent multilateral resistance within the exporting and importing countries (i.e the ease of market access for country i i 's export, and the ease of market access for country j j 's imports, respectively). When applying equation \\eqref{eq:gravity} to data using PPML, a variety of fixed effects are possible. One theory-consistent and flexible empirical form of the gravity equation when using panel data is \\begin{equation} X_{ijt} =\\exp \\left[\\gamma_{it}+\\eta_{jt}+\\lambda_{ij}+\\beta Z_{ijt}\\right]+\\varepsilon_{ijt} \\label{eq:PPML_gravity_panel} \\end{equation} \\begin{equation} X_{ijt} =\\exp \\left[\\gamma_{it}+\\eta_{jt}+\\lambda_{ij}+\\beta Z_{ijt}\\right]+\\varepsilon_{ijt} \\label{eq:PPML_gravity_panel} \\end{equation} In equation \\eqref{eq:PPML_gravity_panel}, \\gamma_{it} \\gamma_{it} are exporter time-varying fixed effect, \\eta_{jt} \\eta_{jt} are importer time-varying fixed effects, \\lambda_{ij} \\lambda_{ij} are exporter-importer time-invariant fixed effects, and Z_{ijt} Z_{ijt} is the vector of time-variant bilateral determinants of trade, such as tariff levels. If only cross-section data is available, a theory-consistent empirical form of the gravity equation could be \\begin{equation} X_{ij} =\\exp \\left[\\gamma_{i}+\\eta_{j}+\\beta Z_{ij}\\right]+\\varepsilon_{ij} \\label{eq:PPML_gravity_cross} \\end{equation} \\begin{equation} X_{ij} =\\exp \\left[\\gamma_{i}+\\eta_{j}+\\beta Z_{ij}\\right]+\\varepsilon_{ij} \\label{eq:PPML_gravity_cross} \\end{equation} In this case, \\gamma_{it} \\gamma_{it} are exporter fixed effect, \\eta_{jt} \\eta_{jt} are importer fixed effects, and Z_{ij} Z_{ij} is the vector of bilateral determinants of trade, such as distance. Estimation Procedure The method estimate performs a sector-by-sector GLM estimation based on a Poisson distribution with data diagnostics that help increase the likelihood of convergence. See estimate . If sector_by_sector is specified, the routine is repeated for each sector individually, estimating a separate model each time. The estimate routine inherits all specifications from those supplied to the EstimationModel . Technical Details of PPML Implementation We implement PPML following Santos Silva and Tenreyro (2006) . The PPML assumes that the variance is proportional to the mean so that the only condition required for PPML to be consistent is the correct specification of the conditional mean. The PPML also gives the same weight to each observation in the estimation and so is desirable when there is not much available information on the nature of heteroscedasticity in the trade data. Santos Silva and Tenreyro (2006) provide simulation evidence that the PPML is well behaved in a wide range of situations and can deal with certain types of measurement error in the dependent variable. The PPML, being a non-linear estimator, is also able to handle zero trade flows in the estimation. Common problems with PPML estimation include the non-existence of estimates due to perfect collinearity and numerical difficulties in running the algorithm. Non-existence of estimates Santos Silva and Tenreyro (2010) show that PPML estimates may not exist if there is perfect collinearity for the subsample with positive observations of the dependent variable (common in trade data where some countries do not trade in certain years or sectors and so are perfectly collinear when X_{ij}>0 X_{ij}>0 ). In this scenario, either the estimation algorithm will fail to converge or convergence is spurious and characterized by a \u201cperfect\u201d fit for observations where X_{ij}=0 X_{ij}=0 . To check for non-existence, Santos Silva and Tenreyro (2011) use a short STATA code that first identifies and drops the problematic regressors before estimating it with PPML. Their code also issues a warning that the model is overfitting due to spurious convergence. We implement their STATA code in Python in order to obtain the same procedures for identifying and dropping problematic variables, testing for perfect collinearity and checking if the X_{ij}=0 X_{ij}=0 observations are perfectly predicted by the estimated model. All these diagnostics are stored as PPML diagnostics and available to the user after every GME estimation. Non-convergence Santos Silva and Tenreyro (2011) also find that sensitivity to numerical problems can prevent estimation algorithms from locating the maximum and finding PPML estimates that converge. In particular, these numerical issues arise when there are collinear regressors that have different magnitudes or regressors that are extremely, but not perfectly collinear. They recommend using the iterated, re-weighted least squares (IRLS) as the optimization algorithm to deal with such numerical complications, which is also the default method for GLM estimation in the statsmodels package used by GME. Thus, the PPML estimator in GME is robust to numerical problems arising from different data configurations.","title":"Methodology"},{"location":"estimate_technical/#gravity-estimation-methodology","text":"","title":"Gravity Estimation Methodology"},{"location":"estimate_technical/#theory-based-gravity-equation","text":"The GME module estimates the structural gravity equation \\eqref{eq:gravity} using the Poisson Pseudo Maximum Likelihood (PPML) estimator, a special case of the the Generalized Linear Model (GLM) framework. \\begin{equation} X_{ij} =\\frac{Y_i E_j}{Y}\\left(\\frac{t_{ij}}{S_i P_j}\\right)^{1-\\sigma}. \\label{eq:gravity} \\end{equation} \\begin{equation} X_{ij} =\\frac{Y_i E_j}{Y}\\left(\\frac{t_{ij}}{S_i P_j}\\right)^{1-\\sigma}. \\label{eq:gravity} \\end{equation} In this equation, X_{ij} X_{ij} are the value of exports from country i i to country j j , Y_{i} Y_{i} is country i i 's domestic production, E_{j} E_{j} is country j j 's aggregate expenditure, Y Y is global production, t_{ij} t_{ij} is the bilateral cost of trading between country i i and country j j , and $\\sigma$ is the elasticity of substitution among goods from varying source countries. The structural terms S_{i} S_{i} and P_{j} P_{j} represent multilateral resistance within the exporting and importing countries (i.e the ease of market access for country i i 's export, and the ease of market access for country j j 's imports, respectively). When applying equation \\eqref{eq:gravity} to data using PPML, a variety of fixed effects are possible. One theory-consistent and flexible empirical form of the gravity equation when using panel data is \\begin{equation} X_{ijt} =\\exp \\left[\\gamma_{it}+\\eta_{jt}+\\lambda_{ij}+\\beta Z_{ijt}\\right]+\\varepsilon_{ijt} \\label{eq:PPML_gravity_panel} \\end{equation} \\begin{equation} X_{ijt} =\\exp \\left[\\gamma_{it}+\\eta_{jt}+\\lambda_{ij}+\\beta Z_{ijt}\\right]+\\varepsilon_{ijt} \\label{eq:PPML_gravity_panel} \\end{equation} In equation \\eqref{eq:PPML_gravity_panel}, \\gamma_{it} \\gamma_{it} are exporter time-varying fixed effect, \\eta_{jt} \\eta_{jt} are importer time-varying fixed effects, \\lambda_{ij} \\lambda_{ij} are exporter-importer time-invariant fixed effects, and Z_{ijt} Z_{ijt} is the vector of time-variant bilateral determinants of trade, such as tariff levels. If only cross-section data is available, a theory-consistent empirical form of the gravity equation could be \\begin{equation} X_{ij} =\\exp \\left[\\gamma_{i}+\\eta_{j}+\\beta Z_{ij}\\right]+\\varepsilon_{ij} \\label{eq:PPML_gravity_cross} \\end{equation} \\begin{equation} X_{ij} =\\exp \\left[\\gamma_{i}+\\eta_{j}+\\beta Z_{ij}\\right]+\\varepsilon_{ij} \\label{eq:PPML_gravity_cross} \\end{equation} In this case, \\gamma_{it} \\gamma_{it} are exporter fixed effect, \\eta_{jt} \\eta_{jt} are importer fixed effects, and Z_{ij} Z_{ij} is the vector of bilateral determinants of trade, such as distance.","title":"Theory-based gravity equation"},{"location":"estimate_technical/#estimation-procedure","text":"The method estimate performs a sector-by-sector GLM estimation based on a Poisson distribution with data diagnostics that help increase the likelihood of convergence. See estimate . If sector_by_sector is specified, the routine is repeated for each sector individually, estimating a separate model each time. The estimate routine inherits all specifications from those supplied to the EstimationModel .","title":"Estimation Procedure"},{"location":"estimate_technical/#technical-details-of-ppml-implementation","text":"We implement PPML following Santos Silva and Tenreyro (2006) . The PPML assumes that the variance is proportional to the mean so that the only condition required for PPML to be consistent is the correct specification of the conditional mean. The PPML also gives the same weight to each observation in the estimation and so is desirable when there is not much available information on the nature of heteroscedasticity in the trade data. Santos Silva and Tenreyro (2006) provide simulation evidence that the PPML is well behaved in a wide range of situations and can deal with certain types of measurement error in the dependent variable. The PPML, being a non-linear estimator, is also able to handle zero trade flows in the estimation. Common problems with PPML estimation include the non-existence of estimates due to perfect collinearity and numerical difficulties in running the algorithm.","title":"Technical Details of PPML Implementation"},{"location":"estimate_technical/#non-existence-of-estimates","text":"Santos Silva and Tenreyro (2010) show that PPML estimates may not exist if there is perfect collinearity for the subsample with positive observations of the dependent variable (common in trade data where some countries do not trade in certain years or sectors and so are perfectly collinear when X_{ij}>0 X_{ij}>0 ). In this scenario, either the estimation algorithm will fail to converge or convergence is spurious and characterized by a \u201cperfect\u201d fit for observations where X_{ij}=0 X_{ij}=0 . To check for non-existence, Santos Silva and Tenreyro (2011) use a short STATA code that first identifies and drops the problematic regressors before estimating it with PPML. Their code also issues a warning that the model is overfitting due to spurious convergence. We implement their STATA code in Python in order to obtain the same procedures for identifying and dropping problematic variables, testing for perfect collinearity and checking if the X_{ij}=0 X_{ij}=0 observations are perfectly predicted by the estimated model. All these diagnostics are stored as PPML diagnostics and available to the user after every GME estimation.","title":"Non-existence of estimates"},{"location":"estimate_technical/#non-convergence","text":"Santos Silva and Tenreyro (2011) also find that sensitivity to numerical problems can prevent estimation algorithms from locating the maximum and finding PPML estimates that converge. In particular, these numerical issues arise when there are collinear regressors that have different magnitudes or regressors that are extremely, but not perfectly collinear. They recommend using the iterated, re-weighted least squares (IRLS) as the optimization algorithm to deal with such numerical complications, which is also the default method for GLM estimation in the statsmodels package used by GME. Thus, the PPML estimator in GME is robust to numerical problems arising from different data configurations.","title":"Non-convergence"},{"location":"estimation_tutorial/","text":"Estimating a Gravity Model This tutorial demonstrates a basic gravity analysis including loading data, constructing some summary statistics, estimating a model, and outputting the results in several possible formats. For more information, see the list of commands and API reference in this documentation. Load Data The gme package uses a special object, called gme.EstimationData to store data. EstimationData includes a Pandas.DataFrame containing data (trade + gravity + etc.) for estimation as well as additional information about that data and methods that can be used to summarize and/or manipulate the data. This tutorial will demonstrate some of these features. First, we must begin by creating a gme.EstimationData. Doing so requires the inputting of a Pandas.DataFrame and several pieces of \"meta-data\" that describe the data. Start by loading a dataset using the read_csv() function from pandas. In the sample code below, we will read a dataset directly from the internet, but you could just as easily read the same file from your hard drive. >> > from src import gme as gme >> > import pandas as pd >> > gravity_data = pd . read_csv ( 'https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv' ) >> > gravity_data . head () importer exporter year trade_value agree_pta common_language 0 AUS BRA 1989 3.035469e+08 0.0 1.0 1 AUS CAN 1989 8.769946e+08 0.0 1.0 2 AUS CHE 1989 4.005245e+08 0.0 1.0 3 AUS DEU 1989 2.468977e+09 0.0 0.0 4 AUS DNK 1989 1.763072e+08 0.0 1.0 contiguity log_distance 0 0.0 9.553332 1 0.0 9.637676 2 0.0 9.687557 3 0.0 9.675007 4 0.0 9.657311 # Next, we use the loaded data to create an EstimationData instance called gme_data >> > gme_data = gme . EstimationData ( data_frame = gravity_data , imp_var_name = 'importer' , exp_var_name = 'exporter' , trade_var_name = 'trade_value' , year_var_name = 'year' , notes = 'Downloaded from https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv' ) In creating an instance of the EstimationData object, the user is asked to supply a Pandas.DataFrame, which we a loaded in the previous lines, and several types of descriptive arguments. These arguments ( imp_var_name , exp_var_name , trade_var_name , and year_var_name ) specify the columns in the supplied DataFrame corresponding to particular types of information that will likely be present in any gravity analysis (the column containing the importer ID, exporter ID, trade values, and year, respectively). These \"meta-data\" fields can be useful as they prevent users from having to re-enter these same basic characteristics of the data at later points and permit the automatic construction of certain types of summary information. Finally, an optional note is supplied to the EstimationData. The EstimationData object has a attribute that stores a list user-supplied strings for later reference. In this case, we have supplied a note indicating from where the data originated. Working with EstimationData In addition to providing an object class that communicates conveniently with the gme.EstimationModel (see below), the EstimationData provides a collection of data summary and manipulation tools. For example, simply calling (or printing) the object, returns a summary of the scope of the data: >>> gme_data number of countries : 62 number of exporters : 62 number of importers : 62 number of years : 27 number of sectors : not_applicable dimensions : ( 98612 , 8 ) As can be seen from the console return, the dataset we are using covers 62 importers and exporters, 27 years, and contains 98,612 rows and 8 columns. Because this particular dataset does not have multiple sectors, that field is marked as 'not applicable'. 1 Other summary information can be reported in the following ways: # Return the number of importers in the dataset. >>> gme_data . number_of_importers 62 # Return a list of the column names >>> gme_data . columns [ 'importer' , 'exporter' , 'year' , 'trade_value' , 'agree_pta' , 'common_language' , 'contiguity' , 'log_distance' ] # Return a list of years in the dataset >>> gme_data . year_list () [ 1989 , 1990 , 1991 , 1992 , 1993 , 1994 , 1995 , 1996 , 1997 , 1998 , 1999 , 2000 , 2001 , 2002 , 2003 , 2004 , 2005 , 2006 , 2007 , 2008 , 2009 , 2010 , 2011 , 2012 , 2013 , 2014 , 2015 ] # Return a dictionary containing a list of countries in the dataset for each year. >>> country_list = gme_data . countries_each_year () >>> country_list [ 1989 ] [ 'IRN' , 'BOL' , 'TUR' , 'ARG' , 'CHL' , 'HUN' , 'KEN' , 'VEN' , 'ZAF' , 'URY' , 'BRA' , 'DZA' , 'PER' , 'IRL' , 'DNK' , 'GHA' , 'KOR' , 'PAK' , 'COL' , 'IND' , 'ISL' , 'ISR' , 'ESP' , 'ITA' , 'NLD' , 'NGA' , 'AUS' , 'SWE' , 'PRY' , 'GBR' , 'IDN' , 'HKG' , 'NOR' , 'TUN' , 'EGY' , 'KWT' , 'DEU' , 'CHE' , 'MYS' , 'NZL' , 'LBY' , 'USA' , 'SDN' , 'CHN' , 'GRC' , 'MEX' , 'CAN' , 'PRT' , 'SAU' , 'POL' , 'PHL' , 'THA' , 'FRA' , 'JPN' , 'MAR' , 'AUT' , 'FIN' , 'SGP' , 'ECU' ] # Additionally, many of the descriptive methods from Pandas.DataFrames have been inherited: >>> gme_data . dtypes () importer object exporter object year int64 trade_value float64 agree_pta float64 common_language float64 contiguity float64 log_distance float64 dtype : object >>> gme_data . info () < class ' pandas . core . frame . DataFrame '> RangeIndex : 98612 entries , 0 to 98611 Data columns ( total 8 columns ): importer 98612 non - null object exporter 98612 non - null object year 98612 non - null int64 trade_value 98612 non - null float64 agree_pta 97676 non - null float64 common_language 97676 non - null float64 contiguity 97676 non - null float64 log_distance 97676 non - null float64 dtypes : float64 ( 5 ), int64 ( 1 ), object ( 2 ) memory usage : 6.0 + MB >>> gme_data . describe () year trade_value agree_pta common_language \\ count 98612.000000 9.861200e+04 97676.000000 97676.000000 mean 2002.210441 1.856316e+09 0.381547 0.380646 std 7.713050 1.004735e+10 0.485769 0.485548 min 1989.000000 0.000000e+00 0.000000 0.000000 25 % 1996.000000 1.084703e+06 0.000000 0.000000 50 % 2002.000000 6.597395e+07 0.000000 0.000000 75 % 2009.000000 6.125036e+08 1.000000 1.000000 max 2015.000000 4.977686e+11 1.000000 1.000000 contiguity log_distance count 97676.000000 97676.000000 mean 0.034051 8.722631 std 0.181362 0.818818 min 0.000000 5.061335 25 % 0.000000 8.222970 50 % 0.000000 9.012502 75 % 0.000000 9.303026 max 1.000000 9.890765 Additionally, the EstimationData object retains the full ability to work with the supplied DataFrame. The DataFrame can be easily accessed by referring to its attribute in EstimationData. # Return the column of trade_values >>> gme_data . data_frame [ 'trade_value' ] 0 3.035469e+08 1 8.769946e+08 2 4.005245e+08 ... 98609 0.000000e+00 98610 0.000000e+00 98611 0.000000e+00 Name : trade_value , Length : 98612 , dtype : float64 Finally, the EstimationData object features a tool for easy aggregation and custom summary information. Additionally, because the method used for this process returns a DataFrame, the aggregated information can itself be used for many other applications, including the creation of a new EstimationData object. # Calculate mean, minimum, and maximum trade values and distances for each importer. >>> aggregated_data_1 = gme_data . tabulate_by_group ( tba_variables = [ 'trade_value' , 'log_distance' ], by_group = [ 'importer' ], how = [ 'mean' , 'min' , 'max' ]) >>> aggregated_data_1 . head ( 5 ) importer_ trade_value_mean trade_value_min trade_value_max \\ 0 ARG 4.579184e+08 0.0 2.218091e+10 1 AUS 1.704595e+09 0.0 4.620843e+10 2 AUT 1.241109e+09 0.0 6.973252e+10 3 BEL 4.783322e+09 0.0 9.963373e+10 4 BOL 5.506187e+07 0.0 1.810665e+09 log_distance_mean log_distance_min log_distance_max 0 9.137979 6.377581 9.856877 1 9.421134 7.926203 9.774024 2 8.231444 5.723756 9.800177 3 8.223088 5.061335 9.824095 4 9.090685 7.150738 9.874070 # Calculate mean, standard deviation, and count of trade for each importer/exporter pair. >>> aggregated_data_2 = gme_data . tabulate_by_group ( tab_variables = [ 'trade_value' ], by_group = [ 'importer' , 'exporter' ], how = [ 'mean' , 'std' , 'count' ]) >>> aggregated_data_2 . head ( 5 ) importer_ exporter_ trade_value_mean trade_value_std trade_value_count 0 ARG AUS 1.205231e+08 1.056954e+08 27 1 ARG AUT 9.942091e+07 8.018963e+07 27 2 ARG BEL 2.972734e+08 2.129833e+08 17 3 ARG BOL 3.378161e+08 6.339966e+08 27 4 ARG BRA 8.209756e+09 6.723047e+09 27 Note Knowing when to end a command with ( ) : When first learning python, it can be confusing trying to determine when a command applied to an object should be followed by parentheses. In the preceding code example, you will see instances of both: gme_data.columns and gme_data.year_list( ) , for example. Knowing which format to use is largely a matter of becoming familiar with the functions you are using. However, there is a logic to it. Each time a command is applied to an object (i.e. using the syntax object.command ), you are calling either an attribute of the object or a method on the object. An attribute is a piece of information that has already been created and included in the object whereas a method is effectively a function that can be run on the object. A method will be called with two parentheses because they will often accept additional arguments. For example, this is the case with the DataFrame method head( ) , which can accept a user-provided number of rows. However, you will often find that you do not need to supply additional arguments to a method, in which case you leave the parentheses empty. An attribute, by comparison, does not feature ( ) because there is no need or ability to provide additional input because the contents of that attribute have already been computed. As mentioned before, knowing whether a command is an attribute or a method, however, simply requires familiarity with the object. Creating and estimating a model Once a EstimationData has been created, estimating a gravity model using the data is fairly straightforward. There are two basic step for estimation: (1) define a model and (2) estimate the model. Defining the model amounts to creating another object called EstimationModel . Like the EstimationData, EstimationModel is meant to standardize and simplify the steps typically taken to specify and estimate a gravity model. While the EstimationData is meant to be an object that is created once for each study, many EstimationModels will likely be defined and redefined as you test different specifications. Thus, the arguments and attributes of the EstimationModel reflect the different types of modifications you may want to make as you select your preferred specification. As with the EstimationData, the EstimationModel is largely a dataset, with added information that define the characteristics of the particular model. The following examples depict several model specifications, each demonstrating different types of model aspects that can be specified. # A simple case in which 'trade_value' is dependent on 'log_distance', 'agree_pta', etc. >>> model_baseline = gme . EstimationModel ( estimation_data = gme_data , lhs_var = 'trade_value' , rhs_var = [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' ]) # A specification that will generate and include importer-year and exporter-year # fixed effects >>> fixed_effects_model = gme . EstimationModel ( estimation_data = gme_data , lhs_var = 'trade_value' , rhs_var = [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' ], fixed_effects = [[ 'importer' , 'year' ], [ 'exporter' , 'year' ]]) # A specification that uses a subset of the data. The United States ('USA') will be omitted # and only the years 2013--2015 will be included. >>> data_subset_model = gme . EstimationModel ( estimation_data = gme_data , lhs_var = 'trade_value' , rhs_var = [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' ], drop_imp_exp = [ 'USA' ], keep_years = [ 2015 , 2014 , 2013 ]) When specifying a model, there are several key types of attributes that can be included: Model Variables : The variables to be included are specified using the arguments lhs_vars and rhs_vars , which denote the left-hand-side dependent variable and right-hand-side independent variables, respectively. Fixed Effects : The model, at the point at which it is estimated, will construct fixed effects if any are specified by fixed_effects . These can be either single variables (e.g. ['importer']), or interacted variables (e.g. [['importer', 'year']]). For example, entering [ 'importer', ['exporter', 'year']] would yield a set of importer fixed effects and a set of exporter-year fixed effects. Data Subsets : Subsets of the data to use for estimation can be specified in a variety of ways. The arguments keep_years and drop_years can be used to select only a subset of years to include. Similarly the keep_imp , keep_exp , and keep_imp_exp arguments, and their corresponding drop_... options can do the same for importers and/or exporters. Once a model has been defined, running a PPML estimation according to the supplied specification is quite straightforward. It only requires the application of a single method of the EstimationModel: .estimate() . No further inputs are required. # Define a new, fixed effects model using only a subset of years (to reduce the computation time) >>> fixed_effects_model_2 = gme . EstimationModel ( estimation_data = gme_data , lhs_var = 'trade_value' , rhs_var = [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' ], fixed_effects = [[ 'importer' , 'year' ], [ 'exporter' , 'year' ]], keep_years = [ 2013 , 2014 , 2015 ]) # Conduct a PPML estimation of the fixed effects model. estimates = fixed_effects_model_2 . estimate () select specification variables : [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' , 'trade_value' , 'importer' , 'exporter' , 'year' ], Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } drop_intratrade : no , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } drop_imp : none , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } drop_exp : none , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } keep_imp : all available , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } keep_exp : all available , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } drop_years : none , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } keep_years : [ 2013 , 2014 , 2015 ], Observations excluded by user : { 'rows' : 87632 , 'columns' : 0 } drop_missing : yes , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } Estimation began at 08 : 58 AM on Jun 19 , 2018 Estimation completed at 08 : 58 AM on Jun 19 , 2018 The results ( estimates ) are stored as a dictionary with each entry in the dictionary corresponding to a single estimation. 2 The storing of the results in this way is primarily to facilitate the sector_by_sector , which separately estimates a model for each product/industry/sector, and returns a set of estimation results for each. In the case in which multiple sectors are not considered, as in the examples considered here, the results dictionary contains a single entry with the key 'all'. Viewing, formatting, and outputting the results The first step to viewing and working with the regression estimates is unpacking them from the dictionary in which they have been stored. A dictionary is an object in which each item stored in the dictionary is associated with a key. That key can be used to return its associated item. In the above example, estimates is a dictionary in which each item is an object of results. In our example there is only one object because only one regression was run. In cases in which multiple regressions are run because multiple sectors are estimated separately, the dictionary would contain multiple results, each keyed with the name of the respective sector. # Return a list of keys in the object >>> estimates . keys () dict_keys ([ 'all' ]) # Return the result object and save it to a new variable for convenience >>> results = estimates [ 'all' ] The estimation uses tools from the statsmodels package so that the results inherit all of the features of the statsmodels GLM results object. 3 This means that the object contains a plethora of fields reflecting things like coefficient estimates, standard errors, p-values, AIC/BIC, etc. Similarly, there is a useful method associated with the object that can be used for creating summary tables. # print a summary of the results >>> results . summary () < class ' statsmodels . iolib . summary . Summary '> Generalized Linear Model Regression Results ============================================================================== Dep . Variable : trade_value No . Observations : 8700 Model : GLM Df Residuals : 8371 Model Family : Poisson Df Model : 328 Link Function : log Scale : 1.0 Method : IRLS Log - Likelihood : - 4.8282e+12 Date : Wed , 20 Jun 2018 Deviance : 9.6565e+12 Time : 13 : 36 : 10 Pearson chi2 : 1.22e+13 No . Iterations : 10 ============================================================================================ coef std err z P >| z | [ 0.025 0.975 ] -------------------------------------------------------------------------------------------- log_distance - 0.7398 0.024 - 30.982 0.000 - 0.787 - 0.693 agree_pta 0.3342 0.043 7.824 0.000 0.250 0.418 common_language 0.1288 0.039 3.270 0.001 0.052 0.206 contiguity 0.2552 0.047 5.423 0.000 0.163 0.347 importer_year_fe_ARG2013 26.9804 0.361 74.690 0.000 26.272 27.688 importer_year_fe_ARG2014 26.8032 0.344 77.840 0.000 26.128 27.478 importer_year_fe_AUS2013 28.1690 0.315 89.455 0.000 27.552 28.786 ... ( truncated for this tutorial ) ============================================================================================ # Extract the estimated parameter values (returned as a Pandas.Series) >>> coefficients = results . params >>> coefficients . head () log_distance - 0.739840 agree_pta 0.334219 common_language 0.128770 contiguity 0.255161 importer_year_fe_ARG2013 26.980367 dtype : float64 # Extract the standard errors >>> results . bse log_distance 0.023879 agree_pta 0.042720 ... exporter_year_fe_VEN2015 0.346733 Length : 329 , dtype : float64 # Extract the p-values >>> results . pvalues log_distance 9.318804e-211 agree_pta 5.134355e-15 ... exporter_year_fe_VEN2015 5.681631e-03 Length : 329 , dtype : float64 # Return fitted values >>> results . fittedvalues 0 1.610136e+09 1 3.044133e+08 2 5.799368e+08 ... 9359 1.329831e+10 Length : 8700 , dtype : float64 The estimate method also provides some diagnostic information that helps judge the quality of the regression. This information includes a listing of columns that dropped due to collinearities or an absence of trade and an indicator for over-fitting. # Return diagnostic information (a Pandas.Series or DataFrame) >>> fixed_effects_model_2 . ppml_diagnostics Overfit Warning No Collinearities Yes Number of Columns Excluded 41 Perfectly Collinear Variables [ exporter_year_fe_ZAF2013 , exporter_year_fe_ZA ... Zero Trade Variables [ importer_year_fe_ARG2015 , importer_year_fe_AU ... Completion Time 0.25 minutes dtype : object # Retrieve the full list of collinear columns >>> fixed_effects_model_2 . ppml_diagnostics [ 'Perfectly Collinear Variables' ] [ 'exporter_year_fe_ZAF2013' , 'exporter_year_fe_ZAF2014' , 'exporter_year_fe_ZAF2015' ] The gme package also features several tools to help compile and format the results for use within python or outside of it. These tools include one method named combine_sector_results() for pulling all coefficients, standard errors, and p-values from from multiple sectors into a single DataFrame. The second, called format_regression_tables , creates formatted tables for presentation that can be exported as a text file, csv file, or LaTeX file with some stylized syntax. # Collect coefficients, standard errors, and p-values from the regression results. >>> combined_results = fixed_effects_model_2 . combine_sector_results () >>> combined_results . head () all_coeff all_pvalue all_stderr log_distance - 0.739840 9.318804e-211 ( 0.023879411125052336 ) agree_pta 0.334219 5.134355e-15 ( 0.04271952339258154 ) common_language 0.128770 1.076932e-03 ( 0.03938367074719932 ) contiguity 0.255161 5.857612e-08 ( 0.04705076644539403 ) importer_year_fe_ARG2013 26.980367 0.000000e+00 ( 0.3612289201097519 ) # Had there been multiple sectors/regressions in the model results, there would # have been additional columns in the 'combined' DataFrame. # Format the table and export to a csv file >>> fixed_effects_model . format_regression_table ( format = 'csv' , se_below = True , omit_fe_prefix = [ 'importer_year' , 'exporter_year' ], path = 'C: \\\\ formatted_table.csv' ) # which writes a table to the hard drive that looks like: Variable all agree_pta 0.334 *** ( 0.043 ) common_language 0.129 *** ( 0.039 ) contiguity 0.255 *** ( 0.047 ) log_distance - 0.740 *** ( 0.024 ) AIC 9656495631872.947 BIC 9656495371812.434 Likelihood - 4828247815607.474 Obs . 8700 It is also worth noting that the commands combine_sector_results() and format_regression_tables() can both be used in one of two ways. They are both stand-alone functions that can be supplied dictionaries of results (produced directly from the EstimationModel or custom assembled by a user), as in the format_regression_table() example. Alternatively, both can be used as methods on the EstimationObject itself, as in the combine_sector_results() example. Tip LaTeX users : The method format_regression_table can output a table into a csv file with some desired LaTeX syntax. This can be done by specifying format = '.csv' and 'latex_syntax'=True . This option allows users to manipulate the tables in spreadsheet software while retaining LaTeX syntax. We recommend reading the file into the spreadsheet software as 'text data' so that it does not try to interpret the formatting of the table entries. The gme package also has several ways to save the results to a file, either in full or in a space-saving slim version. See the API Reference portion of this documentation for further information. Had their been multiple sectors, we could have indicated so by adding the input sector_var_name = 'sector_column' in the declaration of the EstimationData. \u21a9 Additionally, the results of the estimation are saved as a attribute of the estimation model--- EstimationModel.results_dict ---and can be retrieved that way as well. \u21a9 For more details about the statsmodels results object, see http://www.statsmodels.org/0.6.1/generated/statsmodels.genmod.generalized_linear_model.GLMResults.html . \u21a9","title":"Estimation"},{"location":"estimation_tutorial/#estimating-a-gravity-model","text":"This tutorial demonstrates a basic gravity analysis including loading data, constructing some summary statistics, estimating a model, and outputting the results in several possible formats. For more information, see the list of commands and API reference in this documentation.","title":"Estimating a Gravity Model"},{"location":"estimation_tutorial/#load-data","text":"The gme package uses a special object, called gme.EstimationData to store data. EstimationData includes a Pandas.DataFrame containing data (trade + gravity + etc.) for estimation as well as additional information about that data and methods that can be used to summarize and/or manipulate the data. This tutorial will demonstrate some of these features. First, we must begin by creating a gme.EstimationData. Doing so requires the inputting of a Pandas.DataFrame and several pieces of \"meta-data\" that describe the data. Start by loading a dataset using the read_csv() function from pandas. In the sample code below, we will read a dataset directly from the internet, but you could just as easily read the same file from your hard drive. >> > from src import gme as gme >> > import pandas as pd >> > gravity_data = pd . read_csv ( 'https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv' ) >> > gravity_data . head () importer exporter year trade_value agree_pta common_language 0 AUS BRA 1989 3.035469e+08 0.0 1.0 1 AUS CAN 1989 8.769946e+08 0.0 1.0 2 AUS CHE 1989 4.005245e+08 0.0 1.0 3 AUS DEU 1989 2.468977e+09 0.0 0.0 4 AUS DNK 1989 1.763072e+08 0.0 1.0 contiguity log_distance 0 0.0 9.553332 1 0.0 9.637676 2 0.0 9.687557 3 0.0 9.675007 4 0.0 9.657311 # Next, we use the loaded data to create an EstimationData instance called gme_data >> > gme_data = gme . EstimationData ( data_frame = gravity_data , imp_var_name = 'importer' , exp_var_name = 'exporter' , trade_var_name = 'trade_value' , year_var_name = 'year' , notes = 'Downloaded from https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv' ) In creating an instance of the EstimationData object, the user is asked to supply a Pandas.DataFrame, which we a loaded in the previous lines, and several types of descriptive arguments. These arguments ( imp_var_name , exp_var_name , trade_var_name , and year_var_name ) specify the columns in the supplied DataFrame corresponding to particular types of information that will likely be present in any gravity analysis (the column containing the importer ID, exporter ID, trade values, and year, respectively). These \"meta-data\" fields can be useful as they prevent users from having to re-enter these same basic characteristics of the data at later points and permit the automatic construction of certain types of summary information. Finally, an optional note is supplied to the EstimationData. The EstimationData object has a attribute that stores a list user-supplied strings for later reference. In this case, we have supplied a note indicating from where the data originated.","title":"Load Data"},{"location":"estimation_tutorial/#working-with-estimationdata","text":"In addition to providing an object class that communicates conveniently with the gme.EstimationModel (see below), the EstimationData provides a collection of data summary and manipulation tools. For example, simply calling (or printing) the object, returns a summary of the scope of the data: >>> gme_data number of countries : 62 number of exporters : 62 number of importers : 62 number of years : 27 number of sectors : not_applicable dimensions : ( 98612 , 8 ) As can be seen from the console return, the dataset we are using covers 62 importers and exporters, 27 years, and contains 98,612 rows and 8 columns. Because this particular dataset does not have multiple sectors, that field is marked as 'not applicable'. 1 Other summary information can be reported in the following ways: # Return the number of importers in the dataset. >>> gme_data . number_of_importers 62 # Return a list of the column names >>> gme_data . columns [ 'importer' , 'exporter' , 'year' , 'trade_value' , 'agree_pta' , 'common_language' , 'contiguity' , 'log_distance' ] # Return a list of years in the dataset >>> gme_data . year_list () [ 1989 , 1990 , 1991 , 1992 , 1993 , 1994 , 1995 , 1996 , 1997 , 1998 , 1999 , 2000 , 2001 , 2002 , 2003 , 2004 , 2005 , 2006 , 2007 , 2008 , 2009 , 2010 , 2011 , 2012 , 2013 , 2014 , 2015 ] # Return a dictionary containing a list of countries in the dataset for each year. >>> country_list = gme_data . countries_each_year () >>> country_list [ 1989 ] [ 'IRN' , 'BOL' , 'TUR' , 'ARG' , 'CHL' , 'HUN' , 'KEN' , 'VEN' , 'ZAF' , 'URY' , 'BRA' , 'DZA' , 'PER' , 'IRL' , 'DNK' , 'GHA' , 'KOR' , 'PAK' , 'COL' , 'IND' , 'ISL' , 'ISR' , 'ESP' , 'ITA' , 'NLD' , 'NGA' , 'AUS' , 'SWE' , 'PRY' , 'GBR' , 'IDN' , 'HKG' , 'NOR' , 'TUN' , 'EGY' , 'KWT' , 'DEU' , 'CHE' , 'MYS' , 'NZL' , 'LBY' , 'USA' , 'SDN' , 'CHN' , 'GRC' , 'MEX' , 'CAN' , 'PRT' , 'SAU' , 'POL' , 'PHL' , 'THA' , 'FRA' , 'JPN' , 'MAR' , 'AUT' , 'FIN' , 'SGP' , 'ECU' ] # Additionally, many of the descriptive methods from Pandas.DataFrames have been inherited: >>> gme_data . dtypes () importer object exporter object year int64 trade_value float64 agree_pta float64 common_language float64 contiguity float64 log_distance float64 dtype : object >>> gme_data . info () < class ' pandas . core . frame . DataFrame '> RangeIndex : 98612 entries , 0 to 98611 Data columns ( total 8 columns ): importer 98612 non - null object exporter 98612 non - null object year 98612 non - null int64 trade_value 98612 non - null float64 agree_pta 97676 non - null float64 common_language 97676 non - null float64 contiguity 97676 non - null float64 log_distance 97676 non - null float64 dtypes : float64 ( 5 ), int64 ( 1 ), object ( 2 ) memory usage : 6.0 + MB >>> gme_data . describe () year trade_value agree_pta common_language \\ count 98612.000000 9.861200e+04 97676.000000 97676.000000 mean 2002.210441 1.856316e+09 0.381547 0.380646 std 7.713050 1.004735e+10 0.485769 0.485548 min 1989.000000 0.000000e+00 0.000000 0.000000 25 % 1996.000000 1.084703e+06 0.000000 0.000000 50 % 2002.000000 6.597395e+07 0.000000 0.000000 75 % 2009.000000 6.125036e+08 1.000000 1.000000 max 2015.000000 4.977686e+11 1.000000 1.000000 contiguity log_distance count 97676.000000 97676.000000 mean 0.034051 8.722631 std 0.181362 0.818818 min 0.000000 5.061335 25 % 0.000000 8.222970 50 % 0.000000 9.012502 75 % 0.000000 9.303026 max 1.000000 9.890765 Additionally, the EstimationData object retains the full ability to work with the supplied DataFrame. The DataFrame can be easily accessed by referring to its attribute in EstimationData. # Return the column of trade_values >>> gme_data . data_frame [ 'trade_value' ] 0 3.035469e+08 1 8.769946e+08 2 4.005245e+08 ... 98609 0.000000e+00 98610 0.000000e+00 98611 0.000000e+00 Name : trade_value , Length : 98612 , dtype : float64 Finally, the EstimationData object features a tool for easy aggregation and custom summary information. Additionally, because the method used for this process returns a DataFrame, the aggregated information can itself be used for many other applications, including the creation of a new EstimationData object. # Calculate mean, minimum, and maximum trade values and distances for each importer. >>> aggregated_data_1 = gme_data . tabulate_by_group ( tba_variables = [ 'trade_value' , 'log_distance' ], by_group = [ 'importer' ], how = [ 'mean' , 'min' , 'max' ]) >>> aggregated_data_1 . head ( 5 ) importer_ trade_value_mean trade_value_min trade_value_max \\ 0 ARG 4.579184e+08 0.0 2.218091e+10 1 AUS 1.704595e+09 0.0 4.620843e+10 2 AUT 1.241109e+09 0.0 6.973252e+10 3 BEL 4.783322e+09 0.0 9.963373e+10 4 BOL 5.506187e+07 0.0 1.810665e+09 log_distance_mean log_distance_min log_distance_max 0 9.137979 6.377581 9.856877 1 9.421134 7.926203 9.774024 2 8.231444 5.723756 9.800177 3 8.223088 5.061335 9.824095 4 9.090685 7.150738 9.874070 # Calculate mean, standard deviation, and count of trade for each importer/exporter pair. >>> aggregated_data_2 = gme_data . tabulate_by_group ( tab_variables = [ 'trade_value' ], by_group = [ 'importer' , 'exporter' ], how = [ 'mean' , 'std' , 'count' ]) >>> aggregated_data_2 . head ( 5 ) importer_ exporter_ trade_value_mean trade_value_std trade_value_count 0 ARG AUS 1.205231e+08 1.056954e+08 27 1 ARG AUT 9.942091e+07 8.018963e+07 27 2 ARG BEL 2.972734e+08 2.129833e+08 17 3 ARG BOL 3.378161e+08 6.339966e+08 27 4 ARG BRA 8.209756e+09 6.723047e+09 27 Note Knowing when to end a command with ( ) : When first learning python, it can be confusing trying to determine when a command applied to an object should be followed by parentheses. In the preceding code example, you will see instances of both: gme_data.columns and gme_data.year_list( ) , for example. Knowing which format to use is largely a matter of becoming familiar with the functions you are using. However, there is a logic to it. Each time a command is applied to an object (i.e. using the syntax object.command ), you are calling either an attribute of the object or a method on the object. An attribute is a piece of information that has already been created and included in the object whereas a method is effectively a function that can be run on the object. A method will be called with two parentheses because they will often accept additional arguments. For example, this is the case with the DataFrame method head( ) , which can accept a user-provided number of rows. However, you will often find that you do not need to supply additional arguments to a method, in which case you leave the parentheses empty. An attribute, by comparison, does not feature ( ) because there is no need or ability to provide additional input because the contents of that attribute have already been computed. As mentioned before, knowing whether a command is an attribute or a method, however, simply requires familiarity with the object.","title":"Working with EstimationData"},{"location":"estimation_tutorial/#creating-and-estimating-a-model","text":"Once a EstimationData has been created, estimating a gravity model using the data is fairly straightforward. There are two basic step for estimation: (1) define a model and (2) estimate the model. Defining the model amounts to creating another object called EstimationModel . Like the EstimationData, EstimationModel is meant to standardize and simplify the steps typically taken to specify and estimate a gravity model. While the EstimationData is meant to be an object that is created once for each study, many EstimationModels will likely be defined and redefined as you test different specifications. Thus, the arguments and attributes of the EstimationModel reflect the different types of modifications you may want to make as you select your preferred specification. As with the EstimationData, the EstimationModel is largely a dataset, with added information that define the characteristics of the particular model. The following examples depict several model specifications, each demonstrating different types of model aspects that can be specified. # A simple case in which 'trade_value' is dependent on 'log_distance', 'agree_pta', etc. >>> model_baseline = gme . EstimationModel ( estimation_data = gme_data , lhs_var = 'trade_value' , rhs_var = [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' ]) # A specification that will generate and include importer-year and exporter-year # fixed effects >>> fixed_effects_model = gme . EstimationModel ( estimation_data = gme_data , lhs_var = 'trade_value' , rhs_var = [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' ], fixed_effects = [[ 'importer' , 'year' ], [ 'exporter' , 'year' ]]) # A specification that uses a subset of the data. The United States ('USA') will be omitted # and only the years 2013--2015 will be included. >>> data_subset_model = gme . EstimationModel ( estimation_data = gme_data , lhs_var = 'trade_value' , rhs_var = [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' ], drop_imp_exp = [ 'USA' ], keep_years = [ 2015 , 2014 , 2013 ]) When specifying a model, there are several key types of attributes that can be included: Model Variables : The variables to be included are specified using the arguments lhs_vars and rhs_vars , which denote the left-hand-side dependent variable and right-hand-side independent variables, respectively. Fixed Effects : The model, at the point at which it is estimated, will construct fixed effects if any are specified by fixed_effects . These can be either single variables (e.g. ['importer']), or interacted variables (e.g. [['importer', 'year']]). For example, entering [ 'importer', ['exporter', 'year']] would yield a set of importer fixed effects and a set of exporter-year fixed effects. Data Subsets : Subsets of the data to use for estimation can be specified in a variety of ways. The arguments keep_years and drop_years can be used to select only a subset of years to include. Similarly the keep_imp , keep_exp , and keep_imp_exp arguments, and their corresponding drop_... options can do the same for importers and/or exporters. Once a model has been defined, running a PPML estimation according to the supplied specification is quite straightforward. It only requires the application of a single method of the EstimationModel: .estimate() . No further inputs are required. # Define a new, fixed effects model using only a subset of years (to reduce the computation time) >>> fixed_effects_model_2 = gme . EstimationModel ( estimation_data = gme_data , lhs_var = 'trade_value' , rhs_var = [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' ], fixed_effects = [[ 'importer' , 'year' ], [ 'exporter' , 'year' ]], keep_years = [ 2013 , 2014 , 2015 ]) # Conduct a PPML estimation of the fixed effects model. estimates = fixed_effects_model_2 . estimate () select specification variables : [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' , 'trade_value' , 'importer' , 'exporter' , 'year' ], Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } drop_intratrade : no , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } drop_imp : none , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } drop_exp : none , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } keep_imp : all available , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } keep_exp : all available , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } drop_years : none , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } keep_years : [ 2013 , 2014 , 2015 ], Observations excluded by user : { 'rows' : 87632 , 'columns' : 0 } drop_missing : yes , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } Estimation began at 08 : 58 AM on Jun 19 , 2018 Estimation completed at 08 : 58 AM on Jun 19 , 2018 The results ( estimates ) are stored as a dictionary with each entry in the dictionary corresponding to a single estimation. 2 The storing of the results in this way is primarily to facilitate the sector_by_sector , which separately estimates a model for each product/industry/sector, and returns a set of estimation results for each. In the case in which multiple sectors are not considered, as in the examples considered here, the results dictionary contains a single entry with the key 'all'.","title":"Creating and estimating a model"},{"location":"estimation_tutorial/#viewing-formatting-and-outputting-the-results","text":"The first step to viewing and working with the regression estimates is unpacking them from the dictionary in which they have been stored. A dictionary is an object in which each item stored in the dictionary is associated with a key. That key can be used to return its associated item. In the above example, estimates is a dictionary in which each item is an object of results. In our example there is only one object because only one regression was run. In cases in which multiple regressions are run because multiple sectors are estimated separately, the dictionary would contain multiple results, each keyed with the name of the respective sector. # Return a list of keys in the object >>> estimates . keys () dict_keys ([ 'all' ]) # Return the result object and save it to a new variable for convenience >>> results = estimates [ 'all' ] The estimation uses tools from the statsmodels package so that the results inherit all of the features of the statsmodels GLM results object. 3 This means that the object contains a plethora of fields reflecting things like coefficient estimates, standard errors, p-values, AIC/BIC, etc. Similarly, there is a useful method associated with the object that can be used for creating summary tables. # print a summary of the results >>> results . summary () < class ' statsmodels . iolib . summary . Summary '> Generalized Linear Model Regression Results ============================================================================== Dep . Variable : trade_value No . Observations : 8700 Model : GLM Df Residuals : 8371 Model Family : Poisson Df Model : 328 Link Function : log Scale : 1.0 Method : IRLS Log - Likelihood : - 4.8282e+12 Date : Wed , 20 Jun 2018 Deviance : 9.6565e+12 Time : 13 : 36 : 10 Pearson chi2 : 1.22e+13 No . Iterations : 10 ============================================================================================ coef std err z P >| z | [ 0.025 0.975 ] -------------------------------------------------------------------------------------------- log_distance - 0.7398 0.024 - 30.982 0.000 - 0.787 - 0.693 agree_pta 0.3342 0.043 7.824 0.000 0.250 0.418 common_language 0.1288 0.039 3.270 0.001 0.052 0.206 contiguity 0.2552 0.047 5.423 0.000 0.163 0.347 importer_year_fe_ARG2013 26.9804 0.361 74.690 0.000 26.272 27.688 importer_year_fe_ARG2014 26.8032 0.344 77.840 0.000 26.128 27.478 importer_year_fe_AUS2013 28.1690 0.315 89.455 0.000 27.552 28.786 ... ( truncated for this tutorial ) ============================================================================================ # Extract the estimated parameter values (returned as a Pandas.Series) >>> coefficients = results . params >>> coefficients . head () log_distance - 0.739840 agree_pta 0.334219 common_language 0.128770 contiguity 0.255161 importer_year_fe_ARG2013 26.980367 dtype : float64 # Extract the standard errors >>> results . bse log_distance 0.023879 agree_pta 0.042720 ... exporter_year_fe_VEN2015 0.346733 Length : 329 , dtype : float64 # Extract the p-values >>> results . pvalues log_distance 9.318804e-211 agree_pta 5.134355e-15 ... exporter_year_fe_VEN2015 5.681631e-03 Length : 329 , dtype : float64 # Return fitted values >>> results . fittedvalues 0 1.610136e+09 1 3.044133e+08 2 5.799368e+08 ... 9359 1.329831e+10 Length : 8700 , dtype : float64 The estimate method also provides some diagnostic information that helps judge the quality of the regression. This information includes a listing of columns that dropped due to collinearities or an absence of trade and an indicator for over-fitting. # Return diagnostic information (a Pandas.Series or DataFrame) >>> fixed_effects_model_2 . ppml_diagnostics Overfit Warning No Collinearities Yes Number of Columns Excluded 41 Perfectly Collinear Variables [ exporter_year_fe_ZAF2013 , exporter_year_fe_ZA ... Zero Trade Variables [ importer_year_fe_ARG2015 , importer_year_fe_AU ... Completion Time 0.25 minutes dtype : object # Retrieve the full list of collinear columns >>> fixed_effects_model_2 . ppml_diagnostics [ 'Perfectly Collinear Variables' ] [ 'exporter_year_fe_ZAF2013' , 'exporter_year_fe_ZAF2014' , 'exporter_year_fe_ZAF2015' ] The gme package also features several tools to help compile and format the results for use within python or outside of it. These tools include one method named combine_sector_results() for pulling all coefficients, standard errors, and p-values from from multiple sectors into a single DataFrame. The second, called format_regression_tables , creates formatted tables for presentation that can be exported as a text file, csv file, or LaTeX file with some stylized syntax. # Collect coefficients, standard errors, and p-values from the regression results. >>> combined_results = fixed_effects_model_2 . combine_sector_results () >>> combined_results . head () all_coeff all_pvalue all_stderr log_distance - 0.739840 9.318804e-211 ( 0.023879411125052336 ) agree_pta 0.334219 5.134355e-15 ( 0.04271952339258154 ) common_language 0.128770 1.076932e-03 ( 0.03938367074719932 ) contiguity 0.255161 5.857612e-08 ( 0.04705076644539403 ) importer_year_fe_ARG2013 26.980367 0.000000e+00 ( 0.3612289201097519 ) # Had there been multiple sectors/regressions in the model results, there would # have been additional columns in the 'combined' DataFrame. # Format the table and export to a csv file >>> fixed_effects_model . format_regression_table ( format = 'csv' , se_below = True , omit_fe_prefix = [ 'importer_year' , 'exporter_year' ], path = 'C: \\\\ formatted_table.csv' ) # which writes a table to the hard drive that looks like: Variable all agree_pta 0.334 *** ( 0.043 ) common_language 0.129 *** ( 0.039 ) contiguity 0.255 *** ( 0.047 ) log_distance - 0.740 *** ( 0.024 ) AIC 9656495631872.947 BIC 9656495371812.434 Likelihood - 4828247815607.474 Obs . 8700 It is also worth noting that the commands combine_sector_results() and format_regression_tables() can both be used in one of two ways. They are both stand-alone functions that can be supplied dictionaries of results (produced directly from the EstimationModel or custom assembled by a user), as in the format_regression_table() example. Alternatively, both can be used as methods on the EstimationObject itself, as in the combine_sector_results() example. Tip LaTeX users : The method format_regression_table can output a table into a csv file with some desired LaTeX syntax. This can be done by specifying format = '.csv' and 'latex_syntax'=True . This option allows users to manipulate the tables in spreadsheet software while retaining LaTeX syntax. We recommend reading the file into the spreadsheet software as 'text data' so that it does not try to interpret the formatting of the table entries. The gme package also has several ways to save the results to a file, either in full or in a space-saving slim version. See the API Reference portion of this documentation for further information. Had their been multiple sectors, we could have indicated so by adding the input sector_var_name = 'sector_column' in the declaration of the EstimationData. \u21a9 Additionally, the results of the estimation are saved as a attribute of the estimation model--- EstimationModel.results_dict ---and can be retrieved that way as well. \u21a9 For more details about the statsmodels results object, see http://www.statsmodels.org/0.6.1/generated/statsmodels.genmod.generalized_linear_model.GLMResults.html . \u21a9","title":"Viewing, formatting, and outputting the results"},{"location":"getting_started/","text":"Getting Started Preparing the Software Dependencies The GME package requires pandas and statsmodels packages. Installation Download and install the GME package. Tip Easily copy the code below by using the copy button in the upper right corner of the code block. pip install gme A Basic Example: Run PPML estimation Step 1. Import the needed packages import gme as gme import pandas as pd Step 2. Create EstimationData The GME package is built upon a specialized data object called EstimationData. EstimationData contains data in a Pandas DataDrame as well as a collection of additional information and tools that are useful for gravity modeling, such as A log of the history of the data, such as the location of the file it was read from and modifications made to it, Metadata, such as names of the columns containing importer, exporter, and year information, so that they need not be continuously supplied, Several tools for producing summary statistics or other types of commonly sought descriptive information. Begin by loading example trade data. The dataset used in the following code is available for download or can be accessed directly with python and pandas, as shown below. sample_data = pd . read_csv ( 'https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv' ) sample_data . head () The first command above reads the data file into memory while the second shows the column names and first 5 lines of the data file. importer exporter year trade_value agree_pta common_language \\ 0 AUS BRA 1989 3.035469e+08 0.0 1.0 1 AUS CAN 1989 8.769946e+08 0.0 1.0 2 AUS CHE 1989 4.005245e+08 0.0 1.0 3 AUS DEU 1989 2.468977e+09 0.0 0.0 4 AUS DNK 1989 1.763072e+08 0.0 1.0 contiguity log_distance 0 0.0 9.553332 1 0.0 9.637676 2 0.0 9.687557 3 0.0 9.675007 4 0.0 9.657311 Next, create an instance of the EstimationData object using the sample data. To create an EstimationData instance (called gme_data in the example below), you need to supply a Pandas DataFrame and identifiers for certain key columns, such as the trade flows, importer/exporter, year, and sector (if applicable). gme_data = gme . EstimationData ( data_frame = sample_data , imp_var_name = 'importer' , exp_var_name = 'exporter' , trade_var_name = 'trade_value' , year_var_name = 'year' ) print ( gme_data ) The print command above produces basic summary statistics, contained in a printable representation of the EstimationData class: number of countries: 62 number of exporters: 62 number of importers: 62 number of years: 27 number of sectors: not_applicable dimensions: (98612, 8) Step 3. Create an EstimationModel After creating an EstimationData object, you need to create an EstimationModel object, which will be used to produce gravity estimates. To create an EstimationModel instance (called gme_model in the example below), you need to supply an EstimationData object and a specification. gme_model = gme . EstimationModel ( estimation_data = gme_data , lhs_var = 'trade_value' , rhs_var = [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' ], fixed_effects = [ 'importer' , 'exporter' ], keep_years = [ 2015 ]) The initialization of EstimationModel establishes a reference to the EstimationData object rather than a copy of the data. Step 4. Estimate the model What is a method? A method is a function that is connected only to a particular object. Click here to see the relevant Python documentation. Once the EstimationModel is defined, it can be estimated by applying the method .estimate() gme_model . estimate () The code provides some information while it is running: select specification variables: ['log_distance', 'agree_pta', 'common_language', 'contiguity', 'trade_value', 'importer', 'exporter', 'year'], Observations excluded by user: {'rows': 0, 'columns': 0} drop_intratrade: no, Observations excluded by user: {'rows': 0, 'columns': 0} drop_imp: none, Observations excluded by user: {'rows': 0, 'columns': 0} drop_exp: none, Observations excluded by user: {'rows': 0, 'columns': 0} keep_imp: all available, Observations excluded by user: {'rows': 0, 'columns': 0} keep_exp: all available, Observations excluded by user: {'rows': 0, 'columns': 0} drop_years: none, Observations excluded by user: {'rows': 0, 'columns': 0} keep_years: [2015], Observations excluded by user: {'rows': 94952, 'columns': 0} drop_missing: yes, Observations excluded by user: {'rows': 0, 'columns': 0} Estimation began at 09:29 AM on Jun 13, 2018 Estimation completed at 09:29 AM on Jun 13, 2018''' The results are stored in a collection (called dictionary in Python) with each sector having its own set of results. If no sectors were supplied or used, there would be only one set of results in the dictionary, labeled 'all'. A simple table with regression results can be produced with the following command: gme_model . format_regression_table ( format = \u2018 txt \u2019 , path = \u201c C : \\\\ Documents \\\\ regression_table . txt \u201d ) which produces the regression results: Variable all a_agree_pta agree_pta 0.338*** a_agree_pta_se (0.088) a_common_language common_language 0.063 a_common_language_se (0.071) a_contiguity contiguity 0.211*** a_contiguity_se (0.085) a_exporter_fe_ARG exporter_fe_ARG -0.444 a_exporter_fe_ARG_se (0.365) a_exporter_fe_AUS exporter_fe_AUS 0.619 a_exporter_fe_AUS_se (0.500) ... ... a_importer_fe_USA importer_fe_USA 30.791*** a_importer_fe_USA_se (0.562) a_log_distance log_distance -0.784*** a_log_distance_se (0.051) b_aic AIC 1806014725995.581 b_bic BIC 1806014667986.696 b_llf Likelihood -903007362899.79 b_nobs Obs. 2040","title":"Getting started"},{"location":"getting_started/#getting-started","text":"","title":"Getting Started"},{"location":"getting_started/#preparing-the-software","text":"","title":"Preparing the Software"},{"location":"getting_started/#dependencies","text":"The GME package requires pandas and statsmodels packages.","title":"Dependencies"},{"location":"getting_started/#installation","text":"Download and install the GME package. Tip Easily copy the code below by using the copy button in the upper right corner of the code block. pip install gme","title":"Installation"},{"location":"getting_started/#a-basic-example-run-ppml-estimation","text":"","title":"A Basic Example: Run PPML estimation"},{"location":"getting_started/#step-1-import-the-needed-packages","text":"import gme as gme import pandas as pd","title":"Step 1. Import the needed packages"},{"location":"getting_started/#step-2-create-estimationdata","text":"The GME package is built upon a specialized data object called EstimationData. EstimationData contains data in a Pandas DataDrame as well as a collection of additional information and tools that are useful for gravity modeling, such as A log of the history of the data, such as the location of the file it was read from and modifications made to it, Metadata, such as names of the columns containing importer, exporter, and year information, so that they need not be continuously supplied, Several tools for producing summary statistics or other types of commonly sought descriptive information. Begin by loading example trade data. The dataset used in the following code is available for download or can be accessed directly with python and pandas, as shown below. sample_data = pd . read_csv ( 'https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv' ) sample_data . head () The first command above reads the data file into memory while the second shows the column names and first 5 lines of the data file. importer exporter year trade_value agree_pta common_language \\ 0 AUS BRA 1989 3.035469e+08 0.0 1.0 1 AUS CAN 1989 8.769946e+08 0.0 1.0 2 AUS CHE 1989 4.005245e+08 0.0 1.0 3 AUS DEU 1989 2.468977e+09 0.0 0.0 4 AUS DNK 1989 1.763072e+08 0.0 1.0 contiguity log_distance 0 0.0 9.553332 1 0.0 9.637676 2 0.0 9.687557 3 0.0 9.675007 4 0.0 9.657311 Next, create an instance of the EstimationData object using the sample data. To create an EstimationData instance (called gme_data in the example below), you need to supply a Pandas DataFrame and identifiers for certain key columns, such as the trade flows, importer/exporter, year, and sector (if applicable). gme_data = gme . EstimationData ( data_frame = sample_data , imp_var_name = 'importer' , exp_var_name = 'exporter' , trade_var_name = 'trade_value' , year_var_name = 'year' ) print ( gme_data ) The print command above produces basic summary statistics, contained in a printable representation of the EstimationData class: number of countries: 62 number of exporters: 62 number of importers: 62 number of years: 27 number of sectors: not_applicable dimensions: (98612, 8)","title":"Step 2. Create EstimationData"},{"location":"getting_started/#step-3-create-an-estimationmodel","text":"After creating an EstimationData object, you need to create an EstimationModel object, which will be used to produce gravity estimates. To create an EstimationModel instance (called gme_model in the example below), you need to supply an EstimationData object and a specification. gme_model = gme . EstimationModel ( estimation_data = gme_data , lhs_var = 'trade_value' , rhs_var = [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' ], fixed_effects = [ 'importer' , 'exporter' ], keep_years = [ 2015 ]) The initialization of EstimationModel establishes a reference to the EstimationData object rather than a copy of the data.","title":"Step 3. Create an EstimationModel"},{"location":"getting_started/#step-4-estimate-the-model","text":"What is a method? A method is a function that is connected only to a particular object. Click here to see the relevant Python documentation. Once the EstimationModel is defined, it can be estimated by applying the method .estimate() gme_model . estimate () The code provides some information while it is running: select specification variables: ['log_distance', 'agree_pta', 'common_language', 'contiguity', 'trade_value', 'importer', 'exporter', 'year'], Observations excluded by user: {'rows': 0, 'columns': 0} drop_intratrade: no, Observations excluded by user: {'rows': 0, 'columns': 0} drop_imp: none, Observations excluded by user: {'rows': 0, 'columns': 0} drop_exp: none, Observations excluded by user: {'rows': 0, 'columns': 0} keep_imp: all available, Observations excluded by user: {'rows': 0, 'columns': 0} keep_exp: all available, Observations excluded by user: {'rows': 0, 'columns': 0} drop_years: none, Observations excluded by user: {'rows': 0, 'columns': 0} keep_years: [2015], Observations excluded by user: {'rows': 94952, 'columns': 0} drop_missing: yes, Observations excluded by user: {'rows': 0, 'columns': 0} Estimation began at 09:29 AM on Jun 13, 2018 Estimation completed at 09:29 AM on Jun 13, 2018''' The results are stored in a collection (called dictionary in Python) with each sector having its own set of results. If no sectors were supplied or used, there would be only one set of results in the dictionary, labeled 'all'. A simple table with regression results can be produced with the following command: gme_model . format_regression_table ( format = \u2018 txt \u2019 , path = \u201c C : \\\\ Documents \\\\ regression_table . txt \u201d ) which produces the regression results: Variable all a_agree_pta agree_pta 0.338*** a_agree_pta_se (0.088) a_common_language common_language 0.063 a_common_language_se (0.071) a_contiguity contiguity 0.211*** a_contiguity_se (0.085) a_exporter_fe_ARG exporter_fe_ARG -0.444 a_exporter_fe_ARG_se (0.365) a_exporter_fe_AUS exporter_fe_AUS 0.619 a_exporter_fe_AUS_se (0.500) ... ... a_importer_fe_USA importer_fe_USA 30.791*** a_importer_fe_USA_se (0.562) a_log_distance log_distance -0.784*** a_log_distance_se (0.051) b_aic AIC 1806014725995.581 b_bic BIC 1806014667986.696 b_llf Likelihood -903007362899.79 b_nobs Obs. 2040","title":"Step 4. Estimate the model"},{"location":"list_of_commands/","text":"Data Summary tablulate_by_group(tab_variables,by_group,how) Summarize columns by a user-specified grouping. Can be used to tabulate, aggregate, and summarize data. year_list : Returns a list of years present in the data. countries_each_year : Returns a dictionary keyed by year ID containing a list of country IDs present in each corresponding year. sector_list : Returns a list of unique sector IDs. dtypes : Returns the data types of the columns in the EstimationData.data_frame using Pandas.DataFrame.dtypes(). See Pandas documentation for more information. info : Print summary information about EstimationData.data_frame using Pandas.DataFrame.info(). See Pandas documentation for more information. describe : Generates some descriptive statistics for EstimationData.data_frame using Pandas.DataFrame.describe(). See Pandas documentation for more information. add_note(note) : Add a note to the list of notes in 'notes' attribute. Estimation estimate : Estimate a PPML model combine_sector_results : Combine multiple result_dict entries into a single data frame. format_regression_table : Format regression results into a text, csv, or LaTeX table for presentation.","title":"List of commands"},{"location":"list_of_commands/#data-summary","text":"tablulate_by_group(tab_variables,by_group,how) Summarize columns by a user-specified grouping. Can be used to tabulate, aggregate, and summarize data. year_list : Returns a list of years present in the data. countries_each_year : Returns a dictionary keyed by year ID containing a list of country IDs present in each corresponding year. sector_list : Returns a list of unique sector IDs. dtypes : Returns the data types of the columns in the EstimationData.data_frame using Pandas.DataFrame.dtypes(). See Pandas documentation for more information. info : Print summary information about EstimationData.data_frame using Pandas.DataFrame.info(). See Pandas documentation for more information. describe : Generates some descriptive statistics for EstimationData.data_frame using Pandas.DataFrame.describe(). See Pandas documentation for more information. add_note(note) : Add a note to the list of notes in 'notes' attribute.","title":"Data Summary"},{"location":"list_of_commands/#estimation","text":"estimate : Estimate a PPML model combine_sector_results : Combine multiple result_dict entries into a single data frame. format_regression_table : Format regression results into a text, csv, or LaTeX table for presentation.","title":"Estimation"},{"location":"python_intro_tutorial/","text":"Introduction to Python This tutorial demonstrates some basic python concepts and syntax that are integral for working with the GME package. Additionally, it demonstrates aspects of the pandas package, which provides a collection of data tools that are both a major part of the GME tools and exceptionally useful for data analysis in Python. This tutorial is far from comprehensive, even when it comes to basic usage, but should help you become sufficiently well versed to begin working with the GME package. Basic Python Concepts and Syntax Computation One of the most fundamental processes is basic arithmetic. >>> 2 + 2 4 >>> 3 * 5 15 >>> ( 3 * 10 ) / 16 + 14 15.875 Variables and Types Variables can be used to store information for further interaction. >>> a = 16 # The variable you want to assign is on the left side, the value you want to set it to is on the right. >>> b = 10 >>> c = a * b >>> print ( a , b , c ) 16 10 160 >>> b = 25 >>> c = a * b >>> print ( a , b , c ) 16 25 400 Variables and data more broadly are much more expansive than just integers. The following is a non-exhaustive list of the data types you will frequently use. Integers : ( int ) A simple number without decimal points. # The type() function returns the data type of the argument. >>> type ( 12 ) < class ' int '> >>> type ( a ) < class ' int '> Float : ( float ) A number with decimal places. >>> type ( 3.25 ) < class ' float '> >>> type ( 2 ** ( . 5 )) < class ' float '> # you can also specify the type you would like a value to take >>> type ( float ( a )) < class ' float '> >>> print ( a ) 16.0 String : ( str ) Non-numeric characters. Strings are denoted by wrapping characters in either single or double quotes (e.g. ' ' or \" \"). >>> str_example = 'Hello World!' >>> print ( str_example ) Hello World >>> type ( str_example ) < class ' str '> >>> type ( \"12\" ) < class ' str '> >>> type ( 'a' ) < class ' str '> # We can also combine (concatenate) strings >>> print ( \"12\" + \"14\" ) 1214 Boolean : ( bool ) A binary type that equals True or False >>> boolean_example = True >>> type ( boolean_example ) < class ' bool '> List : ( list ) A list contains a series of values and, potentially, types. They are created using square brackets (e.g. [ ] ) >>> list_example = [ 1 , 12 , 25 , 37 , 22 ] >>> type ( list_example ) < class ' list '> # They are indexed >>> list_example [ 0 ] # Square brackets denote indexes in a list. i.e. the zeroth element of list_example in this case 1 >>> list_example [ 3 ] 37 >>> list_example [ 1 : 3 ] # and sliceable [ 12 , 25 ] # They can also contain other data types >>> list_example_2 = [ \"This\" , \"list\" , \"contains\" , 'strings' , float ( 22.134 ), a , b , c ] >>> print ( list_example_2 ) [ 'This' , 'list' , 'contains' , 'strings' , 22.134 , 16 , 25 , 400 ] >>> type ( list_example_2 [ 0 ]) < class ' str '> >>> type ( list_example_2 [ 7 ]) < class ' int '> # They can be combined >>> list_example_3 = list_example + list_example_2 >>> list_example_3 [ 1 , 12 , 25 , 37 , 22 , 'This' , 'list' , 'contains' , 'strings' , 22.134 , 16 , 25 , 400 ] # They can be added to >>> list_example . append ( 'last' ) >>> print ( list_example ) [ 1 , 12 , 25 , 37 , 22 , 'last' ] # and can be counted/measured >>> len ( list_example_3 ) 13 Tuple : ( tuple ) A tuple is similar to a list except that it's size and order is fixed. Many of the other features hold from lists but the inability to change the shape or order may offer some desirable protections in your code. They are declared using parentheses (e.g. ( ) ). >>> tuple_example = ( 'Hello' , 25 , a , c ) >>> type ( tuple_example ) < class ' tuple '> >>> tuple_example [ 2 ] 16 >>> tuple_example . append ( 'new element' ) ''' Traceback (most recent call last): File \"<input>\", line 1, in <module> AttributeError: 'tuple' object has no attribute 'append' ''' Dictionary : ( dict ) A dictionary is a very useful datatype which, like a list or tuple, can contain a series of values. However, it has the added feature that elements are 'keyed', such that each value is associated with a key. # The dictionary is declared using curly braces (e.g. { } ). Within the dictionary, a key (str) is followed by a colon then the value to assign to that key (e.g. key:value = 'first':'peter') >>> dictionary_example = { 'first' : 'Karl' , 'last' : 'Marx' , 'job' : 'economist' , 'ext' : 3186 } >>> print ( dictionary_example ) { 'first' : 'Karl' , 'last' : 'Marx' , 'job' : 'economist' , 'ext' : 3186 , 'location' : { 'city' : 'Washington' , 'state' : 'DC' }} >>> type ( dictionary_example ) < class ' dict '> # Dictionaries are indexed using the keys. >>> dictionary_example [ 'last' ] 'Marx' # Can be added to by creating a new key. >>> dictionary_example [ 'location' ] = { 'city' : 'Washington' , 'state' : 'DC' } >>> print ( dictionary_example ) { 'first' : 'Karl' , 'last' : 'Marx' , 'job' : 'economist' , 'ext' : 3186 , 'location' : { 'city' : 'Washington' , 'state' : 'DC' }} >>> dictionary_example [ 'location' ][ 'city' ] # chained indexing 'Washington' # The keys in a dictionary can be retrieved in the following way: >>> dictionary_example . keys () dict_keys ([ 'first' , 'last' , 'job' , 'ext' , 'location' ]) Logical Operators and Decision Tests Logical tests are a way of comparing things and performing conditional operations # Lets create some initial (int) values to work with >>> x = 10 >>> y = 10 >>> z = 5 >>> x == y # '==' tests the equivalence of the left and right side True >>> x != y # '!=' tests the non-equivalence of both sides False >>> x == ( y and z ) # 'and' tests the equivalence to both y *AND* z False >>> x == ( y or z ) # 'or' tests the equivalence to y *OR* z True >>> x > z # inequalities work as expected with '>', '<', '>=', and '<=' True >>> x <= y True # Logical operators are especially useful with if statements >>> x = 20 >>> if x > 11 : ... print ( 'x is bigger than 11' ) x is not bigger than 11 # or with 'else' statements >>> x = 10 >>> if x > 11 : ... print ( 'x is bigger than 11' ) ... else : ... print ( 'x is not bigger than 11' ) x is not bigger than 11 In Python, statements are grouped by offsetting commands using tabs. Unlike many other languages that wrap statements like if with particular syntax like parentheses, braces, or specific begin and end statements, Python interprets things entirely based on tab indentation. For example, a collection of commands belonging to a statement within another statement should be preceded by two tabs. Iteration One thing you will find very useful is the ability to automate and perform routine, repeating tasks. For loops are a useful tool for these tasks because they perform an action for each value in an 'iterable' object (e.g. a list or tuple ) >>> iteration_sequence = [ 0 , 1 , 2 , 3 , 4 ] >>> for i in iteration_sequence : # i gives the iteration value, iteration_sequence gives a (list) of values to iterate over, ... print ( i ) # : begins the loop. a tab indent indicates commands to be interpreted in the loop. # Unlike many languages, there is no closing statement for the loop, the indentation # fully declares the scope of the command. 0 1 2 3 4 # Suppose we wanted to use this to make some calculation (e.g. a Fibonacci Sequence) >>> fibonacci_sequence = [ 1 , 2 ] >>> for i in range ( 2 , 20 ): ... next_number = fibonacci_sequence [ i - 1 ] + fibonacci_sequence [ i - 2 ] # This adds the previous two entries in the (list) ... fibonacci_sequence . append ( next_number ) # this adds the new number to the end of the list >>> print ( fibonacci_sequence ) [ 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , 89 , 144 , 233 , 377 , 610 , 987 , 1597 , 2584 , 4181 , 6765 , 10946 ] # You can also iterate on elements rather than sequential numbers >>> dictionary_serge = { 'first' : 'Adam' , 'last' : 'Smith' , 'job' : 'economist' , 'location' :{ 'city' : 'Arlington' , 'state' : 'VA' }} >>> dictionary_list = [ dictionary_example , dictionary_serge ] # This creates a (list) of two (dictionaries) >>> for person in dictionary_list : # This time person is a dictionary rather than a simple index, and can be used that way ... print ( 'The ' + person [ 'job' ] + ' ' + person [ 'first' ] + ' ' + person [ 'last' ] ... + ' lives in ' + person [ 'location' ][ 'city' ]) ... if person [ 'location' ][ 'state' ] != 'DC' : ... person [ 'statehood' ] = True ... else : ... person [ 'statehood' ] = False The economist Karl Marx lives in Washington The economist Adam Smith lives in Arlington >>> print ( dictionary_list [ 0 ][ 'statehood' ], dictionary_list [ 1 ][ 'statehood' ]) False True Functions Functions provide a way of packaging and generalizing a routine task. A function is defined using the def command # Lets work on generalizing the fibonacci calculation >>> def fibonacci_nums ( sequence_length ): # 'def' tells python that you are about to define a function ... # 'fibonacci_nums' is the name of the fucntion ... # 'sequence_length' is an input to be supplied to the function ... fibonacci_sequence = [ 1 , 2 ] ... for i in range ( 2 , sequence_length ): ... next_number = fibonacci_sequence [ i - 1 ] + fibonacci_sequence [ i - 2 ] # This adds the previous two entries in the (list) ... fibonacci_sequence . append ( next_number ) # this adds the new number to the end of the list ... return fibonacci_sequence # this determines what the function returns. In this case it is a list of fibonacci numbers >>> fibonacci_nums ( 30 ) # Lets test the new function [ 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , 89 , 144 , 233 , 377 , 610 , 987 , 1597 , 2584 , 4181 , 6765 , 10946 , 17711 , 28657 , 46368 , 75025 , 121393 , 196418 , 317811 , 514229 , 832040 , 1346269 ] Data Manipulation and Regression Analysis The following with provide a basic crash course in using python to perform standard econometric/data analysis tasks. Our reccomendation for this type of work is to use the pandas package, which contains data manipulation tools, and the statsmodels package, which contains statistical tools. For users familiar with R, you should find the syntax and programming conventions of these packages very similar to those in R. Importing tools By default, python does not have a ton of tools. Fortunately, others have written tons of them. For econometrics, we will be extensively using two: pandas, which is a data manipulation toolbox, and statsmodels, which is a statistical package (numpy, which a package for arrays, is useful too). >>> import pandas as pd # The 'pd' gives a prefix that will be used to identify functions as coming from that package/module >>> import statsmodels.api as sm >>> import numpy as np Load Data Python (or, more accurately, pandas) has a robust set of tools for loading data in in a wide range of formats such as .csv, .dta (stata), or .xls (excel). For the sake of this tutorial, we will be loading some data from the internet via a URL. Most of the time, you will probably be loading data from a file location of your computer, which uses the same synatx with a file path replacing the URL. >>> loaded_trade_data = pd . read_csv ( ... 'https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv' ) # pd.read_csv() has created a Pandas DataFrame, which is the object that contains the data. # There are also read_excel, read_sql, read_stata, etc. versions of this function for a wide range of data file types. # To view the data, there are several options: >>> loaded_trade_data . head ( 5 ) # This prints the first 5 lines of the Dataframe importer exporter year trade_value agree_pta common_language \\ 0 AUS BRA 1989 3.035469e+08 0.0 1.0 1 AUS CAN 1989 8.769946e+08 0.0 1.0 2 AUS CHE 1989 4.005245e+08 0.0 1.0 3 AUS DEU 1989 2.468977e+09 0.0 0.0 4 AUS DNK 1989 1.763072e+08 0.0 1.0 contiguity log_distance 0 0.0 9.553332 1 0.0 9.637676 2 0.0 9.687557 3 0.0 9.675007 4 0.0 9.657311 Working With the Data >>> loaded_data . columns # This prints the column headers Index ([ 'importer' , 'exporter' , 'year' , 'trade_value' , 'agree_pta' , 'common_language' , 'contiguity' , 'log_distance' ], dtype = 'object' ) >>> loaded_data . shape # Returns the dimensions of the data ( 98612 , 8 ) >>> loaded_data . info () # Returns some useful info such as dtypes, memory usage, dimensions. < class ' pandas . core . frame . DataFrame '> RangeIndex : 98612 entries , 0 to 98611 Data columns ( total 8 columns ): importer 98612 non - null object exporter 98612 non - null object year 98612 non - null int64 trade_value 98612 non - null float64 agree_pta 97676 non - null float64 common_language 97676 non - null float64 contiguity 97676 non - null float64 log_distance 97676 non - null float64 dtypes : float64 ( 5 ), int64 ( 1 ), object ( 2 ) memory usage : 6.0 + MB >>> loaded_data . describe () # Some basic summary stats of the variables year trade_value agree_pta common_language \\ count 98612.000000 9.861200e+04 97676.000000 97676.000000 mean 2002.210441 1.856316e+09 0.381547 0.380646 std 7.713050 1.004735e+10 0.485769 0.485548 min 1989.000000 0.000000e+00 0.000000 0.000000 25 % 1996.000000 1.084703e+06 0.000000 0.000000 50 % 2002.000000 6.597395e+07 0.000000 0.000000 75 % 2009.000000 6.125036e+08 1.000000 1.000000 max 2015.000000 4.977686e+11 1.000000 1.000000 contiguity log_distance count 97676.000000 97676.000000 mean 0.034051 8.722631 std 0.181362 0.818818 min 0.000000 5.061335 25 % 0.000000 8.222970 50 % 0.000000 9.012502 75 % 0.000000 9.303026 max 1.000000 9.890765 >>> loaded_data [ 'importer' ] # The brackets ( [] ) are used to index or \"slice\" the data. In this case, ... # it returns the entire \"reporter_iso3\" column. 0 AUS 1 AUS 2 AUS ... 98609 VEN 98610 VEN 98611 VEN Name : importer , Length : 98612 , dtype : object >>> loaded_data . loc [ 3 , \"importer\" ] # The command \".loc\" can be used to identify specific row and column indexes 'AUS' # Suppose we wanted to figure out what countries are present as exporting partners in the data: >>> partner_countries = loaded_data [ 'exporter' ] . unique () >>> num_of_partners = len ( partner_countries ) >>> print ( num_of_partners ) 62 # Suppose we wanted to summarize the data by country >>> trade_data_temp = loaded_data . groupby ( 'importer' ) # Creates a modified dataframe where observations # are grouped by reporter (and saved to a new object to avoid over-writting the original >>> imports_summary = trade_data_temp [ 'trade_value' ] . agg ([ 'mean' , 'max' , 'count' ]) # this returns a DataFrame with the # desired stats for each 'group'. >>> imports_summary = imports_summary . reset_index () # By default, the groups get turned into index labels. the reset_index() # command turns them back into a column, which is generally useful. # Suppose we wanted to drop the distance variable. >>> trade_data = loaded_data . drop ( 'log_distance' , axis = 1 ) # This drops the column (axis = 1) 'log_distance', # or alternatively, suppose we wanted to keep only the trade data and identifiers. >>> trade_data = loaded_data [[ 'importer' , 'exporter' , 'year' , 'trade_value' ]] # would do the same # thing by returning only the specified list of columns. #Perhaps we want to rename a column >>> trade_data = trade_data . rename ( columns = { 'trade_value' : 'bilateral_trade' }) # How can we deal with missing values? >>> trade_data = trade_data . dropna ( how = 'any' ) # This drops 'n/a' rows if they appear in 'any' column. >>> loaded_data . shape , trade_data . shape # Check to see if we lost any rows. (( 98612 , 8 ), ( 84924 , 4 )) # How can we subset the data based on a condition? >>> trade_data = trade_data [ trade_data [ 'year' ] > 1992 ] # suppose we wanted to save this modified trade dataset. trade_data . to_csv ( \"C:Documents \\\\ modified_trade_data.csv\" ) # Lets now create a dataset of only the gravity variables >>> gravity_data = loaded_data [[ 'importer' , 'exporter' , 'year' , 'agree_pta' , ... 'common_language' , 'contiguity' , 'log_distance' ]] # Next, lets merge the gravity data back onto to the trade data >>> estimation_data = pd . merge ( left = trade_data , # left: the first of two datasets to merge ... right = gravity_data , # right: the second dataset, ... how = 'left' , # this specifies the type of merge ('left', 'right', 'inner', 'outer') ... left_on = [ 'importer' , 'exporter' , 'year' ], ... # A list of columns to merge on in the left dataset ... right_on = [ 'importer' , 'exporter' , 'year' ] # A list of columns to merge on in the right dataset ... ) # Note that the right_on and left_on lists should be in corresponding orders. # Merge Types: # 'inner' - retains only rows in which their are matches in both dataframes (i.e. _merge = 3 in stata) # 'outer' - retains all rows from both dataframes, even if they did not match (i.e. _merge = 3 & 1 & 2) # 'left' - retains all rows from the left dataframe, with or without matches (i.e. _merge = 3 & 1) # 'right' - retains all rows from the right dataframe, with or without matches (i.e. _merge = 3 & 2) >>> estimation_data . head ( 5 ) importer exporter year bilateral_trade agree_pta common_language \\ 0 ARG AUS 1993 7.353949e+07 0.0 1.0 1 ARG BOL 1993 1.077911e+08 1.0 1.0 2 ARG BRA 1993 3.566469e+09 1.0 1.0 3 ARG CAN 1993 8.783576e+07 0.0 1.0 4 ARG CHE 1993 1.639864e+08 0.0 1.0 contiguity log_distance 0 0.0 9.398793 1 1.0 7.536893 2 1.0 7.783794 3 0.0 9.152224 4 0.0 9.327306 >>> estimation_data = estimation_data . dropna ( how = 'any' ) # drop any rows missing gravity data ``` python # Our dataset contains some zero trade flows, lets drop them >>> positive_trade_rows = estimation_data [ 'bilateral_trade' ] != 0 # Identify rows with positive trade >>> estimation_data = estimation_data . loc [ positive_trade_rows ,:] # This returns the rows identified in the previous steps # and all columns (:). # Create a new variable with log trade >>> estimation_data [ 'log_trade' ] = np . log ( estimation_data [ 'bilateral_trade' ]) # Create Dummy variables >>> importer_fixed_effects = pd . get_dummies ( data = estimation_data [ 'importer' ], prefix = 'imp_fe' ) # data specifies what # column should be used as the ID for dummies, 'prefix' is an option that lets you name the fixed effects >>> importer_fixed_effects . head ( 1 ) imp_fe_ARG imp_fe_AUS imp_fe_AUT imp_fe_BEL imp_fe_BOL imp_fe_BRA \\ 0 1 0 0 0 0 0 imp_fe_CAN imp_fe_CHE imp_fe_CHL imp_fe_CHN ... imp_fe_SDN \\ 0 0 0 0 0 ... 0 imp_fe_SGP imp_fe_SWE imp_fe_THA imp_fe_TUN imp_fe_TUR imp_fe_URY \\ 0 0 0 0 0 0 0 imp_fe_USA imp_fe_VEN imp_fe_ZAF 0 0 0 0 [ 1 rows x 61 columns ] >>> exporter_fixed_effects = pd . get_dummies ( estimation_data [ 'exporter' ], prefix = 'exp_fe' ) # repeat for exporters >>> exporter_fixed_effects = exporter_fixed_effects . drop ( 'exp_fe_ZAF' , axis = 1 ) # drop a column to avoid collinearity # Split the data into right side and left side variables >>> lhs_data = estimation_data [[ 'log_trade' ]] >>> rhs_data = estimation_data [[ 'log_distance' , 'contiguity' , 'agree_pta' , 'common_language' ]] # Add the dummies to the right hand variables >>> rhs_data = pd . concat ( objs = [ rhs_data , importer_fixed_effects , exporter_fixed_effects ], axis = 1 ) # concatinate # is a method of combining two datasets that does not rely on merging/joining on keys, it does so on rows or columns # 'objs' is a list of DataFrames to concatinate (they need to have the same row or column IDs as each other) # 'axis' specifies whether you want to concatinate rows or colomns. axis = 1 specifies that the columns are to be # combined (i.e. horizontal combining) >>> rhs_data . columns Index ([ 'log_distance' , 'contiguity' , 'agree_pta' , 'common_language' , 'imp_fe_ARG' , 'imp_fe_AUS' , 'imp_fe_AUT' , 'imp_fe_BEL' , 'imp_fe_BOL' , 'imp_fe_BRA' , ... 'exp_fe_SDN' , 'exp_fe_SGP' , 'exp_fe_SWE' , 'exp_fe_THA' , 'exp_fe_TUN' , 'exp_fe_TUR' , 'exp_fe_TWN' , 'exp_fe_URY' , 'exp_fe_USA' , 'exp_fe_VEN' ], dtype = 'object' , length = 126 ) Estimation >>> ols_model = sm . OLS ( lhs_data , rhs_data ) # This step is a little odd. It creates a python \"regression object\", to which # various other OLS fuctions can be applied >>> ols_results = ols_model . fit () # .fit() is probably the most important function as it returns an object with # the estimated results. >>> ols_results . summary () # returns the results in a standard format < class ' statsmodels . iolib . summary . Summary '> \"\"\" OLS Regression Results ============================================================================== Dep. Variable: log_trade R-squared: 0.760 Model: OLS Adj. R-squared: 0.760 Method: Least Squares F-statistic: 1882. Date: Wed, 01 Aug 2018 Prob (F-statistic): 0.00 Time: 12:31:34 Log-Likelihood: -1.3837e+05 No. Observations: 74384 AIC: 2.770e+05 Df Residuals: 74258 BIC: 2.781e+05 Df Model: 125 Covariance Type: nonrobust =================================================================================== coef std err t P>|t| [0.025 0.975] ----------------------------------------------------------------------------------- log_distance -1.1302 0.012 -96.040 0.000 -1.153 -1.107 contiguity 0.2422 0.037 6.582 0.000 0.170 0.314 agree_pta 0.1996 0.016 12.687 0.000 0.169 0.230 common_language 0.6633 0.018 37.904 0.000 0.629 0.698 imp_fe_ARG 28.3824 0.135 210.554 0.000 28.118 28.647 ... ... ... ... ... ... ... (truncated for the tutorial) exp_fe_USA 3.2536 0.062 52.103 0.000 3.131 3.376 exp_fe_VEN -2.2117 0.063 -35.183 0.000 -2.335 -2.088 ============================================================================== Omnibus: 13854.709 Durbin-Watson: 1.701 Prob(Omnibus): 0.000 Jarque-Bera (JB): 70317.321 Skew: -0.814 Prob(JB): 0.00 Kurtosis: 7.476 Cond. No. 1.46e+03 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 1.46e+03. This might indicate that there are strong multicollinearity or other numerical problems. \"\"\" >>> ols_results . params # returns only the parameter values log_distance - 1.130207 contiguity 0.242191 agree_pta 0.199577 common_language 0.663323 imp_fe_ARG 28.382397 imp_fe_AUS 29.955248 ... exp_fe_USA 3.253590 exp_fe_VEN - 2.211676 Length : 126 , dtype : float64 >>> ols_results . rsquared # The r squared values 0.7600680070806417 >>> ols_results . HC1_se # Huber/White standard errors. You could also return this originally with ols_model.fit(cov_type = 'HC1') log_distance 0.011628 contiguity 0.036585 agree_pta 0.016563 common_language 0.017452 imp_fe_ARG 0.129097 imp_fe_AUS 0.125993 ... exp_fe_USA 0.038809 exp_fe_VEN 0.069779 Length : 126 , dtype : float64 ols_results . pvalues # pvalueslog_distance 0.000000e+00 contiguity 4.682417e-11 agree_pta 7.591490e-37 common_language 0.000000e+00 imp_fe_ARG 0.000000e+00 imp_fe_AUS 0.000000e+00 ... exp_fe_USA 0.000000e+00 exp_fe_VEN 6.068582e-269 Length : 126 , dtype : float64 Saving the results There are many different way to export results including writing tables to a text file, saving estimates in a csv dataset, storing the python objects themselves. In this tutorial, we will look at the first two options. For the third option, see the pickle package. To write the results to a text file, you need to 'open' a text file on the hard drive, write the results to the file, and close the file. >>> text_file = open ( \"C: \\\\ Documents \\\\ regression_results.txt\" , \"w\" ) >>> text_file . write ( ols_results . summary () . as_text ()) >>> text_file . close () Alternatively, you could export some of these results to excel. >>> type ( ols_results . params ) # Statsmodels conveniently returns Pandas Objects, so we can feed them into a dataframe < class ' pandas . core . series . Series '> >>> results_dataframe = pd . concat ( ... { 'coefficients' : ols_results . params , 'SE' : ols_results . HC1_se , 'p_values' : ols_results . pvalues }, axis = 1 ) >>> results_dataframe . head ( 5 ) SE coefficients p_values log_distance 0.011628 - 1.130207 0.000000e+00 contiguity 0.036585 0.242191 4.682417e-11 agree_pta 0.016563 0.199577 7.591490e-37 common_language 0.017452 0.663323 0.000000e+00 imp_fe_ARG 0.129097 28.382397 0.000000e+00 >>> results_dataframe . to_csv ( \"C: \\\\ Documents \\\\ regression_results.csv\" )","title":"Introduction to Python"},{"location":"python_intro_tutorial/#introduction-to-python","text":"This tutorial demonstrates some basic python concepts and syntax that are integral for working with the GME package. Additionally, it demonstrates aspects of the pandas package, which provides a collection of data tools that are both a major part of the GME tools and exceptionally useful for data analysis in Python. This tutorial is far from comprehensive, even when it comes to basic usage, but should help you become sufficiently well versed to begin working with the GME package.","title":"Introduction to Python"},{"location":"python_intro_tutorial/#basic-python-concepts-and-syntax","text":"","title":"Basic Python Concepts and Syntax"},{"location":"python_intro_tutorial/#computation","text":"One of the most fundamental processes is basic arithmetic. >>> 2 + 2 4 >>> 3 * 5 15 >>> ( 3 * 10 ) / 16 + 14 15.875","title":"Computation"},{"location":"python_intro_tutorial/#variables-and-types","text":"Variables can be used to store information for further interaction. >>> a = 16 # The variable you want to assign is on the left side, the value you want to set it to is on the right. >>> b = 10 >>> c = a * b >>> print ( a , b , c ) 16 10 160 >>> b = 25 >>> c = a * b >>> print ( a , b , c ) 16 25 400 Variables and data more broadly are much more expansive than just integers. The following is a non-exhaustive list of the data types you will frequently use. Integers : ( int ) A simple number without decimal points. # The type() function returns the data type of the argument. >>> type ( 12 ) < class ' int '> >>> type ( a ) < class ' int '> Float : ( float ) A number with decimal places. >>> type ( 3.25 ) < class ' float '> >>> type ( 2 ** ( . 5 )) < class ' float '> # you can also specify the type you would like a value to take >>> type ( float ( a )) < class ' float '> >>> print ( a ) 16.0 String : ( str ) Non-numeric characters. Strings are denoted by wrapping characters in either single or double quotes (e.g. ' ' or \" \"). >>> str_example = 'Hello World!' >>> print ( str_example ) Hello World >>> type ( str_example ) < class ' str '> >>> type ( \"12\" ) < class ' str '> >>> type ( 'a' ) < class ' str '> # We can also combine (concatenate) strings >>> print ( \"12\" + \"14\" ) 1214 Boolean : ( bool ) A binary type that equals True or False >>> boolean_example = True >>> type ( boolean_example ) < class ' bool '> List : ( list ) A list contains a series of values and, potentially, types. They are created using square brackets (e.g. [ ] ) >>> list_example = [ 1 , 12 , 25 , 37 , 22 ] >>> type ( list_example ) < class ' list '> # They are indexed >>> list_example [ 0 ] # Square brackets denote indexes in a list. i.e. the zeroth element of list_example in this case 1 >>> list_example [ 3 ] 37 >>> list_example [ 1 : 3 ] # and sliceable [ 12 , 25 ] # They can also contain other data types >>> list_example_2 = [ \"This\" , \"list\" , \"contains\" , 'strings' , float ( 22.134 ), a , b , c ] >>> print ( list_example_2 ) [ 'This' , 'list' , 'contains' , 'strings' , 22.134 , 16 , 25 , 400 ] >>> type ( list_example_2 [ 0 ]) < class ' str '> >>> type ( list_example_2 [ 7 ]) < class ' int '> # They can be combined >>> list_example_3 = list_example + list_example_2 >>> list_example_3 [ 1 , 12 , 25 , 37 , 22 , 'This' , 'list' , 'contains' , 'strings' , 22.134 , 16 , 25 , 400 ] # They can be added to >>> list_example . append ( 'last' ) >>> print ( list_example ) [ 1 , 12 , 25 , 37 , 22 , 'last' ] # and can be counted/measured >>> len ( list_example_3 ) 13 Tuple : ( tuple ) A tuple is similar to a list except that it's size and order is fixed. Many of the other features hold from lists but the inability to change the shape or order may offer some desirable protections in your code. They are declared using parentheses (e.g. ( ) ). >>> tuple_example = ( 'Hello' , 25 , a , c ) >>> type ( tuple_example ) < class ' tuple '> >>> tuple_example [ 2 ] 16 >>> tuple_example . append ( 'new element' ) ''' Traceback (most recent call last): File \"<input>\", line 1, in <module> AttributeError: 'tuple' object has no attribute 'append' ''' Dictionary : ( dict ) A dictionary is a very useful datatype which, like a list or tuple, can contain a series of values. However, it has the added feature that elements are 'keyed', such that each value is associated with a key. # The dictionary is declared using curly braces (e.g. { } ). Within the dictionary, a key (str) is followed by a colon then the value to assign to that key (e.g. key:value = 'first':'peter') >>> dictionary_example = { 'first' : 'Karl' , 'last' : 'Marx' , 'job' : 'economist' , 'ext' : 3186 } >>> print ( dictionary_example ) { 'first' : 'Karl' , 'last' : 'Marx' , 'job' : 'economist' , 'ext' : 3186 , 'location' : { 'city' : 'Washington' , 'state' : 'DC' }} >>> type ( dictionary_example ) < class ' dict '> # Dictionaries are indexed using the keys. >>> dictionary_example [ 'last' ] 'Marx' # Can be added to by creating a new key. >>> dictionary_example [ 'location' ] = { 'city' : 'Washington' , 'state' : 'DC' } >>> print ( dictionary_example ) { 'first' : 'Karl' , 'last' : 'Marx' , 'job' : 'economist' , 'ext' : 3186 , 'location' : { 'city' : 'Washington' , 'state' : 'DC' }} >>> dictionary_example [ 'location' ][ 'city' ] # chained indexing 'Washington' # The keys in a dictionary can be retrieved in the following way: >>> dictionary_example . keys () dict_keys ([ 'first' , 'last' , 'job' , 'ext' , 'location' ])","title":"Variables and  Types"},{"location":"python_intro_tutorial/#logical-operators-and-decision-tests","text":"Logical tests are a way of comparing things and performing conditional operations # Lets create some initial (int) values to work with >>> x = 10 >>> y = 10 >>> z = 5 >>> x == y # '==' tests the equivalence of the left and right side True >>> x != y # '!=' tests the non-equivalence of both sides False >>> x == ( y and z ) # 'and' tests the equivalence to both y *AND* z False >>> x == ( y or z ) # 'or' tests the equivalence to y *OR* z True >>> x > z # inequalities work as expected with '>', '<', '>=', and '<=' True >>> x <= y True # Logical operators are especially useful with if statements >>> x = 20 >>> if x > 11 : ... print ( 'x is bigger than 11' ) x is not bigger than 11 # or with 'else' statements >>> x = 10 >>> if x > 11 : ... print ( 'x is bigger than 11' ) ... else : ... print ( 'x is not bigger than 11' ) x is not bigger than 11 In Python, statements are grouped by offsetting commands using tabs. Unlike many other languages that wrap statements like if with particular syntax like parentheses, braces, or specific begin and end statements, Python interprets things entirely based on tab indentation. For example, a collection of commands belonging to a statement within another statement should be preceded by two tabs.","title":"Logical Operators and Decision Tests"},{"location":"python_intro_tutorial/#iteration","text":"One thing you will find very useful is the ability to automate and perform routine, repeating tasks. For loops are a useful tool for these tasks because they perform an action for each value in an 'iterable' object (e.g. a list or tuple ) >>> iteration_sequence = [ 0 , 1 , 2 , 3 , 4 ] >>> for i in iteration_sequence : # i gives the iteration value, iteration_sequence gives a (list) of values to iterate over, ... print ( i ) # : begins the loop. a tab indent indicates commands to be interpreted in the loop. # Unlike many languages, there is no closing statement for the loop, the indentation # fully declares the scope of the command. 0 1 2 3 4 # Suppose we wanted to use this to make some calculation (e.g. a Fibonacci Sequence) >>> fibonacci_sequence = [ 1 , 2 ] >>> for i in range ( 2 , 20 ): ... next_number = fibonacci_sequence [ i - 1 ] + fibonacci_sequence [ i - 2 ] # This adds the previous two entries in the (list) ... fibonacci_sequence . append ( next_number ) # this adds the new number to the end of the list >>> print ( fibonacci_sequence ) [ 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , 89 , 144 , 233 , 377 , 610 , 987 , 1597 , 2584 , 4181 , 6765 , 10946 ] # You can also iterate on elements rather than sequential numbers >>> dictionary_serge = { 'first' : 'Adam' , 'last' : 'Smith' , 'job' : 'economist' , 'location' :{ 'city' : 'Arlington' , 'state' : 'VA' }} >>> dictionary_list = [ dictionary_example , dictionary_serge ] # This creates a (list) of two (dictionaries) >>> for person in dictionary_list : # This time person is a dictionary rather than a simple index, and can be used that way ... print ( 'The ' + person [ 'job' ] + ' ' + person [ 'first' ] + ' ' + person [ 'last' ] ... + ' lives in ' + person [ 'location' ][ 'city' ]) ... if person [ 'location' ][ 'state' ] != 'DC' : ... person [ 'statehood' ] = True ... else : ... person [ 'statehood' ] = False The economist Karl Marx lives in Washington The economist Adam Smith lives in Arlington >>> print ( dictionary_list [ 0 ][ 'statehood' ], dictionary_list [ 1 ][ 'statehood' ]) False True","title":"Iteration"},{"location":"python_intro_tutorial/#functions","text":"Functions provide a way of packaging and generalizing a routine task. A function is defined using the def command # Lets work on generalizing the fibonacci calculation >>> def fibonacci_nums ( sequence_length ): # 'def' tells python that you are about to define a function ... # 'fibonacci_nums' is the name of the fucntion ... # 'sequence_length' is an input to be supplied to the function ... fibonacci_sequence = [ 1 , 2 ] ... for i in range ( 2 , sequence_length ): ... next_number = fibonacci_sequence [ i - 1 ] + fibonacci_sequence [ i - 2 ] # This adds the previous two entries in the (list) ... fibonacci_sequence . append ( next_number ) # this adds the new number to the end of the list ... return fibonacci_sequence # this determines what the function returns. In this case it is a list of fibonacci numbers >>> fibonacci_nums ( 30 ) # Lets test the new function [ 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , 89 , 144 , 233 , 377 , 610 , 987 , 1597 , 2584 , 4181 , 6765 , 10946 , 17711 , 28657 , 46368 , 75025 , 121393 , 196418 , 317811 , 514229 , 832040 , 1346269 ]","title":"Functions"},{"location":"python_intro_tutorial/#data-manipulation-and-regression-analysis","text":"The following with provide a basic crash course in using python to perform standard econometric/data analysis tasks. Our reccomendation for this type of work is to use the pandas package, which contains data manipulation tools, and the statsmodels package, which contains statistical tools. For users familiar with R, you should find the syntax and programming conventions of these packages very similar to those in R.","title":"Data Manipulation and Regression Analysis"},{"location":"python_intro_tutorial/#importing-tools","text":"By default, python does not have a ton of tools. Fortunately, others have written tons of them. For econometrics, we will be extensively using two: pandas, which is a data manipulation toolbox, and statsmodels, which is a statistical package (numpy, which a package for arrays, is useful too). >>> import pandas as pd # The 'pd' gives a prefix that will be used to identify functions as coming from that package/module >>> import statsmodels.api as sm >>> import numpy as np","title":"Importing tools"},{"location":"python_intro_tutorial/#load-data","text":"Python (or, more accurately, pandas) has a robust set of tools for loading data in in a wide range of formats such as .csv, .dta (stata), or .xls (excel). For the sake of this tutorial, we will be loading some data from the internet via a URL. Most of the time, you will probably be loading data from a file location of your computer, which uses the same synatx with a file path replacing the URL. >>> loaded_trade_data = pd . read_csv ( ... 'https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv' ) # pd.read_csv() has created a Pandas DataFrame, which is the object that contains the data. # There are also read_excel, read_sql, read_stata, etc. versions of this function for a wide range of data file types. # To view the data, there are several options: >>> loaded_trade_data . head ( 5 ) # This prints the first 5 lines of the Dataframe importer exporter year trade_value agree_pta common_language \\ 0 AUS BRA 1989 3.035469e+08 0.0 1.0 1 AUS CAN 1989 8.769946e+08 0.0 1.0 2 AUS CHE 1989 4.005245e+08 0.0 1.0 3 AUS DEU 1989 2.468977e+09 0.0 0.0 4 AUS DNK 1989 1.763072e+08 0.0 1.0 contiguity log_distance 0 0.0 9.553332 1 0.0 9.637676 2 0.0 9.687557 3 0.0 9.675007 4 0.0 9.657311","title":"Load Data"},{"location":"python_intro_tutorial/#working-with-the-data","text":">>> loaded_data . columns # This prints the column headers Index ([ 'importer' , 'exporter' , 'year' , 'trade_value' , 'agree_pta' , 'common_language' , 'contiguity' , 'log_distance' ], dtype = 'object' ) >>> loaded_data . shape # Returns the dimensions of the data ( 98612 , 8 ) >>> loaded_data . info () # Returns some useful info such as dtypes, memory usage, dimensions. < class ' pandas . core . frame . DataFrame '> RangeIndex : 98612 entries , 0 to 98611 Data columns ( total 8 columns ): importer 98612 non - null object exporter 98612 non - null object year 98612 non - null int64 trade_value 98612 non - null float64 agree_pta 97676 non - null float64 common_language 97676 non - null float64 contiguity 97676 non - null float64 log_distance 97676 non - null float64 dtypes : float64 ( 5 ), int64 ( 1 ), object ( 2 ) memory usage : 6.0 + MB >>> loaded_data . describe () # Some basic summary stats of the variables year trade_value agree_pta common_language \\ count 98612.000000 9.861200e+04 97676.000000 97676.000000 mean 2002.210441 1.856316e+09 0.381547 0.380646 std 7.713050 1.004735e+10 0.485769 0.485548 min 1989.000000 0.000000e+00 0.000000 0.000000 25 % 1996.000000 1.084703e+06 0.000000 0.000000 50 % 2002.000000 6.597395e+07 0.000000 0.000000 75 % 2009.000000 6.125036e+08 1.000000 1.000000 max 2015.000000 4.977686e+11 1.000000 1.000000 contiguity log_distance count 97676.000000 97676.000000 mean 0.034051 8.722631 std 0.181362 0.818818 min 0.000000 5.061335 25 % 0.000000 8.222970 50 % 0.000000 9.012502 75 % 0.000000 9.303026 max 1.000000 9.890765 >>> loaded_data [ 'importer' ] # The brackets ( [] ) are used to index or \"slice\" the data. In this case, ... # it returns the entire \"reporter_iso3\" column. 0 AUS 1 AUS 2 AUS ... 98609 VEN 98610 VEN 98611 VEN Name : importer , Length : 98612 , dtype : object >>> loaded_data . loc [ 3 , \"importer\" ] # The command \".loc\" can be used to identify specific row and column indexes 'AUS' # Suppose we wanted to figure out what countries are present as exporting partners in the data: >>> partner_countries = loaded_data [ 'exporter' ] . unique () >>> num_of_partners = len ( partner_countries ) >>> print ( num_of_partners ) 62 # Suppose we wanted to summarize the data by country >>> trade_data_temp = loaded_data . groupby ( 'importer' ) # Creates a modified dataframe where observations # are grouped by reporter (and saved to a new object to avoid over-writting the original >>> imports_summary = trade_data_temp [ 'trade_value' ] . agg ([ 'mean' , 'max' , 'count' ]) # this returns a DataFrame with the # desired stats for each 'group'. >>> imports_summary = imports_summary . reset_index () # By default, the groups get turned into index labels. the reset_index() # command turns them back into a column, which is generally useful. # Suppose we wanted to drop the distance variable. >>> trade_data = loaded_data . drop ( 'log_distance' , axis = 1 ) # This drops the column (axis = 1) 'log_distance', # or alternatively, suppose we wanted to keep only the trade data and identifiers. >>> trade_data = loaded_data [[ 'importer' , 'exporter' , 'year' , 'trade_value' ]] # would do the same # thing by returning only the specified list of columns. #Perhaps we want to rename a column >>> trade_data = trade_data . rename ( columns = { 'trade_value' : 'bilateral_trade' }) # How can we deal with missing values? >>> trade_data = trade_data . dropna ( how = 'any' ) # This drops 'n/a' rows if they appear in 'any' column. >>> loaded_data . shape , trade_data . shape # Check to see if we lost any rows. (( 98612 , 8 ), ( 84924 , 4 )) # How can we subset the data based on a condition? >>> trade_data = trade_data [ trade_data [ 'year' ] > 1992 ] # suppose we wanted to save this modified trade dataset. trade_data . to_csv ( \"C:Documents \\\\ modified_trade_data.csv\" ) # Lets now create a dataset of only the gravity variables >>> gravity_data = loaded_data [[ 'importer' , 'exporter' , 'year' , 'agree_pta' , ... 'common_language' , 'contiguity' , 'log_distance' ]] # Next, lets merge the gravity data back onto to the trade data >>> estimation_data = pd . merge ( left = trade_data , # left: the first of two datasets to merge ... right = gravity_data , # right: the second dataset, ... how = 'left' , # this specifies the type of merge ('left', 'right', 'inner', 'outer') ... left_on = [ 'importer' , 'exporter' , 'year' ], ... # A list of columns to merge on in the left dataset ... right_on = [ 'importer' , 'exporter' , 'year' ] # A list of columns to merge on in the right dataset ... ) # Note that the right_on and left_on lists should be in corresponding orders. # Merge Types: # 'inner' - retains only rows in which their are matches in both dataframes (i.e. _merge = 3 in stata) # 'outer' - retains all rows from both dataframes, even if they did not match (i.e. _merge = 3 & 1 & 2) # 'left' - retains all rows from the left dataframe, with or without matches (i.e. _merge = 3 & 1) # 'right' - retains all rows from the right dataframe, with or without matches (i.e. _merge = 3 & 2) >>> estimation_data . head ( 5 ) importer exporter year bilateral_trade agree_pta common_language \\ 0 ARG AUS 1993 7.353949e+07 0.0 1.0 1 ARG BOL 1993 1.077911e+08 1.0 1.0 2 ARG BRA 1993 3.566469e+09 1.0 1.0 3 ARG CAN 1993 8.783576e+07 0.0 1.0 4 ARG CHE 1993 1.639864e+08 0.0 1.0 contiguity log_distance 0 0.0 9.398793 1 1.0 7.536893 2 1.0 7.783794 3 0.0 9.152224 4 0.0 9.327306 >>> estimation_data = estimation_data . dropna ( how = 'any' ) # drop any rows missing gravity data ``` python # Our dataset contains some zero trade flows, lets drop them >>> positive_trade_rows = estimation_data [ 'bilateral_trade' ] != 0 # Identify rows with positive trade >>> estimation_data = estimation_data . loc [ positive_trade_rows ,:] # This returns the rows identified in the previous steps # and all columns (:). # Create a new variable with log trade >>> estimation_data [ 'log_trade' ] = np . log ( estimation_data [ 'bilateral_trade' ]) # Create Dummy variables >>> importer_fixed_effects = pd . get_dummies ( data = estimation_data [ 'importer' ], prefix = 'imp_fe' ) # data specifies what # column should be used as the ID for dummies, 'prefix' is an option that lets you name the fixed effects >>> importer_fixed_effects . head ( 1 ) imp_fe_ARG imp_fe_AUS imp_fe_AUT imp_fe_BEL imp_fe_BOL imp_fe_BRA \\ 0 1 0 0 0 0 0 imp_fe_CAN imp_fe_CHE imp_fe_CHL imp_fe_CHN ... imp_fe_SDN \\ 0 0 0 0 0 ... 0 imp_fe_SGP imp_fe_SWE imp_fe_THA imp_fe_TUN imp_fe_TUR imp_fe_URY \\ 0 0 0 0 0 0 0 imp_fe_USA imp_fe_VEN imp_fe_ZAF 0 0 0 0 [ 1 rows x 61 columns ] >>> exporter_fixed_effects = pd . get_dummies ( estimation_data [ 'exporter' ], prefix = 'exp_fe' ) # repeat for exporters >>> exporter_fixed_effects = exporter_fixed_effects . drop ( 'exp_fe_ZAF' , axis = 1 ) # drop a column to avoid collinearity # Split the data into right side and left side variables >>> lhs_data = estimation_data [[ 'log_trade' ]] >>> rhs_data = estimation_data [[ 'log_distance' , 'contiguity' , 'agree_pta' , 'common_language' ]] # Add the dummies to the right hand variables >>> rhs_data = pd . concat ( objs = [ rhs_data , importer_fixed_effects , exporter_fixed_effects ], axis = 1 ) # concatinate # is a method of combining two datasets that does not rely on merging/joining on keys, it does so on rows or columns # 'objs' is a list of DataFrames to concatinate (they need to have the same row or column IDs as each other) # 'axis' specifies whether you want to concatinate rows or colomns. axis = 1 specifies that the columns are to be # combined (i.e. horizontal combining) >>> rhs_data . columns Index ([ 'log_distance' , 'contiguity' , 'agree_pta' , 'common_language' , 'imp_fe_ARG' , 'imp_fe_AUS' , 'imp_fe_AUT' , 'imp_fe_BEL' , 'imp_fe_BOL' , 'imp_fe_BRA' , ... 'exp_fe_SDN' , 'exp_fe_SGP' , 'exp_fe_SWE' , 'exp_fe_THA' , 'exp_fe_TUN' , 'exp_fe_TUR' , 'exp_fe_TWN' , 'exp_fe_URY' , 'exp_fe_USA' , 'exp_fe_VEN' ], dtype = 'object' , length = 126 )","title":"Working With the Data"},{"location":"python_intro_tutorial/#estimation","text":">>> ols_model = sm . OLS ( lhs_data , rhs_data ) # This step is a little odd. It creates a python \"regression object\", to which # various other OLS fuctions can be applied >>> ols_results = ols_model . fit () # .fit() is probably the most important function as it returns an object with # the estimated results. >>> ols_results . summary () # returns the results in a standard format < class ' statsmodels . iolib . summary . Summary '> \"\"\" OLS Regression Results ============================================================================== Dep. Variable: log_trade R-squared: 0.760 Model: OLS Adj. R-squared: 0.760 Method: Least Squares F-statistic: 1882. Date: Wed, 01 Aug 2018 Prob (F-statistic): 0.00 Time: 12:31:34 Log-Likelihood: -1.3837e+05 No. Observations: 74384 AIC: 2.770e+05 Df Residuals: 74258 BIC: 2.781e+05 Df Model: 125 Covariance Type: nonrobust =================================================================================== coef std err t P>|t| [0.025 0.975] ----------------------------------------------------------------------------------- log_distance -1.1302 0.012 -96.040 0.000 -1.153 -1.107 contiguity 0.2422 0.037 6.582 0.000 0.170 0.314 agree_pta 0.1996 0.016 12.687 0.000 0.169 0.230 common_language 0.6633 0.018 37.904 0.000 0.629 0.698 imp_fe_ARG 28.3824 0.135 210.554 0.000 28.118 28.647 ... ... ... ... ... ... ... (truncated for the tutorial) exp_fe_USA 3.2536 0.062 52.103 0.000 3.131 3.376 exp_fe_VEN -2.2117 0.063 -35.183 0.000 -2.335 -2.088 ============================================================================== Omnibus: 13854.709 Durbin-Watson: 1.701 Prob(Omnibus): 0.000 Jarque-Bera (JB): 70317.321 Skew: -0.814 Prob(JB): 0.00 Kurtosis: 7.476 Cond. No. 1.46e+03 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 1.46e+03. This might indicate that there are strong multicollinearity or other numerical problems. \"\"\" >>> ols_results . params # returns only the parameter values log_distance - 1.130207 contiguity 0.242191 agree_pta 0.199577 common_language 0.663323 imp_fe_ARG 28.382397 imp_fe_AUS 29.955248 ... exp_fe_USA 3.253590 exp_fe_VEN - 2.211676 Length : 126 , dtype : float64 >>> ols_results . rsquared # The r squared values 0.7600680070806417 >>> ols_results . HC1_se # Huber/White standard errors. You could also return this originally with ols_model.fit(cov_type = 'HC1') log_distance 0.011628 contiguity 0.036585 agree_pta 0.016563 common_language 0.017452 imp_fe_ARG 0.129097 imp_fe_AUS 0.125993 ... exp_fe_USA 0.038809 exp_fe_VEN 0.069779 Length : 126 , dtype : float64 ols_results . pvalues # pvalueslog_distance 0.000000e+00 contiguity 4.682417e-11 agree_pta 7.591490e-37 common_language 0.000000e+00 imp_fe_ARG 0.000000e+00 imp_fe_AUS 0.000000e+00 ... exp_fe_USA 0.000000e+00 exp_fe_VEN 6.068582e-269 Length : 126 , dtype : float64","title":"Estimation"},{"location":"python_intro_tutorial/#saving-the-results","text":"There are many different way to export results including writing tables to a text file, saving estimates in a csv dataset, storing the python objects themselves. In this tutorial, we will look at the first two options. For the third option, see the pickle package. To write the results to a text file, you need to 'open' a text file on the hard drive, write the results to the file, and close the file. >>> text_file = open ( \"C: \\\\ Documents \\\\ regression_results.txt\" , \"w\" ) >>> text_file . write ( ols_results . summary () . as_text ()) >>> text_file . close () Alternatively, you could export some of these results to excel. >>> type ( ols_results . params ) # Statsmodels conveniently returns Pandas Objects, so we can feed them into a dataframe < class ' pandas . core . series . Series '> >>> results_dataframe = pd . concat ( ... { 'coefficients' : ols_results . params , 'SE' : ols_results . HC1_se , 'p_values' : ols_results . pvalues }, axis = 1 ) >>> results_dataframe . head ( 5 ) SE coefficients p_values log_distance 0.011628 - 1.130207 0.000000e+00 contiguity 0.036585 0.242191 4.682417e-11 agree_pta 0.016563 0.199577 7.591490e-37 common_language 0.017452 0.663323 0.000000e+00 imp_fe_ARG 0.129097 28.382397 0.000000e+00 >>> results_dataframe . to_csv ( \"C: \\\\ Documents \\\\ regression_results.csv\" )","title":"Saving the results"},{"location":"tutorials/","text":"Tutorials The following tutorials are meant to introduce users to the gme package and help them become proficcient with many of the features that it offers. Tutorial 1 - Estimating a Gravity Model : This example demonstrates a basic gravity analysis including loading data, constructing some summary statistics, estimating a model, and outputing the results in several possible formats. Tutorial 1 - Estimating a Gravity Model Load Data The gme package uses a special object to manage data: the gme.EstimationData. The data can be thought of as a specialized Pandas DataFrame. 1 The EstimationData features a Pandas.DataFrame containing data (trade + gravity + etc.) for estimation as well as additional information about that data and methods that can be used to summarize and/or manipulate the data. This tutorial will demonstrate some of these features. First, we must begin by creating a gme.EstimationData. Doing so requires the inputting of a Pandas.DataFrame and several pieces of \"meta-data\" that describe the data. Start by loading a dataset using the read_csv() function from pandas. In the sample code below, we will read a dataset directly from the internet, but you could just as easily read the same file from you hard drive. ```python tab=\"Code and output\" import gme as gme import pandas as pd gravity_data = pd.read_csv('https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv') gravity_data.head() importer exporter year trade_value agree_pta common_language \\ 0 AUS BRA 1989 3.035469e+08 0.0 1.0 1 AUS CAN 1989 8.769946e+08 0.0 1.0 2 AUS CHE 1989 4.005245e+08 0.0 1.0 3 AUS DEU 1989 2.468977e+09 0.0 0.0 4 AUS DNK 1989 1.763072e+08 0.0 1.0 contiguity log_distance 0 0.0 9.553332 1 0.0 9.637676 2 0.0 9.687557 3 0.0 9.675007 4 0.0 9.657311 Next, we use the loaded data to create a EstimationData gme_data = gme.EstimationData(data_frame=gravity_data, imp_var_name='importer', exp_var_name='exporter', trade_var_name='trade_value', year_var_name='year', notes='Downloaded from https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv') ```Python tab=\"Code only\" import gme as gme import pandas as pd gravity_data = pd.read_csv('https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv') gravity_data.head() gme_data = gme.EstimationData(data_frame=gravity_data, imp_var_name='importer', exp_var_name='exporter', trade_var_name='trade_value', year_var_name='year', notes='Downloaded from https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv') In creating the EstimationData object, the user is asked to supply a Pandas.DataFrame, which we a loaded in the previous lines, and several types of descriptive arguments. These arguments ( imp_var_name , exp_var_name , trade_var_name , and year_var_name ) specify the columns in the supplied DataFrame corresponding to particular types of information that will likely be present in any gravity analysis (the column containing the importer ID, exporter ID, trade values, and year, respectively). These \"meta-data\" fields can be useful as they prevent users from having to re-enter these same basic characteristics of the data at later points and permit the automatic construction of certain types of summary information. Finally, an optional note is supplied to the EstimationData. The EstimationData object has a attribute that stores a list user-supplied strings for later reference. In this case, we have supplied a note indicating from where the data originated. Working with the EstimationData In addition to providing a object class that communicates conveniently with the gme.EstimationModel (see below), the EstimationData provides a collection of data summary and manipulation tools. For example, simply calling (or printing) the object, returns a summary of the scope of the data: >>> gme_data number of countries : 62 number of exporters : 62 number of importers : 62 number of years : 27 number of sectors : not_applicable dimensions : ( 98612 , 8 ) As can be seen from the console return, the dataset we are using covers 62 importers and exporters, 27 years, and contains 98,612 rows and 8 columns. Because this particular dataset does not have multiple sectors, that field is marked as 'not applicable'. 2 Other summary information can be reported in the following ways: # Return the number of importers in the dataset. >>> gme_data . number_of_importers 62 # Return a list of the column names >>> gme_data . columns [ 'importer' , 'exporter' , 'year' , 'trade_value' , 'agree_pta' , 'common_language' , 'contiguity' , 'log_distance' ] # Return a list of years in the dataset >>> gme_data . year_list () [ 1989 , 1990 , 1991 , 1992 , 1993 , 1994 , 1995 , 1996 , 1997 , 1998 , 1999 , 2000 , 2001 , 2002 , 2003 , 2004 , 2005 , 2006 , 2007 , 2008 , 2009 , 2010 , 2011 , 2012 , 2013 , 2014 , 2015 ] # Return a dictionary containing a list of countries in the dataset for each year. >>> country_list = gme_data . countries_each_year () >>> country_list [ 1989 ] [ 'IRN' , 'BOL' , 'TUR' , 'ARG' , 'CHL' , 'HUN' , 'KEN' , 'VEN' , 'ZAF' , 'URY' , 'BRA' , 'DZA' , 'PER' , 'IRL' , 'DNK' , 'GHA' , 'KOR' , 'PAK' , 'COL' , 'IND' , 'ISL' , 'ISR' , 'ESP' , 'ITA' , 'NLD' , 'NGA' , 'AUS' , 'SWE' , 'PRY' , 'GBR' , 'IDN' , 'HKG' , 'NOR' , 'TUN' , 'EGY' , 'KWT' , 'DEU' , 'CHE' , 'MYS' , 'NZL' , 'LBY' , 'USA' , 'SDN' , 'CHN' , 'GRC' , 'MEX' , 'CAN' , 'PRT' , 'SAU' , 'POL' , 'PHL' , 'THA' , 'FRA' , 'JPN' , 'MAR' , 'AUT' , 'FIN' , 'SGP' , 'ECU' ] # Additionally, many of the descriptive methods from Pandas.DataFrames have been inherited: >>> gme_data . dtypes () importer object exporter object year int64 trade_value float64 agree_pta float64 common_language float64 contiguity float64 log_distance float64 dtype : object >>> gme_data . info () < class ' pandas . core . frame . DataFrame '> RangeIndex : 98612 entries , 0 to 98611 Data columns ( total 8 columns ): importer 98612 non - null object exporter 98612 non - null object year 98612 non - null int64 trade_value 98612 non - null float64 agree_pta 97676 non - null float64 common_language 97676 non - null float64 contiguity 97676 non - null float64 log_distance 97676 non - null float64 dtypes : float64 ( 5 ), int64 ( 1 ), object ( 2 ) memory usage : 6.0 + MB >>> gme_data . describe () year trade_value agree_pta common_language \\ count 98612.000000 9.861200e+04 97676.000000 97676.000000 mean 2002.210441 1.856316e+09 0.381547 0.380646 std 7.713050 1.004735e+10 0.485769 0.485548 min 1989.000000 0.000000e+00 0.000000 0.000000 25 % 1996.000000 1.084703e+06 0.000000 0.000000 50 % 2002.000000 6.597395e+07 0.000000 0.000000 75 % 2009.000000 6.125036e+08 1.000000 1.000000 max 2015.000000 4.977686e+11 1.000000 1.000000 contiguity log_distance count 97676.000000 97676.000000 mean 0.034051 8.722631 std 0.181362 0.818818 min 0.000000 5.061335 25 % 0.000000 8.222970 50 % 0.000000 9.012502 75 % 0.000000 9.303026 max 1.000000 9.890765 Additionally, the EstimationData object retains the full ability to work with the supplied DataFrame. The DataFrame can be easily accessed by referring to its attribute in EstimationData. # Return the column of trade_values >>> gme_data . data_frame [ 'trade_value' ] 0 3.035469e+08 1 8.769946e+08 2 4.005245e+08 ... 98609 0.000000e+00 98610 0.000000e+00 98611 0.000000e+00 Name : trade_value , Length : 98612 , dtype : float64 Finally, the EstimationData object features a tool for easy aggregation and custom summary information. Additionally, because the method used for this process returns a DataFrame, the aggregated information can itself be used for many other applications, including the creation of a new EstimationData object. # Calculate mean, minimum, and maximum trade values and distances for each importer. >>> aggregated_data_1 = gme_data . tabulate_by_group ( tba_variables = [ 'trade_value' , 'log_distance' ], by_group = [ 'importer' ], how = [ 'mean' , 'min' , 'max' ]) >>> aggregated_data_1 . head ( 5 ) importer_ trade_value_mean trade_value_min trade_value_max \\ 0 ARG 4.579184e+08 0.0 2.218091e+10 1 AUS 1.704595e+09 0.0 4.620843e+10 2 AUT 1.241109e+09 0.0 6.973252e+10 3 BEL 4.783322e+09 0.0 9.963373e+10 4 BOL 5.506187e+07 0.0 1.810665e+09 log_distance_mean log_distance_min log_distance_max 0 9.137979 6.377581 9.856877 1 9.421134 7.926203 9.774024 2 8.231444 5.723756 9.800177 3 8.223088 5.061335 9.824095 4 9.090685 7.150738 9.874070 # Calculate mean, standard deviation, and count of trade for each imoprter/exporter pair. >>> aggregated_data_2 = gme_data . tabulate_by_group ( tab_variables = [ 'trade_value' ], by_group = [ 'importer' , 'exporter' ], how = [ 'mean' , 'std' , 'count' ]) >>> aggregated_data_2 . head ( 5 ) importer_ exporter_ trade_value_mean trade_value_std trade_value_count 0 ARG AUS 1.205231e+08 1.056954e+08 27 1 ARG AUT 9.942091e+07 8.018963e+07 27 2 ARG BEL 2.972734e+08 2.129833e+08 17 3 ARG BOL 3.378161e+08 6.339966e+08 27 4 ARG BRA 8.209756e+09 6.723047e+09 27 Note Knowing when to end a command with ( ) : When first learning python, it can be confusing trying to determine when a command applied to an object should be followed by parentheses. In the preceding code example, you will see instances of both: gme_data.columns and gme_data.year_list( ) , for example. Knowing which format to use is largely a matter of becoming familiar with the functions you are using. However, there is a logic to it. Each time a command is applied to an object (i.e. using the syntax object.command ), you are calling either an attribute of the object or a method on the object. An attribute is a piece of information that has already been created and included in the object whereas a method is effectively a function that can be run on the object. A method will be called with two parentheses because they will often accept additional arguments. For example, this is the case with the DataFrame method head( ) , which can accept a user-provided number of rows. However, you will often find that you do not need to supply additional arguments to a method, in which case you leave the parentheses empty. An attribute, by comparison, does not feature ( ) because there is no need or ability to provide additional input because the contents of that attribute have already been computed. As mentioned before, knowing whether a command is an attribute or a method, however, simply requires familiarity with the object. Creating and estimating a model Once a EstimationData has been created, estimating a gravity model using the data is fairly straight forward. There are two basic step for estimation. 1. Define a model 2. Estimate the model Defining the model amounts to creating another object called a EstimationModel . Like the EstimationData, the Estimation model is meant to standardize and simplify the steps typically taken to specify and estimate a gravity model. While the EstimationData is meant to be an object that is created once for each study, many EstimationModels will likely be defined and redefined as you test different specifications. Thus, the arguments and attributes of the EstimationModel reflect the different types of modifications you may want to make as you select your preferred specification. As with the EstimationData, the EstimationModel is largely a dataset---this time in the form of the EstimationData---with added information that define the characteristics of the particular model. The following example depict several model specifications, each demonstrating different types of model aspects that can be specified. # A simple case in which 'trade_value' is dependent on 'log_distance', 'agree_pta', etc. >>> model_baseline = gme . EstimationModel ( data_object = gme_data , lhs_var = 'trade_value' , rhs_var = [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' ]) # A specification that will generate and include importer-year and exporter-year # fixed effects >>> fixed_effects_model = gme . EstimationModel ( data_object = gme_data , lhs_var = 'trade_value' , rhs_var = [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' ], fixed_effects = [[ 'importer' , 'year' ], [ 'exporter' , 'year' ]]) # A specification that uses a subset of the data. The United States ('USA') will be omitted # and only the years 2013--2015 will be included. >>> data_subset_model = gme . EstimationModel ( data_object = gme_data , lhs_var = 'trade_value' , rhs_var = [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' ], drop_imp_exp = [ 'USA' ], keep_years = [ 2015 , 2014 , 2013 ]) When specifying the model, there are several key types of attributes that can be included. Model Variables : The variables to be included are specified using the arguments lhs_vars and rhs_vars , which denote the left-hand-side dependent variable and right-hand-side independent variables, respectively. Fixed Effects : The model, at the point at which it is estimated, will constructed fixed effects if any are specified by fixed_effects . These can be either single variables (e.g. ['importer']), or interacted variables (e.g. ['importer', 'year']). Data Subsets : Subsets of the data to use for estimation can be specified in a variety of ways. The arguments keep_years and drop_years can be used to select only a subset of years to include. Similarly the keep_imp , keep_exp , and keep_imp_exp arguments, and their corresponding drop_... options can do the same for importers and/or exporters. Once a model has been defined, running a PPML estimation according to the supplied specification is quite straightforward, it only requires the application of a single method of the EstimationModel: .estimate() . No further inputs are required. # Define a new, fixed effects model using only a subset of years (to reduce the computation time) >>> fixed_effects_model_2 = gme . EstimationModel ( data_object = gme_data , lhs_var = 'trade_value' , rhs_var = [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' ], fixed_effects = [[ 'importer' , 'year' ], [ 'exporter' , 'year' ]], keep_years = [ 2013 , 2014 , 2015 ]) # Conduct a PPML estimation of the fixed effects model. estimates = fixed_effects_model_2 . estimate () select specification variables : [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' , 'trade_value' , 'importer' , 'exporter' , 'year' ], Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } drop_intratrade : no , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } drop_imp : none , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } drop_exp : none , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } keep_imp : all available , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } keep_exp : all available , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } drop_years : none , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } keep_years : [ 2013 , 2014 , 2015 ], Observations excluded by user : { 'rows' : 87632 , 'columns' : 0 } drop_missing : yes , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } Estimation began at 08 : 58 AM on Jun 19 , 2018 Estimation completed at 08 : 58 AM on Jun 19 , 2018 The results ( estimates ) are stored as a dictionary with each entry in the dictionary corresponding to a single estimation. 3 The storing of the results in this way is primarily to facilitate the sector_by_sector , which separately estimates a model for each product/industry/sector, and returns a set of estimation results for each. In the case in which multiple sectors are not considered, as in the examples considered here, the results dictionary contains a single entry with the key 'all'. Viewing, formatting, and outputting the results The first step to viewing and working with the regression estimates is unpacking them from the dictionary in which they have been stored. A dictionary is an object in which each item stored in the dictionary is associated with a key. That key can be used to return its associated item. In the above example, estimates is a dictionary in which each item is an object of results. In this tutorial, there is only one object in this case because only one regression was run. In cases in which multiple regressions are run because multiple sectors are estimated separately, the dictionary would contain multiple results, each keyed with the name of the respective sector. # Return a list of keys in the object >>> estimates . keys () dict_keys ([ 'all' ]) # Return the result object and save it to a new variable for convenience >>> results = estimates [ 'all' ] The estimation uses tools from the statsmodels package so that the results inherit all of the features of the statsmodels GLM results object. 4 This means that the object contains a plethora of fields reflecting things like coefficient estimates, standard errors, p-values, AIC/BIC, etc. Similarly, there is a useful method associated with the object that can be used for creating summary tables. # print a summary of the results >>> results . summary () < class ' statsmodels . iolib . summary . Summary '> Generalized Linear Model Regression Results ============================================================================== Dep . Variable : trade_value No . Observations : 8700 Model : GLM Df Residuals : 8371 Model Family : Poisson Df Model : 328 Link Function : log Scale : 1.0 Method : IRLS Log - Likelihood : - 4.8282e+12 Date : Wed , 20 Jun 2018 Deviance : 9.6565e+12 Time : 13 : 36 : 10 Pearson chi2 : 1.22e+13 No . Iterations : 10 ============================================================================================ coef std err z P >| z | [ 0.025 0.975 ] -------------------------------------------------------------------------------------------- log_distance - 0.7398 0.024 - 30.982 0.000 - 0.787 - 0.693 agree_pta 0.3342 0.043 7.824 0.000 0.250 0.418 common_language 0.1288 0.039 3.270 0.001 0.052 0.206 contiguity 0.2552 0.047 5.423 0.000 0.163 0.347 importer_year_fe_ARG2013 26.9804 0.361 74.690 0.000 26.272 27.688 importer_year_fe_ARG2014 26.8032 0.344 77.840 0.000 26.128 27.478 importer_year_fe_AUS2013 28.1690 0.315 89.455 0.000 27.552 28.786 ... ( truncated for this tutorial ) ============================================================================================ # Extract the estimated parameter values (returned as a Pandas.Series) >>> coefficients = results . params >>> coefficients , head () log_distance - 0.739840 agree_pta 0.334219 common_language 0.128770 contiguity 0.255161 importer_year_fe_ARG2013 26.980367 dtype : float64 # Extract the standard errors >>> results . bse log_distance 0.023879 agree_pta 0.042720 ... exporter_year_fe_VEN2015 0.346733 Length : 329 , dtype : float64 # Extract the p-values >>> results . pvalues log_distance 9.318804e-211 agree_pta 5.134355e-15 ... exporter_year_fe_VEN2015 5.681631e-03 Length : 329 , dtype : float64 # Return fitted values >>> results . fittedvalues 0 1.610136e+09 1 3.044133e+08 2 5.799368e+08 ... 9359 1.329831e+10 Length : 8700 , dtype : float64 The estimate method also provides some diagnostic information that helps judge the quality of the regression. This information includes a listing of columns that dropped due to collinearities or an absence of trade and an indicator for over-fitting. # Return diagnostic information (a Pandas.Series or DataFrame) >>> fixed_effects_model_2 . ppml_diagnostics Overfit Warning No Collinearities Yes Number of Columns Excluded 41 Perfectly Collinear Variables [ exporter_year_fe_ZAF2013 , exporter_year_fe_ZA ... Zero Trade Variables [ importer_year_fe_ARG2015 , importer_year_fe_AU ... Completion Time 0.25 minutes dtype : object # Retrieve the full list of collinear columns >>> fixed_effects_model_2 . ppml_diagnostics [ 'Perfectly Collinear Variables' ] [ 'exporter_year_fe_ZAF2013' , 'exporter_year_fe_ZAF2014' , 'exporter_year_fe_ZAF2015' ] The gme package also features several tools to help compile and format the results for use within python or outside of it. These tools include one method named combine_sector_results() for pulling all coefficients, standard errors, and p-values from from multiple sectors into a single DataFrame. The second, called format_regression_tables , creates formatted tables for presentation that can be exported as a text file, csv file, or LaTeX file with some stylized syntax. # Collect coefficients, standard errors, and p-values from the regression results. >>> combined_results = fixed_effects_model_2 . combine_sector_results >>> combined_results . head () all_coeff all_pvalue all_stderr log_distance - 0.739840 9.318804e-211 ( 0.023879411125052336 ) agree_pta 0.334219 5.134355e-15 ( 0.04271952339258154 ) common_language 0.128770 1.076932e-03 ( 0.03938367074719932 ) contiguity 0.255161 5.857612e-08 ( 0.04705076644539403 ) importer_year_fe_ARG2013 26.980367 0.000000e+00 ( 0.3612289201097519 ) # Had there been multiple sectors/regressions in the model results, there would # have been additional columns in the 'combined' DataFrame. # Format the table and export to a csv file >>> fixed_effects_model . format_regression_table ( format = 'csv' , se_below = True , omit_fe_prefix = [ 'importer_year' , 'exporter_year' ], path = 'C: \\\\ formatted_table.csv' ) # which writes a table to the hard drive that looks like: Variable all agree_pta 0.334 *** ( 0.043 ) common_language 0.129 *** ( 0.039 ) contiguity 0.255 *** ( 0.047 ) log_distance - 0.740 *** ( 0.024 ) AIC 9656495631872.947 BIC 9656495371812.434 Likelihood - 4828247815607.474 Obs . 8700 It is also worth noting that the commands combine_sector_results() and format_regression_tables() can both be used in one of two ways. They are both stand-alone functions that can be supplied dictionaries of results (produced directly from the EstimationModel or custom assembled by a user), as in the format_regression_table() example. Alternatively, both can be used as methods on the EstimationObject itself, as in the combine_sector_results() example. Tip LaTeX users : The method format_regression_table can output a table into a csv file with some desired LaTeX syntax. This can be done by specifying format = '.csv' and 'latex_syntax'=True . This option allows users manipulate the tables in spreadsheet software while retaining LaTeX syntax. We recommend reading the file into the spreadsheet software as 'text data' so that it does not try to interpret the formatting of the table entries. See https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html for more information on Pandas DataFrames. \u21a9 Had their been multiple sectors, we could have indicated so by adding the input sector_var_name = 'sector_column' in the declaration of the EstimationData. \u21a9 Additionally, the results of the estimation are saved as a attribute of the estimation model--- EstimationModel.results_dict ---and can be retrieved that way as well. \u21a9 For more details about the statsmodels results object, see http://www.statsmodels.org/0.6.1/generated/statsmodels.genmod.generalized_linear_model.GLMResults.html . \u21a9","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"The following tutorials are meant to introduce users to the gme package and help them become proficcient with many of the features that it offers. Tutorial 1 - Estimating a Gravity Model : This example demonstrates a basic gravity analysis including loading data, constructing some summary statistics, estimating a model, and outputing the results in several possible formats.","title":"Tutorials"},{"location":"tutorials/#tutorial-1-estimating-a-gravity-model","text":"","title":"Tutorial 1 - Estimating a Gravity Model"},{"location":"tutorials/#load-data","text":"The gme package uses a special object to manage data: the gme.EstimationData. The data can be thought of as a specialized Pandas DataFrame. 1 The EstimationData features a Pandas.DataFrame containing data (trade + gravity + etc.) for estimation as well as additional information about that data and methods that can be used to summarize and/or manipulate the data. This tutorial will demonstrate some of these features. First, we must begin by creating a gme.EstimationData. Doing so requires the inputting of a Pandas.DataFrame and several pieces of \"meta-data\" that describe the data. Start by loading a dataset using the read_csv() function from pandas. In the sample code below, we will read a dataset directly from the internet, but you could just as easily read the same file from you hard drive. ```python tab=\"Code and output\" import gme as gme import pandas as pd gravity_data = pd.read_csv('https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv') gravity_data.head() importer exporter year trade_value agree_pta common_language \\ 0 AUS BRA 1989 3.035469e+08 0.0 1.0 1 AUS CAN 1989 8.769946e+08 0.0 1.0 2 AUS CHE 1989 4.005245e+08 0.0 1.0 3 AUS DEU 1989 2.468977e+09 0.0 0.0 4 AUS DNK 1989 1.763072e+08 0.0 1.0 contiguity log_distance 0 0.0 9.553332 1 0.0 9.637676 2 0.0 9.687557 3 0.0 9.675007 4 0.0 9.657311","title":"Load Data"},{"location":"tutorials/#next-we-use-the-loaded-data-to-create-a-estimationdata","text":"gme_data = gme.EstimationData(data_frame=gravity_data, imp_var_name='importer', exp_var_name='exporter', trade_var_name='trade_value', year_var_name='year', notes='Downloaded from https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv') ```Python tab=\"Code only\" import gme as gme import pandas as pd gravity_data = pd.read_csv('https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv') gravity_data.head() gme_data = gme.EstimationData(data_frame=gravity_data, imp_var_name='importer', exp_var_name='exporter', trade_var_name='trade_value', year_var_name='year', notes='Downloaded from https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv') In creating the EstimationData object, the user is asked to supply a Pandas.DataFrame, which we a loaded in the previous lines, and several types of descriptive arguments. These arguments ( imp_var_name , exp_var_name , trade_var_name , and year_var_name ) specify the columns in the supplied DataFrame corresponding to particular types of information that will likely be present in any gravity analysis (the column containing the importer ID, exporter ID, trade values, and year, respectively). These \"meta-data\" fields can be useful as they prevent users from having to re-enter these same basic characteristics of the data at later points and permit the automatic construction of certain types of summary information. Finally, an optional note is supplied to the EstimationData. The EstimationData object has a attribute that stores a list user-supplied strings for later reference. In this case, we have supplied a note indicating from where the data originated.","title":"Next, we use the loaded data to create a EstimationData"},{"location":"tutorials/#working-with-the-estimationdata","text":"In addition to providing a object class that communicates conveniently with the gme.EstimationModel (see below), the EstimationData provides a collection of data summary and manipulation tools. For example, simply calling (or printing) the object, returns a summary of the scope of the data: >>> gme_data number of countries : 62 number of exporters : 62 number of importers : 62 number of years : 27 number of sectors : not_applicable dimensions : ( 98612 , 8 ) As can be seen from the console return, the dataset we are using covers 62 importers and exporters, 27 years, and contains 98,612 rows and 8 columns. Because this particular dataset does not have multiple sectors, that field is marked as 'not applicable'. 2 Other summary information can be reported in the following ways: # Return the number of importers in the dataset. >>> gme_data . number_of_importers 62 # Return a list of the column names >>> gme_data . columns [ 'importer' , 'exporter' , 'year' , 'trade_value' , 'agree_pta' , 'common_language' , 'contiguity' , 'log_distance' ] # Return a list of years in the dataset >>> gme_data . year_list () [ 1989 , 1990 , 1991 , 1992 , 1993 , 1994 , 1995 , 1996 , 1997 , 1998 , 1999 , 2000 , 2001 , 2002 , 2003 , 2004 , 2005 , 2006 , 2007 , 2008 , 2009 , 2010 , 2011 , 2012 , 2013 , 2014 , 2015 ] # Return a dictionary containing a list of countries in the dataset for each year. >>> country_list = gme_data . countries_each_year () >>> country_list [ 1989 ] [ 'IRN' , 'BOL' , 'TUR' , 'ARG' , 'CHL' , 'HUN' , 'KEN' , 'VEN' , 'ZAF' , 'URY' , 'BRA' , 'DZA' , 'PER' , 'IRL' , 'DNK' , 'GHA' , 'KOR' , 'PAK' , 'COL' , 'IND' , 'ISL' , 'ISR' , 'ESP' , 'ITA' , 'NLD' , 'NGA' , 'AUS' , 'SWE' , 'PRY' , 'GBR' , 'IDN' , 'HKG' , 'NOR' , 'TUN' , 'EGY' , 'KWT' , 'DEU' , 'CHE' , 'MYS' , 'NZL' , 'LBY' , 'USA' , 'SDN' , 'CHN' , 'GRC' , 'MEX' , 'CAN' , 'PRT' , 'SAU' , 'POL' , 'PHL' , 'THA' , 'FRA' , 'JPN' , 'MAR' , 'AUT' , 'FIN' , 'SGP' , 'ECU' ] # Additionally, many of the descriptive methods from Pandas.DataFrames have been inherited: >>> gme_data . dtypes () importer object exporter object year int64 trade_value float64 agree_pta float64 common_language float64 contiguity float64 log_distance float64 dtype : object >>> gme_data . info () < class ' pandas . core . frame . DataFrame '> RangeIndex : 98612 entries , 0 to 98611 Data columns ( total 8 columns ): importer 98612 non - null object exporter 98612 non - null object year 98612 non - null int64 trade_value 98612 non - null float64 agree_pta 97676 non - null float64 common_language 97676 non - null float64 contiguity 97676 non - null float64 log_distance 97676 non - null float64 dtypes : float64 ( 5 ), int64 ( 1 ), object ( 2 ) memory usage : 6.0 + MB >>> gme_data . describe () year trade_value agree_pta common_language \\ count 98612.000000 9.861200e+04 97676.000000 97676.000000 mean 2002.210441 1.856316e+09 0.381547 0.380646 std 7.713050 1.004735e+10 0.485769 0.485548 min 1989.000000 0.000000e+00 0.000000 0.000000 25 % 1996.000000 1.084703e+06 0.000000 0.000000 50 % 2002.000000 6.597395e+07 0.000000 0.000000 75 % 2009.000000 6.125036e+08 1.000000 1.000000 max 2015.000000 4.977686e+11 1.000000 1.000000 contiguity log_distance count 97676.000000 97676.000000 mean 0.034051 8.722631 std 0.181362 0.818818 min 0.000000 5.061335 25 % 0.000000 8.222970 50 % 0.000000 9.012502 75 % 0.000000 9.303026 max 1.000000 9.890765 Additionally, the EstimationData object retains the full ability to work with the supplied DataFrame. The DataFrame can be easily accessed by referring to its attribute in EstimationData. # Return the column of trade_values >>> gme_data . data_frame [ 'trade_value' ] 0 3.035469e+08 1 8.769946e+08 2 4.005245e+08 ... 98609 0.000000e+00 98610 0.000000e+00 98611 0.000000e+00 Name : trade_value , Length : 98612 , dtype : float64 Finally, the EstimationData object features a tool for easy aggregation and custom summary information. Additionally, because the method used for this process returns a DataFrame, the aggregated information can itself be used for many other applications, including the creation of a new EstimationData object. # Calculate mean, minimum, and maximum trade values and distances for each importer. >>> aggregated_data_1 = gme_data . tabulate_by_group ( tba_variables = [ 'trade_value' , 'log_distance' ], by_group = [ 'importer' ], how = [ 'mean' , 'min' , 'max' ]) >>> aggregated_data_1 . head ( 5 ) importer_ trade_value_mean trade_value_min trade_value_max \\ 0 ARG 4.579184e+08 0.0 2.218091e+10 1 AUS 1.704595e+09 0.0 4.620843e+10 2 AUT 1.241109e+09 0.0 6.973252e+10 3 BEL 4.783322e+09 0.0 9.963373e+10 4 BOL 5.506187e+07 0.0 1.810665e+09 log_distance_mean log_distance_min log_distance_max 0 9.137979 6.377581 9.856877 1 9.421134 7.926203 9.774024 2 8.231444 5.723756 9.800177 3 8.223088 5.061335 9.824095 4 9.090685 7.150738 9.874070 # Calculate mean, standard deviation, and count of trade for each imoprter/exporter pair. >>> aggregated_data_2 = gme_data . tabulate_by_group ( tab_variables = [ 'trade_value' ], by_group = [ 'importer' , 'exporter' ], how = [ 'mean' , 'std' , 'count' ]) >>> aggregated_data_2 . head ( 5 ) importer_ exporter_ trade_value_mean trade_value_std trade_value_count 0 ARG AUS 1.205231e+08 1.056954e+08 27 1 ARG AUT 9.942091e+07 8.018963e+07 27 2 ARG BEL 2.972734e+08 2.129833e+08 17 3 ARG BOL 3.378161e+08 6.339966e+08 27 4 ARG BRA 8.209756e+09 6.723047e+09 27 Note Knowing when to end a command with ( ) : When first learning python, it can be confusing trying to determine when a command applied to an object should be followed by parentheses. In the preceding code example, you will see instances of both: gme_data.columns and gme_data.year_list( ) , for example. Knowing which format to use is largely a matter of becoming familiar with the functions you are using. However, there is a logic to it. Each time a command is applied to an object (i.e. using the syntax object.command ), you are calling either an attribute of the object or a method on the object. An attribute is a piece of information that has already been created and included in the object whereas a method is effectively a function that can be run on the object. A method will be called with two parentheses because they will often accept additional arguments. For example, this is the case with the DataFrame method head( ) , which can accept a user-provided number of rows. However, you will often find that you do not need to supply additional arguments to a method, in which case you leave the parentheses empty. An attribute, by comparison, does not feature ( ) because there is no need or ability to provide additional input because the contents of that attribute have already been computed. As mentioned before, knowing whether a command is an attribute or a method, however, simply requires familiarity with the object.","title":"Working with the EstimationData"},{"location":"tutorials/#creating-and-estimating-a-model","text":"Once a EstimationData has been created, estimating a gravity model using the data is fairly straight forward. There are two basic step for estimation. 1. Define a model 2. Estimate the model Defining the model amounts to creating another object called a EstimationModel . Like the EstimationData, the Estimation model is meant to standardize and simplify the steps typically taken to specify and estimate a gravity model. While the EstimationData is meant to be an object that is created once for each study, many EstimationModels will likely be defined and redefined as you test different specifications. Thus, the arguments and attributes of the EstimationModel reflect the different types of modifications you may want to make as you select your preferred specification. As with the EstimationData, the EstimationModel is largely a dataset---this time in the form of the EstimationData---with added information that define the characteristics of the particular model. The following example depict several model specifications, each demonstrating different types of model aspects that can be specified. # A simple case in which 'trade_value' is dependent on 'log_distance', 'agree_pta', etc. >>> model_baseline = gme . EstimationModel ( data_object = gme_data , lhs_var = 'trade_value' , rhs_var = [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' ]) # A specification that will generate and include importer-year and exporter-year # fixed effects >>> fixed_effects_model = gme . EstimationModel ( data_object = gme_data , lhs_var = 'trade_value' , rhs_var = [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' ], fixed_effects = [[ 'importer' , 'year' ], [ 'exporter' , 'year' ]]) # A specification that uses a subset of the data. The United States ('USA') will be omitted # and only the years 2013--2015 will be included. >>> data_subset_model = gme . EstimationModel ( data_object = gme_data , lhs_var = 'trade_value' , rhs_var = [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' ], drop_imp_exp = [ 'USA' ], keep_years = [ 2015 , 2014 , 2013 ]) When specifying the model, there are several key types of attributes that can be included. Model Variables : The variables to be included are specified using the arguments lhs_vars and rhs_vars , which denote the left-hand-side dependent variable and right-hand-side independent variables, respectively. Fixed Effects : The model, at the point at which it is estimated, will constructed fixed effects if any are specified by fixed_effects . These can be either single variables (e.g. ['importer']), or interacted variables (e.g. ['importer', 'year']). Data Subsets : Subsets of the data to use for estimation can be specified in a variety of ways. The arguments keep_years and drop_years can be used to select only a subset of years to include. Similarly the keep_imp , keep_exp , and keep_imp_exp arguments, and their corresponding drop_... options can do the same for importers and/or exporters. Once a model has been defined, running a PPML estimation according to the supplied specification is quite straightforward, it only requires the application of a single method of the EstimationModel: .estimate() . No further inputs are required. # Define a new, fixed effects model using only a subset of years (to reduce the computation time) >>> fixed_effects_model_2 = gme . EstimationModel ( data_object = gme_data , lhs_var = 'trade_value' , rhs_var = [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' ], fixed_effects = [[ 'importer' , 'year' ], [ 'exporter' , 'year' ]], keep_years = [ 2013 , 2014 , 2015 ]) # Conduct a PPML estimation of the fixed effects model. estimates = fixed_effects_model_2 . estimate () select specification variables : [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' , 'trade_value' , 'importer' , 'exporter' , 'year' ], Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } drop_intratrade : no , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } drop_imp : none , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } drop_exp : none , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } keep_imp : all available , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } keep_exp : all available , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } drop_years : none , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } keep_years : [ 2013 , 2014 , 2015 ], Observations excluded by user : { 'rows' : 87632 , 'columns' : 0 } drop_missing : yes , Observations excluded by user : { 'rows' : 0 , 'columns' : 0 } Estimation began at 08 : 58 AM on Jun 19 , 2018 Estimation completed at 08 : 58 AM on Jun 19 , 2018 The results ( estimates ) are stored as a dictionary with each entry in the dictionary corresponding to a single estimation. 3 The storing of the results in this way is primarily to facilitate the sector_by_sector , which separately estimates a model for each product/industry/sector, and returns a set of estimation results for each. In the case in which multiple sectors are not considered, as in the examples considered here, the results dictionary contains a single entry with the key 'all'.","title":"Creating and estimating a model"},{"location":"tutorials/#viewing-formatting-and-outputting-the-results","text":"The first step to viewing and working with the regression estimates is unpacking them from the dictionary in which they have been stored. A dictionary is an object in which each item stored in the dictionary is associated with a key. That key can be used to return its associated item. In the above example, estimates is a dictionary in which each item is an object of results. In this tutorial, there is only one object in this case because only one regression was run. In cases in which multiple regressions are run because multiple sectors are estimated separately, the dictionary would contain multiple results, each keyed with the name of the respective sector. # Return a list of keys in the object >>> estimates . keys () dict_keys ([ 'all' ]) # Return the result object and save it to a new variable for convenience >>> results = estimates [ 'all' ] The estimation uses tools from the statsmodels package so that the results inherit all of the features of the statsmodels GLM results object. 4 This means that the object contains a plethora of fields reflecting things like coefficient estimates, standard errors, p-values, AIC/BIC, etc. Similarly, there is a useful method associated with the object that can be used for creating summary tables. # print a summary of the results >>> results . summary () < class ' statsmodels . iolib . summary . Summary '> Generalized Linear Model Regression Results ============================================================================== Dep . Variable : trade_value No . Observations : 8700 Model : GLM Df Residuals : 8371 Model Family : Poisson Df Model : 328 Link Function : log Scale : 1.0 Method : IRLS Log - Likelihood : - 4.8282e+12 Date : Wed , 20 Jun 2018 Deviance : 9.6565e+12 Time : 13 : 36 : 10 Pearson chi2 : 1.22e+13 No . Iterations : 10 ============================================================================================ coef std err z P >| z | [ 0.025 0.975 ] -------------------------------------------------------------------------------------------- log_distance - 0.7398 0.024 - 30.982 0.000 - 0.787 - 0.693 agree_pta 0.3342 0.043 7.824 0.000 0.250 0.418 common_language 0.1288 0.039 3.270 0.001 0.052 0.206 contiguity 0.2552 0.047 5.423 0.000 0.163 0.347 importer_year_fe_ARG2013 26.9804 0.361 74.690 0.000 26.272 27.688 importer_year_fe_ARG2014 26.8032 0.344 77.840 0.000 26.128 27.478 importer_year_fe_AUS2013 28.1690 0.315 89.455 0.000 27.552 28.786 ... ( truncated for this tutorial ) ============================================================================================ # Extract the estimated parameter values (returned as a Pandas.Series) >>> coefficients = results . params >>> coefficients , head () log_distance - 0.739840 agree_pta 0.334219 common_language 0.128770 contiguity 0.255161 importer_year_fe_ARG2013 26.980367 dtype : float64 # Extract the standard errors >>> results . bse log_distance 0.023879 agree_pta 0.042720 ... exporter_year_fe_VEN2015 0.346733 Length : 329 , dtype : float64 # Extract the p-values >>> results . pvalues log_distance 9.318804e-211 agree_pta 5.134355e-15 ... exporter_year_fe_VEN2015 5.681631e-03 Length : 329 , dtype : float64 # Return fitted values >>> results . fittedvalues 0 1.610136e+09 1 3.044133e+08 2 5.799368e+08 ... 9359 1.329831e+10 Length : 8700 , dtype : float64 The estimate method also provides some diagnostic information that helps judge the quality of the regression. This information includes a listing of columns that dropped due to collinearities or an absence of trade and an indicator for over-fitting. # Return diagnostic information (a Pandas.Series or DataFrame) >>> fixed_effects_model_2 . ppml_diagnostics Overfit Warning No Collinearities Yes Number of Columns Excluded 41 Perfectly Collinear Variables [ exporter_year_fe_ZAF2013 , exporter_year_fe_ZA ... Zero Trade Variables [ importer_year_fe_ARG2015 , importer_year_fe_AU ... Completion Time 0.25 minutes dtype : object # Retrieve the full list of collinear columns >>> fixed_effects_model_2 . ppml_diagnostics [ 'Perfectly Collinear Variables' ] [ 'exporter_year_fe_ZAF2013' , 'exporter_year_fe_ZAF2014' , 'exporter_year_fe_ZAF2015' ] The gme package also features several tools to help compile and format the results for use within python or outside of it. These tools include one method named combine_sector_results() for pulling all coefficients, standard errors, and p-values from from multiple sectors into a single DataFrame. The second, called format_regression_tables , creates formatted tables for presentation that can be exported as a text file, csv file, or LaTeX file with some stylized syntax. # Collect coefficients, standard errors, and p-values from the regression results. >>> combined_results = fixed_effects_model_2 . combine_sector_results >>> combined_results . head () all_coeff all_pvalue all_stderr log_distance - 0.739840 9.318804e-211 ( 0.023879411125052336 ) agree_pta 0.334219 5.134355e-15 ( 0.04271952339258154 ) common_language 0.128770 1.076932e-03 ( 0.03938367074719932 ) contiguity 0.255161 5.857612e-08 ( 0.04705076644539403 ) importer_year_fe_ARG2013 26.980367 0.000000e+00 ( 0.3612289201097519 ) # Had there been multiple sectors/regressions in the model results, there would # have been additional columns in the 'combined' DataFrame. # Format the table and export to a csv file >>> fixed_effects_model . format_regression_table ( format = 'csv' , se_below = True , omit_fe_prefix = [ 'importer_year' , 'exporter_year' ], path = 'C: \\\\ formatted_table.csv' ) # which writes a table to the hard drive that looks like: Variable all agree_pta 0.334 *** ( 0.043 ) common_language 0.129 *** ( 0.039 ) contiguity 0.255 *** ( 0.047 ) log_distance - 0.740 *** ( 0.024 ) AIC 9656495631872.947 BIC 9656495371812.434 Likelihood - 4828247815607.474 Obs . 8700 It is also worth noting that the commands combine_sector_results() and format_regression_tables() can both be used in one of two ways. They are both stand-alone functions that can be supplied dictionaries of results (produced directly from the EstimationModel or custom assembled by a user), as in the format_regression_table() example. Alternatively, both can be used as methods on the EstimationObject itself, as in the combine_sector_results() example. Tip LaTeX users : The method format_regression_table can output a table into a csv file with some desired LaTeX syntax. This can be done by specifying format = '.csv' and 'latex_syntax'=True . This option allows users manipulate the tables in spreadsheet software while retaining LaTeX syntax. We recommend reading the file into the spreadsheet software as 'text data' so that it does not try to interpret the formatting of the table entries. See https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html for more information on Pandas DataFrames. \u21a9 Had their been multiple sectors, we could have indicated so by adding the input sector_var_name = 'sector_column' in the declaration of the EstimationData. \u21a9 Additionally, the results of the estimation are saved as a attribute of the estimation model--- EstimationModel.results_dict ---and can be retrieved that way as well. \u21a9 For more details about the statsmodels results object, see http://www.statsmodels.org/0.6.1/generated/statsmodels.genmod.generalized_linear_model.GLMResults.html . \u21a9","title":"Viewing, formatting, and outputting the results"},{"location":"updates/","text":"Version 1.3 Updates Several updates have been made to address bugs, improve the robustness of the estimation procedure, and add features. Backward Compatibility Issues Version 1.3 introduces a change that impacts compatibility with version 1.2. There has been a change to the format of omit_fixed_effect argument of EstimationModel such that past usage will generally not be compatible in version 1.3. See the EstimationModel documentation for details on the new format. Bug Fixes Fixed error in format_regression_table in which the default significance levels were 0.1, 0.05, and 0.1 rather than 0.01, 0.05, and 0.1. The error was causing estimates to receive *** when they were only 10% significant. Fixed issue with the retention of record identifiers in EstimationModel.modified_data after estimation. Previously, the modified estimation data did not include identifiers like importer, exporter, or year. These identifiers are now included in the modified data returned after estimation. Fixed issue in which pre-estimation diagnostics would drop observations associated with collinear fixed effects if the trade values of those observations were all greater than zero. Improved Robustness The omit_fixed_effect argument has been made more robust. It now allows for users to select specific fixed effects to drop in order to avoid collinearity. Notably, the new implementation uses dictionary inputs instead of list inputs so that older uses will no longer work properly in version 1.3. See New Features Option for clustered standard errors ( cluster_on ) has been added to EstimationModel . Added method for creating concatenated 'pair' variables from categorical variables to EstimationData ( add_pair_var ). Added make_data_square function. Added correlation method to EstimationData , which produces a correlation table and visualization. Added kernel density plot function coefficient_kd_plot to visualize sector-by-sector estimated results. Tables created by format_regression_table now report the significance levels at the bottom of the table when written to a file. There is now also an optional notes argument to add a user-supplied note to the bottom of the written table.","title":"What's New in v1.3"},{"location":"updates/#version-13-updates","text":"Several updates have been made to address bugs, improve the robustness of the estimation procedure, and add features. Backward Compatibility Issues Version 1.3 introduces a change that impacts compatibility with version 1.2. There has been a change to the format of omit_fixed_effect argument of EstimationModel such that past usage will generally not be compatible in version 1.3. See the EstimationModel documentation for details on the new format.","title":"Version 1.3 Updates"},{"location":"updates/#bug-fixes","text":"Fixed error in format_regression_table in which the default significance levels were 0.1, 0.05, and 0.1 rather than 0.01, 0.05, and 0.1. The error was causing estimates to receive *** when they were only 10% significant. Fixed issue with the retention of record identifiers in EstimationModel.modified_data after estimation. Previously, the modified estimation data did not include identifiers like importer, exporter, or year. These identifiers are now included in the modified data returned after estimation. Fixed issue in which pre-estimation diagnostics would drop observations associated with collinear fixed effects if the trade values of those observations were all greater than zero.","title":"Bug Fixes"},{"location":"updates/#improved-robustness","text":"The omit_fixed_effect argument has been made more robust. It now allows for users to select specific fixed effects to drop in order to avoid collinearity. Notably, the new implementation uses dictionary inputs instead of list inputs so that older uses will no longer work properly in version 1.3. See","title":"Improved Robustness"},{"location":"updates/#new-features","text":"Option for clustered standard errors ( cluster_on ) has been added to EstimationModel . Added method for creating concatenated 'pair' variables from categorical variables to EstimationData ( add_pair_var ). Added make_data_square function. Added correlation method to EstimationData , which produces a correlation table and visualization. Added kernel density plot function coefficient_kd_plot to visualize sector-by-sector estimated results. Tables created by format_regression_table now report the significance levels at the bottom of the table when written to a file. There is now also an optional notes argument to add a user-supplied note to the bottom of the written table.","title":"New Features"},{"location":"api_docs/EstimationData/","text":"Class gme.EstimationData ( data_frame=None, name:str='unnamed', imp_var_name:str='importer', exp_var_name:str='exporter', year_var_name:str='year', trade_var_name:str=None, sector_var_name:str=None, notes:List[str]=[] ) Description An object used for storing data for gravity modeling and producing some summary statistics. Arguments data_frame : Pandas.DataFrame \u2003 A DataFrame containing trade, gravity, etc. data. name : (optional) str \u2003 A name for the dataset. imp_var_name : str \u2003 The name of the column containing importer IDs. exp_var_name : str \u2003 The name of the column containing exporter IDs. year_var_name : str \u2003 The name of the column containing year data. trade_var_name : (optional) str \u2003 The name of the column containing trade data. sector_var_name : (optional) str \u2003 The name of the column containing sector/industry/product IDs. notes : (optional) str \u2003 A string to be included as a note n the object. Attributes data_frame : Pandas.DataFrame \u2003 The supplied DataFrame. name : str \u2003 The supplied data name. imp_var_name : str \u2003 The name of the column containing importer IDs. exp_var_name : str \u2003 The name of the column containing exporter IDs. year_var_name : str \u2003 The name of the column containing year data. trade_var_name : str \u2003 The name of the column containing trade data. sector_var_name : str \u2003 The name of the column containing sector/industry/product IDs. notes : List[str] \u2003 A list of notes. number_of_exporters : int \u2003 The number of unique exporter IDs in the dataset. number_of_importers : int \u2003 The number of unique importer IDs in the dataset. shape : List[int] \u2003 The dimensions of the dataset. countries : List[str] \u2003 A list of the unique country IDs in the dataset. number_of_countries : int \u2003 The number of unique country IDs in the dataset. number_of_years : int \u2003 The number of years in the dataset columns : List[str] \u2003 A list of column names in the dataset. number_of_sectors : int \u2003 If a sector is specified, the number of unique sector IDs in the dataset. Methods tablulate_by_group : \u2003 Summarize columns by a user-specified grouping. Can be used to tabulate, aggregate, \u2003 summarize,etc. data. \u2003 Arguments: \u2003\u2003 tab_variables : List[str] Column names of variables to be tabulated \u2003\u2003 by_group : List[str] Column names of variables by which to group observations for tabulation. \u2003\u2003 how : List[str] The method by which to combine observations within a group. Can accept 'count', 'mean', 'median', 'min', 'max', 'sum', 'prod', 'std', and 'var'. It may work with other numpy or pandas functions. \u2003 Returns : Pandas.DataFrame \u2003\u2003 A DataFrame of tabulated values for each group. year_list : \u2003 Returns a list of years present in the data. countries_each_year : \u2003 Returns a dictionary keyed by year ID containing a list of country IDs present in each \u2003 corresponding year. sector_list : \u2003 Returns a list of unique sector IDs dtypes : \u2003 Returns the data types of the columns in the EstimationData.data_frame using \u2003 Pandas.DataFrame.dtypes(). See Pandas documentation for more information. info : \u2003 Print summary information about EstimationData.data_frame using \u2003 Pandas.DataFrame.dtypes(). See Pandas documentation for more information. describe : \u2003 Generates some descriptive statistics for EstimationData.data_frame using \u2003 Pandas.DataFrame.describe(). See Pandas documentation for more information. add_note : \u2003 Add a note to the list of notes in 'notes' attribute. \u2003 Arguments : \u2003\u2003 note : str A note to add to EstimationData. \u2003 Returns : None correlation : \u2003 Return and plot the correlation matrix of the data. \u2003 Arguments : \u2003\u2003 columns : List[str] (optional) A list of column names to include in the matrix. Default is to include all columns. \u2003\u2003 plot : bool If True, it plots a heatmap of the correlation matrix. \u2003 Returns : Pandas.DataFrame \u2003\u2003 A correlation matrix add_pair_var : \u2003 Create a new variable that is a concatenation of categorical variables for use as a fixed effect or \u2003 cluster category, for example. Original values are separated by '_' in the concatenated value. \u2003 Arguments : \u2003\u2003 var_list : List[str] List of names of columns to concatinate. \u2003\u2003 var_name : (Optional) str Column name to use for the new variable. By default, it uses the names in var_list separated by '_'. \u2003\u2003 symmetric : bool If False, all variables are concatenated in the order of the var_list (e.g. ARG_USA and USA_ARG would both would be created). If True, ignores ordering of values across columns and concatenates alphabetically (e.g. all ARG and USA pairings would concatenate as ARG_USA) \u2003 Returns : None , adds new column to EstimationData.dataframe. Examples # Load a DataFrame >>> import pandas as pd >>> gravity_data = pd . read_csv ( 'https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv' ) >>> gravity_data . head ( 5 ) importer exporter year trade_value agree_pta common_language \\ 0 AUS BRA 1989 3.035469e+08 0.0 1.0 1 AUS CAN 1989 8.769946e+08 0.0 1.0 2 AUS CHE 1989 4.005245e+08 0.0 1.0 3 AUS DEU 1989 2.468977e+09 0.0 0.0 4 AUS DNK 1989 1.763072e+08 0.0 1.0 contiguity log_distance 0 0.0 9.553332 1 0.0 9.637676 2 0.0 9.687557 3 0.0 9.675007 4 0.0 9.657311 example_estimation_data = EstimationData ( gravity_data , imp_var_name = 'importer' , exp_var_name = 'exporter' , trade_var_name = 'trade_value' , year_var_name = 'year' , notes = 'Downloaded from https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv' ) # tabulate_by_group # Sum trade value by importer and year >>> aggregated_data = example_estimation_data . tabulate_by_group ( tab_variables = [ 'trade_value' ], ... by_group = [ 'importer' , 'year' ], ... how = [ 'sum' ]) >>> aggregated_data . head ( 5 ) importer_ year_ trade_value_sum 0 ARG 1989 0.000000e+00 1 ARG 1990 0.000000e+00 2 ARG 1991 0.000000e+00 3 ARG 1992 0.000000e+00 4 ARG 1993 1.593530e+10 # Summarize minimum and maximum trade flows between each trading pair >>> summarized_data = example_estimation_data . tabulate_by_group ( tab_variables = [ 'trade_value' ], ... by_group = [ 'importer' , 'exporter' ], ... how = [ 'min' , 'max' ]) >>> summarized_data . head ( 5 ) importer_ exporter_ trade_value_min trade_value_max 0 ARG AUS 0.0 4.095529e+08 1 ARG AUT 0.0 2.986187e+08 2 ARG BEL 0.0 7.669537e+08 3 ARG BOL 0.0 2.743706e+09 4 ARG BRA 0.0 2.218091e+10 # year_list >>> example_estimation_data . year_list () [ 1989 , 1990 , 1991 , 1992 , ... # countries_each_year >>> countries = example_estimation_data . countries_each_year () >>> countries . keys () dict_keys ([ 1989 , 1990 , 1991 , 1992 , 1993 , 1994 , 1995 , 1996 , 1997 , 1998 , 1999 , 2000 , 2001 , 2002 , 2003 , 2004 , 2005 , 2006 , 2007 , 2008 , 2009 , 2010 , 2011 , 2012 , 2013 , 2014 , 2015 ]) >>> countries [ 1989 ] [ 'ESP' , 'SGP' , 'PHL' , 'NGA' , 'VEN' , ... # dtypes >>> example_estimation_data . dtypes () importer object exporter object year int64 trade_value float64 agree_pta float64 common_language float64 contiguity float64 log_distance float64 dtype : object >>> example_estimation_data . info () < class ' pandas . core . frame . DataFrame '> RangeIndex : 98612 entries , 0 to 98611 Data columns ( total 8 columns ): importer 98612 non - null object exporter 98612 non - null object year 98612 non - null int64 trade_value 98612 non - null float64 agree_pta 97676 non - null float64 common_language 97676 non - null float64 contiguity 97676 non - null float64 log_distance 97676 non - null float64 dtypes : float64 ( 5 ), int64 ( 1 ), object ( 2 ) memory usage : 6.0 + MB # describe >>> example_estimation_data . describe () year trade_value agree_pta common_language \\ count 98612.000000 9.861200e+04 97676.000000 97676.000000 mean 2002.210441 1.856316e+09 0.381547 0.380646 std 7.713050 1.004735e+10 0.485769 0.485548 min 1989.000000 0.000000e+00 0.000000 0.000000 25 % 1996.000000 1.084703e+06 0.000000 0.000000 50 % 2002.000000 6.597395e+07 0.000000 0.000000 75 % 2009.000000 6.125036e+08 1.000000 1.000000 max 2015.000000 4.977686e+11 1.000000 1.000000 contiguity log_distance count 97676.000000 97676.000000 mean 0.034051 8.722631 std 0.181362 0.818818 min 0.000000 5.061335 25 % 0.000000 8.222970 50 % 0.000000 9.012502 75 % 0.000000 9.303026 max 1.000000 9.890765 # notes >>> example_estimation_data . add_note ( 'year IDs are integers' ) >>> example_estimation_data . notes [ 'Downloaded from https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv' , 'year IDs are integers' ]","title":"EstimationData"},{"location":"api_docs/EstimationData/#class","text":"gme.EstimationData ( data_frame=None, name:str='unnamed', imp_var_name:str='importer', exp_var_name:str='exporter', year_var_name:str='year', trade_var_name:str=None, sector_var_name:str=None, notes:List[str]=[] )","title":"Class"},{"location":"api_docs/EstimationData/#description","text":"An object used for storing data for gravity modeling and producing some summary statistics.","title":"Description"},{"location":"api_docs/EstimationData/#arguments","text":"data_frame : Pandas.DataFrame \u2003 A DataFrame containing trade, gravity, etc. data. name : (optional) str \u2003 A name for the dataset. imp_var_name : str \u2003 The name of the column containing importer IDs. exp_var_name : str \u2003 The name of the column containing exporter IDs. year_var_name : str \u2003 The name of the column containing year data. trade_var_name : (optional) str \u2003 The name of the column containing trade data. sector_var_name : (optional) str \u2003 The name of the column containing sector/industry/product IDs. notes : (optional) str \u2003 A string to be included as a note n the object.","title":"Arguments"},{"location":"api_docs/EstimationData/#attributes","text":"data_frame : Pandas.DataFrame \u2003 The supplied DataFrame. name : str \u2003 The supplied data name. imp_var_name : str \u2003 The name of the column containing importer IDs. exp_var_name : str \u2003 The name of the column containing exporter IDs. year_var_name : str \u2003 The name of the column containing year data. trade_var_name : str \u2003 The name of the column containing trade data. sector_var_name : str \u2003 The name of the column containing sector/industry/product IDs. notes : List[str] \u2003 A list of notes. number_of_exporters : int \u2003 The number of unique exporter IDs in the dataset. number_of_importers : int \u2003 The number of unique importer IDs in the dataset. shape : List[int] \u2003 The dimensions of the dataset. countries : List[str] \u2003 A list of the unique country IDs in the dataset. number_of_countries : int \u2003 The number of unique country IDs in the dataset. number_of_years : int \u2003 The number of years in the dataset columns : List[str] \u2003 A list of column names in the dataset. number_of_sectors : int \u2003 If a sector is specified, the number of unique sector IDs in the dataset.","title":"Attributes"},{"location":"api_docs/EstimationData/#methods","text":"tablulate_by_group : \u2003 Summarize columns by a user-specified grouping. Can be used to tabulate, aggregate, \u2003 summarize,etc. data. \u2003 Arguments: \u2003\u2003 tab_variables : List[str] Column names of variables to be tabulated \u2003\u2003 by_group : List[str] Column names of variables by which to group observations for tabulation. \u2003\u2003 how : List[str] The method by which to combine observations within a group. Can accept 'count', 'mean', 'median', 'min', 'max', 'sum', 'prod', 'std', and 'var'. It may work with other numpy or pandas functions. \u2003 Returns : Pandas.DataFrame \u2003\u2003 A DataFrame of tabulated values for each group. year_list : \u2003 Returns a list of years present in the data. countries_each_year : \u2003 Returns a dictionary keyed by year ID containing a list of country IDs present in each \u2003 corresponding year. sector_list : \u2003 Returns a list of unique sector IDs dtypes : \u2003 Returns the data types of the columns in the EstimationData.data_frame using \u2003 Pandas.DataFrame.dtypes(). See Pandas documentation for more information. info : \u2003 Print summary information about EstimationData.data_frame using \u2003 Pandas.DataFrame.dtypes(). See Pandas documentation for more information. describe : \u2003 Generates some descriptive statistics for EstimationData.data_frame using \u2003 Pandas.DataFrame.describe(). See Pandas documentation for more information. add_note : \u2003 Add a note to the list of notes in 'notes' attribute. \u2003 Arguments : \u2003\u2003 note : str A note to add to EstimationData. \u2003 Returns : None correlation : \u2003 Return and plot the correlation matrix of the data. \u2003 Arguments : \u2003\u2003 columns : List[str] (optional) A list of column names to include in the matrix. Default is to include all columns. \u2003\u2003 plot : bool If True, it plots a heatmap of the correlation matrix. \u2003 Returns : Pandas.DataFrame \u2003\u2003 A correlation matrix add_pair_var : \u2003 Create a new variable that is a concatenation of categorical variables for use as a fixed effect or \u2003 cluster category, for example. Original values are separated by '_' in the concatenated value. \u2003 Arguments : \u2003\u2003 var_list : List[str] List of names of columns to concatinate. \u2003\u2003 var_name : (Optional) str Column name to use for the new variable. By default, it uses the names in var_list separated by '_'. \u2003\u2003 symmetric : bool If False, all variables are concatenated in the order of the var_list (e.g. ARG_USA and USA_ARG would both would be created). If True, ignores ordering of values across columns and concatenates alphabetically (e.g. all ARG and USA pairings would concatenate as ARG_USA) \u2003 Returns : None , adds new column to EstimationData.dataframe.","title":"Methods"},{"location":"api_docs/EstimationData/#examples","text":"# Load a DataFrame >>> import pandas as pd >>> gravity_data = pd . read_csv ( 'https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv' ) >>> gravity_data . head ( 5 ) importer exporter year trade_value agree_pta common_language \\ 0 AUS BRA 1989 3.035469e+08 0.0 1.0 1 AUS CAN 1989 8.769946e+08 0.0 1.0 2 AUS CHE 1989 4.005245e+08 0.0 1.0 3 AUS DEU 1989 2.468977e+09 0.0 0.0 4 AUS DNK 1989 1.763072e+08 0.0 1.0 contiguity log_distance 0 0.0 9.553332 1 0.0 9.637676 2 0.0 9.687557 3 0.0 9.675007 4 0.0 9.657311 example_estimation_data = EstimationData ( gravity_data , imp_var_name = 'importer' , exp_var_name = 'exporter' , trade_var_name = 'trade_value' , year_var_name = 'year' , notes = 'Downloaded from https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv' ) # tabulate_by_group # Sum trade value by importer and year >>> aggregated_data = example_estimation_data . tabulate_by_group ( tab_variables = [ 'trade_value' ], ... by_group = [ 'importer' , 'year' ], ... how = [ 'sum' ]) >>> aggregated_data . head ( 5 ) importer_ year_ trade_value_sum 0 ARG 1989 0.000000e+00 1 ARG 1990 0.000000e+00 2 ARG 1991 0.000000e+00 3 ARG 1992 0.000000e+00 4 ARG 1993 1.593530e+10 # Summarize minimum and maximum trade flows between each trading pair >>> summarized_data = example_estimation_data . tabulate_by_group ( tab_variables = [ 'trade_value' ], ... by_group = [ 'importer' , 'exporter' ], ... how = [ 'min' , 'max' ]) >>> summarized_data . head ( 5 ) importer_ exporter_ trade_value_min trade_value_max 0 ARG AUS 0.0 4.095529e+08 1 ARG AUT 0.0 2.986187e+08 2 ARG BEL 0.0 7.669537e+08 3 ARG BOL 0.0 2.743706e+09 4 ARG BRA 0.0 2.218091e+10 # year_list >>> example_estimation_data . year_list () [ 1989 , 1990 , 1991 , 1992 , ... # countries_each_year >>> countries = example_estimation_data . countries_each_year () >>> countries . keys () dict_keys ([ 1989 , 1990 , 1991 , 1992 , 1993 , 1994 , 1995 , 1996 , 1997 , 1998 , 1999 , 2000 , 2001 , 2002 , 2003 , 2004 , 2005 , 2006 , 2007 , 2008 , 2009 , 2010 , 2011 , 2012 , 2013 , 2014 , 2015 ]) >>> countries [ 1989 ] [ 'ESP' , 'SGP' , 'PHL' , 'NGA' , 'VEN' , ... # dtypes >>> example_estimation_data . dtypes () importer object exporter object year int64 trade_value float64 agree_pta float64 common_language float64 contiguity float64 log_distance float64 dtype : object >>> example_estimation_data . info () < class ' pandas . core . frame . DataFrame '> RangeIndex : 98612 entries , 0 to 98611 Data columns ( total 8 columns ): importer 98612 non - null object exporter 98612 non - null object year 98612 non - null int64 trade_value 98612 non - null float64 agree_pta 97676 non - null float64 common_language 97676 non - null float64 contiguity 97676 non - null float64 log_distance 97676 non - null float64 dtypes : float64 ( 5 ), int64 ( 1 ), object ( 2 ) memory usage : 6.0 + MB # describe >>> example_estimation_data . describe () year trade_value agree_pta common_language \\ count 98612.000000 9.861200e+04 97676.000000 97676.000000 mean 2002.210441 1.856316e+09 0.381547 0.380646 std 7.713050 1.004735e+10 0.485769 0.485548 min 1989.000000 0.000000e+00 0.000000 0.000000 25 % 1996.000000 1.084703e+06 0.000000 0.000000 50 % 2002.000000 6.597395e+07 0.000000 0.000000 75 % 2009.000000 6.125036e+08 1.000000 1.000000 max 2015.000000 4.977686e+11 1.000000 1.000000 contiguity log_distance count 97676.000000 97676.000000 mean 0.034051 8.722631 std 0.181362 0.818818 min 0.000000 5.061335 25 % 0.000000 8.222970 50 % 0.000000 9.012502 75 % 0.000000 9.303026 max 1.000000 9.890765 # notes >>> example_estimation_data . add_note ( 'year IDs are integers' ) >>> example_estimation_data . notes [ 'Downloaded from https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv' , 'year IDs are integers' ]","title":"Examples"},{"location":"api_docs/EstimationModel/","text":"Class gme. EstimationModel ( estimation_data: gme.EstimationData = None, lhs_var: str = None, rhs_var: List[str] = None, sector_by_sector: bool = False, drop_imp_exp: List[str] = [ ], drop_imp: List[str] = [ ], drop_exp: List[str] = [ ], keep_imp_exp: List[str] = [ ], keep_imp: List[str] = [ ], keep_exp: List[str] = [ ], drop_years: List[str] = [ ], keep_years: List[str] = [ ], drop_missing: bool = True, variables_to_drop_missing: List[str] = None, fixed_effects:List[Union[str,List[str]]] = [ ], omit_fixed_effect:List[Union[str,List[str]]] = ['exporter','exporter-year', 'year'], std_errors:str = 'HC1', iteration_limit:int = 1000, drop_intratrade:bool = False, retain_modified_data:bool = False, full_results:bool = False ) Description The class used to specify and run an gravity estimation. A gme.EstimationData must be supplied along with a collection of largely optional arguments that specify variables to include, fixed effects to create, and how to perform the regression, among other options. After the definition of the model, additional methods such as estimate , which performs the PPML estimation, or combine_sector_results , which combines the results for each sector (if applicable) can be called. Arguments estimation_data : gme.EstimationData A GME EstimationData to use as the basis of the gravity model. spec_name : (optional) str A name for the model. lhs_var : str The column name of the variable to be used as the dependent or 'left-hand-side' variable in the regression. rhs_var : List[str] A list of column names for the independent or 'right-hand-side' variable(s) to be used in the regression. sector_by_sector : bool If true, separate models are estimated for each sector, individually. Default is False. If True, a sector_var_name must have been supplied to the EstimationData. drop_imp_exp : (optional) List[str] A list of country identifiers to be excluded from the estimation when they appear as an importer or exporter. drop_imp : (optional) List[str] A list of country identifiers to be excluded from the estimation when they appear as an importer. drop_exp : (optional) List[str] A list of country identifiers to be excluded from the estimation when they appear as an exporter. keep_imp_exp : (optional) List[str] A list of countries to include in the estimation as either importers or exporters. All others not specified are excluded. keep_imp : (optional) List[str] A list of countries to include in the estimation as importers. All others not specified are excluded. keep_exp : (optional) List[str] A list of countries to include in the estimation as exporters. All others not specified are excluded. drop_years : (optional) list A list of years to exclude from the estimation. The list elements should match the dtype of the year column in the EstimationData. keep_years : (optional) list A list of years to include in the estimation. The list elements should match the dtype of the year column in the EstimationData. drop_missing : bool If True, rows with missing values are dropped. Default is true, which drops if observations are missing in any of the columns specified by lhs_var or rhs_var. variables_to_drop_missing : (optional) List[str] A list of column names for specifying which columns to check for missing values when dropping rows. fixed_effects : (optional) List[Union[str,List[str]]] A list of variables to construct fixed effects based on. Can accept single string entries, which create fixed effects corresponding to that variable or lists of strings that create fixed effects corresponding to the interaction of the list items. For example, fixed_effects = ['importer',['exporter','year']] would create a set of importer fixed effects and a set of exporter-year fixed effects. omit_fixed_effect : (optional) Dict[Union[str,Tuple[str]]:List[str, Tuple[str]]] A dictionary of fixed effect categories and values to be dropped from dataframe. The dictionary key(s) can be either a single string corresponding to one dimension of the fixed effects or a tuple of strings specifying multiple dimensions. The values should then be either a list of strings for 1 dimension or a list of tuples for multiple dimensions. For example, {'importer':['DEU', 'ZAF'], 'exporter':['USA']} or {('importer','year'):[('ARG','2015'),('BEL','2013')]}. The fixed effect categories in the keys need to be a subset of the list supplied for fixed_effects. If not specified, the collinearity diagnostics will identify a column to drop on its own. (Version 1.2 or earlier) : omit_fixed_effect : (optional) List[Union[str,List[str]]] The fixed effect category from which to drop a fixed effect to avoid collinearity. The entry should be a subset of the list supplied for fixed_effects. In each case, the last fixed effect is dropped. If not specified, the colinearity diagnostics will identify a column to drop on its own. std_errors : (optional) str Specifies the type of standard errors to be computed. Default is HC1, heteroskedascticity robust errors. See [statsmodels documentation](http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.RegressionResults.html) for alternative options. iteration_limit : (optional) int Upper limit on the number of iterations for the estimation procedure. Default is 1000. drop_intratrade : (optional) bool If True, intra-national trade flows (importer == exporter) are excluded from the regression. Default is False. retain_modified_data : (optional) bool If True, the estimation DataFrames for each sector after they have been (potentially) modified during the pre-diagnostics for collinearity and convergence issues. Default is False. WARNING: these object sizes can be very large in memory so use with caution. full_results : bool If True, estimate() returns the full results object from the GLM estimation. These results can be quite large as each estimated sector's results will contain a full copy of the data used for its estimation, vectors of predicted values, and other memory intensive pieces of data. If False, estimate() returns a smaller subset of the results that are likely most useful (e.g. .params, .nobs, .bse, .pvalues, .aic, .bic). For a list of these attributes, see the documentation for the function [SlimResults](SlimResults). cluster_on : (optional) str The name of a column of categorical variables to use as clusters for clustered standard errors. Attributes estimation_data : Return the EstimationData. results_dict : Return the dictionary of regression results (after applying estimate method). modified_data : Return data modified data after removing problematic columns (after applying estimate method) ppml_diagnostics : Return PPML estimation diagnostic information (after applying estimate method). See [estimate](estimate_method.md). Methods estimate : Estimate a PPML model. See [estimate](../estimate_technical.md). combine_sector_results : Combine multiple result_dict entries into a single DataFrame. See [combine_sector_results](combine_sector_results.md). format_regression_table : Format regression results into a text, csv, or LaTeX table for presentation. See [format_regression_table](format_regression_table.md) Examples # Declare an EstimationModel >>> sample_estimation_model = gme . EstimationModel ( data_object = gme_data , ... lhs_var = 'trade_value' , ... rhs_var = [ 'log_distance' , ... 'agree_pta' , ... 'common_language' , ... 'contiguity' ]) # Estimate the model >>> sample_estimation_model . estimate () # Extract the results >>> results_dictionary = sample_estimation_model . results_dict # Write the estimates, p-values, and std. errors from all sectors to a .csv file. >>> sample_estimation_model . combine_sector_results ( \"c: \\\\ folder \\\\ saved_results.csv\" ) # Create and export a formatted table of estimation results >>> sample_estimation_model . format_regression_table ( format = 'csv' , ... path = \"c: \\\\ folder \\\\ saved_results.csv\" ) # Define a model with fixed effects and drop specific ones >>> fe_model = EstimationModel ( est_data , lhs_var = 'trade_value' , rhs_var = grav_vars , fixed_effects = [[ 'importer' , 'year' ], [ 'exporter' , 'year' ]], omit_fixed_effect = {( 'importer' , 'year' ):[( 'DEU' , '2015' )], 'exporter' :[ 'USA' ]})","title":"EstimationModel"},{"location":"api_docs/EstimationModel/#class","text":"gme. EstimationModel ( estimation_data: gme.EstimationData = None, lhs_var: str = None, rhs_var: List[str] = None, sector_by_sector: bool = False, drop_imp_exp: List[str] = [ ], drop_imp: List[str] = [ ], drop_exp: List[str] = [ ], keep_imp_exp: List[str] = [ ], keep_imp: List[str] = [ ], keep_exp: List[str] = [ ], drop_years: List[str] = [ ], keep_years: List[str] = [ ], drop_missing: bool = True, variables_to_drop_missing: List[str] = None, fixed_effects:List[Union[str,List[str]]] = [ ], omit_fixed_effect:List[Union[str,List[str]]] = ['exporter','exporter-year', 'year'], std_errors:str = 'HC1', iteration_limit:int = 1000, drop_intratrade:bool = False, retain_modified_data:bool = False, full_results:bool = False )","title":"Class"},{"location":"api_docs/EstimationModel/#description","text":"The class used to specify and run an gravity estimation. A gme.EstimationData must be supplied along with a collection of largely optional arguments that specify variables to include, fixed effects to create, and how to perform the regression, among other options. After the definition of the model, additional methods such as estimate , which performs the PPML estimation, or combine_sector_results , which combines the results for each sector (if applicable) can be called.","title":"Description"},{"location":"api_docs/EstimationModel/#arguments","text":"estimation_data : gme.EstimationData A GME EstimationData to use as the basis of the gravity model. spec_name : (optional) str A name for the model. lhs_var : str The column name of the variable to be used as the dependent or 'left-hand-side' variable in the regression. rhs_var : List[str] A list of column names for the independent or 'right-hand-side' variable(s) to be used in the regression. sector_by_sector : bool If true, separate models are estimated for each sector, individually. Default is False. If True, a sector_var_name must have been supplied to the EstimationData. drop_imp_exp : (optional) List[str] A list of country identifiers to be excluded from the estimation when they appear as an importer or exporter. drop_imp : (optional) List[str] A list of country identifiers to be excluded from the estimation when they appear as an importer. drop_exp : (optional) List[str] A list of country identifiers to be excluded from the estimation when they appear as an exporter. keep_imp_exp : (optional) List[str] A list of countries to include in the estimation as either importers or exporters. All others not specified are excluded. keep_imp : (optional) List[str] A list of countries to include in the estimation as importers. All others not specified are excluded. keep_exp : (optional) List[str] A list of countries to include in the estimation as exporters. All others not specified are excluded. drop_years : (optional) list A list of years to exclude from the estimation. The list elements should match the dtype of the year column in the EstimationData. keep_years : (optional) list A list of years to include in the estimation. The list elements should match the dtype of the year column in the EstimationData. drop_missing : bool If True, rows with missing values are dropped. Default is true, which drops if observations are missing in any of the columns specified by lhs_var or rhs_var. variables_to_drop_missing : (optional) List[str] A list of column names for specifying which columns to check for missing values when dropping rows. fixed_effects : (optional) List[Union[str,List[str]]] A list of variables to construct fixed effects based on. Can accept single string entries, which create fixed effects corresponding to that variable or lists of strings that create fixed effects corresponding to the interaction of the list items. For example, fixed_effects = ['importer',['exporter','year']] would create a set of importer fixed effects and a set of exporter-year fixed effects. omit_fixed_effect : (optional) Dict[Union[str,Tuple[str]]:List[str, Tuple[str]]] A dictionary of fixed effect categories and values to be dropped from dataframe. The dictionary key(s) can be either a single string corresponding to one dimension of the fixed effects or a tuple of strings specifying multiple dimensions. The values should then be either a list of strings for 1 dimension or a list of tuples for multiple dimensions. For example, {'importer':['DEU', 'ZAF'], 'exporter':['USA']} or {('importer','year'):[('ARG','2015'),('BEL','2013')]}. The fixed effect categories in the keys need to be a subset of the list supplied for fixed_effects. If not specified, the collinearity diagnostics will identify a column to drop on its own. (Version 1.2 or earlier) : omit_fixed_effect : (optional) List[Union[str,List[str]]] The fixed effect category from which to drop a fixed effect to avoid collinearity. The entry should be a subset of the list supplied for fixed_effects. In each case, the last fixed effect is dropped. If not specified, the colinearity diagnostics will identify a column to drop on its own. std_errors : (optional) str Specifies the type of standard errors to be computed. Default is HC1, heteroskedascticity robust errors. See [statsmodels documentation](http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.RegressionResults.html) for alternative options. iteration_limit : (optional) int Upper limit on the number of iterations for the estimation procedure. Default is 1000. drop_intratrade : (optional) bool If True, intra-national trade flows (importer == exporter) are excluded from the regression. Default is False. retain_modified_data : (optional) bool If True, the estimation DataFrames for each sector after they have been (potentially) modified during the pre-diagnostics for collinearity and convergence issues. Default is False. WARNING: these object sizes can be very large in memory so use with caution. full_results : bool If True, estimate() returns the full results object from the GLM estimation. These results can be quite large as each estimated sector's results will contain a full copy of the data used for its estimation, vectors of predicted values, and other memory intensive pieces of data. If False, estimate() returns a smaller subset of the results that are likely most useful (e.g. .params, .nobs, .bse, .pvalues, .aic, .bic). For a list of these attributes, see the documentation for the function [SlimResults](SlimResults). cluster_on : (optional) str The name of a column of categorical variables to use as clusters for clustered standard errors.","title":"Arguments"},{"location":"api_docs/EstimationModel/#attributes","text":"estimation_data : Return the EstimationData. results_dict : Return the dictionary of regression results (after applying estimate method). modified_data : Return data modified data after removing problematic columns (after applying estimate method) ppml_diagnostics : Return PPML estimation diagnostic information (after applying estimate method). See [estimate](estimate_method.md).","title":"Attributes"},{"location":"api_docs/EstimationModel/#methods","text":"estimate : Estimate a PPML model. See [estimate](../estimate_technical.md). combine_sector_results : Combine multiple result_dict entries into a single DataFrame. See [combine_sector_results](combine_sector_results.md). format_regression_table : Format regression results into a text, csv, or LaTeX table for presentation. See [format_regression_table](format_regression_table.md)","title":"Methods"},{"location":"api_docs/EstimationModel/#examples","text":"# Declare an EstimationModel >>> sample_estimation_model = gme . EstimationModel ( data_object = gme_data , ... lhs_var = 'trade_value' , ... rhs_var = [ 'log_distance' , ... 'agree_pta' , ... 'common_language' , ... 'contiguity' ]) # Estimate the model >>> sample_estimation_model . estimate () # Extract the results >>> results_dictionary = sample_estimation_model . results_dict # Write the estimates, p-values, and std. errors from all sectors to a .csv file. >>> sample_estimation_model . combine_sector_results ( \"c: \\\\ folder \\\\ saved_results.csv\" ) # Create and export a formatted table of estimation results >>> sample_estimation_model . format_regression_table ( format = 'csv' , ... path = \"c: \\\\ folder \\\\ saved_results.csv\" ) # Define a model with fixed effects and drop specific ones >>> fe_model = EstimationModel ( est_data , lhs_var = 'trade_value' , rhs_var = grav_vars , fixed_effects = [[ 'importer' , 'year' ], [ 'exporter' , 'year' ]], omit_fixed_effect = {( 'importer' , 'year' ):[( 'DEU' , '2015' )], 'exporter' :[ 'USA' ]})","title":"Examples"},{"location":"api_docs/SlimResults/","text":"Class SlimResults ( glm_results=None ) Description Create a version of the dictionary of results objects that uses less memory. This is the format of results dictionaries generated under the default options in EstimationModel (i.e. the following argument is executed: EstimationModel(..., full_results=False) ). The SlimResults object is a smaller subset of the GLMResultsWrapper object in the statsmodels package(for more info, see statsmodels' GLMResults ). Large attributes, such as copies of the estimating data, are removed from the results to cut back on memory size. The results most commonly referenced are retained, though. The SlimResults object retains only the attributes listed below. For additional information see the documentation for the GLMResultsWrapper in the statsmodels package. Arguments glm_results : statsmodels.genmod.generalized_linear_model.GLMResultsWrapper \u2003 An instance of the statsmodels.GLM.fit() results object Atributes params : Pandas Series \u2003 Estimated parameter values aic : float \u2003 Akaike Information Criterion bic : float \u2003 Bayes Information Criterion llf : float \u2003 Value of log-likelihood function nobs : float \u2003 number of observations bse : Pandas Series \u2003 Beta standard errors for parameter estimates pvalues : Pandas Series \u2003 Two-tailed pvalues for parameter estimates family_name : str \u2003 Name of distribution family used family_link : str \u2003 Estimation link function method : str \u2003 Estimation method fit_history : int \u2003 Number of iterations completed scale : float \u2003 The estimate of the scale / dispersion for the model fit deviance : float \u2003 Deviance measure pearson_chi2 : Pandas Series \u2003 Chi-squared statistic cov_type : str \u2003 Covariance type yname : str \u2003 Column name of endogenous variable xname : List[str] \u2003 Column names of exogenous variables model : str \u2003 Model used for fit df_resid : float df_model : float tvalues : Pandas Series \u2003 T statistics fittedvalues : Pandas Series \u2003 Linear predicted values Methods The SlimResults object replicates two methods from the original GLMResultsWrapper object from statsmodels. conf_int : array create confidence intervals for parameter estimates. **Arguments**: \u2003 **alpha**: (optional) *float* \u2003\u2003 The significance level for the confidence interval. \u2003\u2003 I.e., The default `alpha` = .05 returns a 95% confidence interval. \u2003 **cols**: (optional) *array-like* \u2003\u2003 `cols` specifies which confidence intervals to return summary : object \u2003 print a table summarizing estimation results (replicates statsmodels summary method \u2003 for GLM).","title":"SlimResults"},{"location":"api_docs/SlimResults/#class","text":"SlimResults ( glm_results=None )","title":"Class"},{"location":"api_docs/SlimResults/#description","text":"Create a version of the dictionary of results objects that uses less memory. This is the format of results dictionaries generated under the default options in EstimationModel (i.e. the following argument is executed: EstimationModel(..., full_results=False) ). The SlimResults object is a smaller subset of the GLMResultsWrapper object in the statsmodels package(for more info, see statsmodels' GLMResults ). Large attributes, such as copies of the estimating data, are removed from the results to cut back on memory size. The results most commonly referenced are retained, though. The SlimResults object retains only the attributes listed below. For additional information see the documentation for the GLMResultsWrapper in the statsmodels package.","title":"Description"},{"location":"api_docs/SlimResults/#arguments","text":"glm_results : statsmodels.genmod.generalized_linear_model.GLMResultsWrapper \u2003 An instance of the statsmodels.GLM.fit() results object","title":"Arguments"},{"location":"api_docs/SlimResults/#atributes","text":"params : Pandas Series \u2003 Estimated parameter values aic : float \u2003 Akaike Information Criterion bic : float \u2003 Bayes Information Criterion llf : float \u2003 Value of log-likelihood function nobs : float \u2003 number of observations bse : Pandas Series \u2003 Beta standard errors for parameter estimates pvalues : Pandas Series \u2003 Two-tailed pvalues for parameter estimates family_name : str \u2003 Name of distribution family used family_link : str \u2003 Estimation link function method : str \u2003 Estimation method fit_history : int \u2003 Number of iterations completed scale : float \u2003 The estimate of the scale / dispersion for the model fit deviance : float \u2003 Deviance measure pearson_chi2 : Pandas Series \u2003 Chi-squared statistic cov_type : str \u2003 Covariance type yname : str \u2003 Column name of endogenous variable xname : List[str] \u2003 Column names of exogenous variables model : str \u2003 Model used for fit df_resid : float df_model : float tvalues : Pandas Series \u2003 T statistics fittedvalues : Pandas Series \u2003 Linear predicted values","title":"Atributes"},{"location":"api_docs/SlimResults/#methods","text":"The SlimResults object replicates two methods from the original GLMResultsWrapper object from statsmodels. conf_int : array create confidence intervals for parameter estimates. **Arguments**: \u2003 **alpha**: (optional) *float* \u2003\u2003 The significance level for the confidence interval. \u2003\u2003 I.e., The default `alpha` = .05 returns a 95% confidence interval. \u2003 **cols**: (optional) *array-like* \u2003\u2003 `cols` specifies which confidence intervals to return summary : object \u2003 print a table summarizing estimation results (replicates statsmodels summary method \u2003 for GLM).","title":"Methods"},{"location":"api_docs/coefficient_kd_plot/","text":"Function gme. coefficient_kd_plot ( estimation_model: EstimationModel, variables: List[str], path: str = None, bandwidth: float = 0.5, rename_variables: dict = None ): Description Produce kernel density plots of parameter estimates across different sectors in the results dictionary. Arguments estimation_model : gme.EstimationModel An estimated EstimationModel with more than one sector. variables : List[str] A list of model covariates for which to plot kernel densities. path : (optional) str A path and file name at which to save the plot. Can end in the following file types for example: pdf, svg, and png. bandwidth : float Specify the bandwidth for the density plots. The default is 0.5. rename_variables : (optional) A dictionary of alternative variable names to use in the plot. For example {'original_name':'new_name'} Return None , plots a figure and, if specified, writes it to a file. Examples >>> est_model = EstimationModel ( est_data , lhs_var = 'trade_value' , rhs_var = [ 'contiguity' , 'agree_pta' , 'border' , 'colony' , 'ln_dist' ], fixed_effects = [[ 'importer' , 'year' ], [ 'exporter' , 'year' ]], sector_by_sector = True ) >>> est_model . estimate () >>> coefficient_kd_plot ( est_model , variables = [ 'contiguity' , 'agree_pta' , 'border' , 'colony' , 'ln_dist' ], rename_variables = { 'agree_pta' : 'PTA' , 'ln_dist' : 'distance' }, path = 'C:\\kd_plot.eps' )","title":"coefficient_kd_plot"},{"location":"api_docs/coefficient_kd_plot/#function","text":"gme. coefficient_kd_plot ( estimation_model: EstimationModel, variables: List[str], path: str = None, bandwidth: float = 0.5, rename_variables: dict = None ):","title":"Function"},{"location":"api_docs/coefficient_kd_plot/#description","text":"Produce kernel density plots of parameter estimates across different sectors in the results dictionary.","title":"Description"},{"location":"api_docs/coefficient_kd_plot/#arguments","text":"estimation_model : gme.EstimationModel An estimated EstimationModel with more than one sector. variables : List[str] A list of model covariates for which to plot kernel densities. path : (optional) str A path and file name at which to save the plot. Can end in the following file types for example: pdf, svg, and png. bandwidth : float Specify the bandwidth for the density plots. The default is 0.5. rename_variables : (optional) A dictionary of alternative variable names to use in the plot. For example {'original_name':'new_name'}","title":"Arguments"},{"location":"api_docs/coefficient_kd_plot/#return","text":"None , plots a figure and, if specified, writes it to a file.","title":"Return"},{"location":"api_docs/coefficient_kd_plot/#examples","text":">>> est_model = EstimationModel ( est_data , lhs_var = 'trade_value' , rhs_var = [ 'contiguity' , 'agree_pta' , 'border' , 'colony' , 'ln_dist' ], fixed_effects = [[ 'importer' , 'year' ], [ 'exporter' , 'year' ]], sector_by_sector = True ) >>> est_model . estimate () >>> coefficient_kd_plot ( est_model , variables = [ 'contiguity' , 'agree_pta' , 'border' , 'colony' , 'ln_dist' ], rename_variables = { 'agree_pta' : 'PTA' , 'ln_dist' : 'distance' }, path = 'C:\\kd_plot.eps' )","title":"Examples"},{"location":"api_docs/combine_sector_results/","text":"Function combine_sector_results ( result_dict:dict = None, write_path:str = None, significance_stars: bool = False, round_results: int = None, latex_syntax: bool = True ) Description Extract key result fields (coefficients, standard errors, and p-values) and combine them in a DataFrame. Has the option to write the data to a .csv file with or without extra value formatting options. Arguments result_dict : Dict[statsmodels.genmod.generalized_linear_model.GLMResultsWrapper] A dictionary of GLM fit objects as returned by gme.EsimationModel.estimate() write_path : (optional) str A system location and file name in which to write a csv file containing the combined results. significance_stars : bool If true, combined results are output with significance stars. \\*\\*\\*<0.01, \\*\\*<0.05, and \\*<0.10. Default is False. round_results : (optional) int Rounds combined results to the desired decimal place. latex_syntax : bool If True, reports aspects of results, such as significance stars, using standard latex syntax. Returns Returns : Pandas.DataFrame A DataFrame containing combined GLM results for all results in the supplied dictionary. Examples # Using a gme.EstimationModel named 'sample_model'. >>> sample_results = sample_model . estimate () # Return a dataframe of results >>> result_df = combine_sector_results ( sample_results ) >>> result_df . head ( 5 ) all_coeff all_pvalue all_stderr log_distance - 0.739840 9.318804e-211 0.023879 agree_pta 0.334219 5.134355e-15 0.042719 common_language 0.128770 1.076932e-03 0.039383 contiguity 0.255161 5.857612e-08 0.047050 importer_year_fe_ARG2013 26.980367 0.000000e+00 0.361228 # Export table as a .csv file >>> combine_sector_results ( sample_results , ... round_results = 3 , ... path = 'c: \\\\ Documents \\\\ combined_results_saved.csv' )","title":"combine_sector_results"},{"location":"api_docs/combine_sector_results/#function","text":"combine_sector_results ( result_dict:dict = None, write_path:str = None, significance_stars: bool = False, round_results: int = None, latex_syntax: bool = True )","title":"Function"},{"location":"api_docs/combine_sector_results/#description","text":"Extract key result fields (coefficients, standard errors, and p-values) and combine them in a DataFrame. Has the option to write the data to a .csv file with or without extra value formatting options.","title":"Description"},{"location":"api_docs/combine_sector_results/#arguments","text":"result_dict : Dict[statsmodels.genmod.generalized_linear_model.GLMResultsWrapper] A dictionary of GLM fit objects as returned by gme.EsimationModel.estimate() write_path : (optional) str A system location and file name in which to write a csv file containing the combined results. significance_stars : bool If true, combined results are output with significance stars. \\*\\*\\*<0.01, \\*\\*<0.05, and \\*<0.10. Default is False. round_results : (optional) int Rounds combined results to the desired decimal place. latex_syntax : bool If True, reports aspects of results, such as significance stars, using standard latex syntax.","title":"Arguments"},{"location":"api_docs/combine_sector_results/#returns","text":"Returns : Pandas.DataFrame A DataFrame containing combined GLM results for all results in the supplied dictionary.","title":"Returns"},{"location":"api_docs/combine_sector_results/#examples","text":"# Using a gme.EstimationModel named 'sample_model'. >>> sample_results = sample_model . estimate () # Return a dataframe of results >>> result_df = combine_sector_results ( sample_results ) >>> result_df . head ( 5 ) all_coeff all_pvalue all_stderr log_distance - 0.739840 9.318804e-211 0.023879 agree_pta 0.334219 5.134355e-15 0.042719 common_language 0.128770 1.076932e-03 0.039383 contiguity 0.255161 5.857612e-08 0.047050 importer_year_fe_ARG2013 26.980367 0.000000e+00 0.361228 # Export table as a .csv file >>> combine_sector_results ( sample_results , ... round_results = 3 , ... path = 'c: \\\\ Documents \\\\ combined_results_saved.csv' )","title":"Examples"},{"location":"api_docs/estimate_method/","text":"Function estimate () Description The method estimate performs a sector-by-sector GLM estimation based on a Poisson distribution with data diagnostics that help increase the likelihood of convergence. If sector_by_sector is specified, the routine is repeated for each sector individually, estimating a separate model each time. The estimate routine inherits all specifications from those supplied to the EstimationModel . The routine follows several steps. Creates Fixed effects : Fixed effects are created based on the EstimationModel specification. Pre-Diagnostics : Several steps are taken to increase the likelihood that the estimation will converge successfully. Click here to technical details. Perfect Colinearity: Columns and observations that are perfectly collinear are identified and excluded. Insufficient Variation: Variables in which there is an insufficient level of variation for estimation are excluded. These are typically cases in which a country does not import or export at all for a given level of fixed effect. Estimate : Estimation is run using GLM.fit in statsmodels for the Poisson family distribution. Robust standard errors are computed using the HC1 version of the Huber-White estimator for heteroscedasticity consistent covariance matrix. Post-Diagnostics : A test for over-fit values as in Santos Silva and Tenreyro (2011) . Results : The method returns EstimationModel.results_dict and stores two others ( EstimationModel.ppml_diagnostics and EstimationModel.modified_data ) as attributes of the EstimationModel . EstimationModel.results_dict : This is a dictionary of results objects from the statsmodels GLM.fit routine, each keyed using either the name of the sector if the estimation was sector-by-sector (i.e. sector_by_sector = True ) or with the key 'all' if not. It is both returned and stored as EstimationModel.results_dict . 1 EstimationModel.ppml_diagnostics : A data frame containing a column of pre- and post-diagnostic information for each regression EstimationModel.modified_data : A dictionary using the same keys as results_dict, each containing the modified DataFrames created during the pre-diagnostic stages of the estimations. Because of the large memory footprint of this assignment, storing it is optional and only done if specified (i.e. EstimationModel.retain_modified_data = True ) Example # Create fixed effects and specify sector by sector estimation >>> gme_data = gme . EstimationData ( data_frame = sample_data , imp_var_name = 'importer' , exp_var_name = 'exporter' , sector_var_name = 'sector' trade_var_name = 'trade_value' , year_var_name = 'year' ) >>> sample_estimation_model = gme . EstimationModel ( estimation_data = gme_data , lhs_var = 'trade_value' , rhs_var = [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' ], fixed_effects = [ 'importer' , 'exporter' ], keep_years = [ 2013 , 2014 , 2015 ], sector_by_sector = True ) # Estimate the model >>> sample_estimation_model . estimate () # Generate post-diagnostics >>> diag = sample_estimation_model . ppml_diagnostics >>> print ( diag ) Overfit Warning No Collinearities No Number of Columns Excluded 3 Perfectly Collinear Variables [] Zero Trade Variables [ importer_fe_IRN , importer_fe_LBY , importer_fe ... # Extract the results to a new data frame and save to a .csv >>> results_dictionary = sample_estimation_model . results_dict ( \"c: \\f older\\saved_results.csv\" ) For more details about the statsmodels results object, see http://www.statsmodels.org/0.6.1/generated/statsmodels.genmod.generalized_linear_model.GLMResults.html . \u21a9","title":"estimate"},{"location":"api_docs/estimate_method/#function","text":"estimate ()","title":"Function"},{"location":"api_docs/estimate_method/#description","text":"The method estimate performs a sector-by-sector GLM estimation based on a Poisson distribution with data diagnostics that help increase the likelihood of convergence. If sector_by_sector is specified, the routine is repeated for each sector individually, estimating a separate model each time. The estimate routine inherits all specifications from those supplied to the EstimationModel . The routine follows several steps. Creates Fixed effects : Fixed effects are created based on the EstimationModel specification. Pre-Diagnostics : Several steps are taken to increase the likelihood that the estimation will converge successfully. Click here to technical details. Perfect Colinearity: Columns and observations that are perfectly collinear are identified and excluded. Insufficient Variation: Variables in which there is an insufficient level of variation for estimation are excluded. These are typically cases in which a country does not import or export at all for a given level of fixed effect. Estimate : Estimation is run using GLM.fit in statsmodels for the Poisson family distribution. Robust standard errors are computed using the HC1 version of the Huber-White estimator for heteroscedasticity consistent covariance matrix. Post-Diagnostics : A test for over-fit values as in Santos Silva and Tenreyro (2011) . Results : The method returns EstimationModel.results_dict and stores two others ( EstimationModel.ppml_diagnostics and EstimationModel.modified_data ) as attributes of the EstimationModel . EstimationModel.results_dict : This is a dictionary of results objects from the statsmodels GLM.fit routine, each keyed using either the name of the sector if the estimation was sector-by-sector (i.e. sector_by_sector = True ) or with the key 'all' if not. It is both returned and stored as EstimationModel.results_dict . 1 EstimationModel.ppml_diagnostics : A data frame containing a column of pre- and post-diagnostic information for each regression EstimationModel.modified_data : A dictionary using the same keys as results_dict, each containing the modified DataFrames created during the pre-diagnostic stages of the estimations. Because of the large memory footprint of this assignment, storing it is optional and only done if specified (i.e. EstimationModel.retain_modified_data = True )","title":"Description"},{"location":"api_docs/estimate_method/#example","text":"# Create fixed effects and specify sector by sector estimation >>> gme_data = gme . EstimationData ( data_frame = sample_data , imp_var_name = 'importer' , exp_var_name = 'exporter' , sector_var_name = 'sector' trade_var_name = 'trade_value' , year_var_name = 'year' ) >>> sample_estimation_model = gme . EstimationModel ( estimation_data = gme_data , lhs_var = 'trade_value' , rhs_var = [ 'log_distance' , 'agree_pta' , 'common_language' , 'contiguity' ], fixed_effects = [ 'importer' , 'exporter' ], keep_years = [ 2013 , 2014 , 2015 ], sector_by_sector = True ) # Estimate the model >>> sample_estimation_model . estimate () # Generate post-diagnostics >>> diag = sample_estimation_model . ppml_diagnostics >>> print ( diag ) Overfit Warning No Collinearities No Number of Columns Excluded 3 Perfectly Collinear Variables [] Zero Trade Variables [ importer_fe_IRN , importer_fe_LBY , importer_fe ... # Extract the results to a new data frame and save to a .csv >>> results_dictionary = sample_estimation_model . results_dict ( \"c: \\f older\\saved_results.csv\" ) For more details about the statsmodels results object, see http://www.statsmodels.org/0.6.1/generated/statsmodels.genmod.generalized_linear_model.GLMResults.html . \u21a9","title":"Example"},{"location":"api_docs/format_regression_table/","text":"Function gme. format_regression_table ( results_dict:dict = None, variable_list:List[str] = [], format:str = 'txt', se_below:bool = True, significance_levels:List[float] = [0.1,0.05,0.10], round_values:int = 3, omit_fe_prefix:List[str] = [], table_columns:list = [], path:str = None, include_index:bool = False, latex_syntax:bool = False, r_squared:bool = False ): Description Format estimation results into a standard table format with options for significance stars, LaTeX syntax, standard error positioning, rounding, fixed effect omission, and others options. Arguments results_dict : Dict[statsmodels.genmod.generalized_linear_model.GLMResultsWrapper] A dictionary of GLM fit objects from statsmodels variable_list : (optional) List[str] A list of variables to include in the results table. If none are provided, all variables are included. The default is an empty list, which results in the inclusion of all estimated variables. format : str Determines the file formatting of text. Accepts 'tex' for LaTeX, 'txt' for plain text, or 'csv' for a csv table. Default is 'txt'. se_below : bool If True, standard errors are presented below estimates. If False, they are presented in a column to the right. The default is True. significance_levels : List[float] A list specifying the three percentages, from lowest to highest, on which to base significance stars. The default value is [0.01, 0.05, 0.10]. round_values : int The number of decimal points to include in the reported figures. The default is 3. omit_fe_prefix : (optional) List[str] A list of strings such that any variable starting with that string are omitted from the created table. The value is an empty list that omits no variables. table_columns : (optional) List[str] A list of keys from the results_dict to be included in the created table. The default is an empty list, which results in all values being created path : (optional) str A system path and file name to write the created table to. File extensions of .txt (format = 'txt'), .tex or .txt (format = 'tex'), or .csv (format = 'csv') are recommended. include_index : bool/ If true, the outputed .csv file will contain row numbers. Default is False. latex_syntax : bool If true, the table will include LaTeX syntax, regardless of the chosen format. Default is False. variable_order : (optional) List[str] If supplied, provides an specific ordering in which to list the variables in the table. r_squared : bool If True, it includes R^2 values in the table. This is primarily useful if OLS regression results are supplied. Default is False. note : (optional) str Adds an optional note to the bottom of a table written via the path argument. Returns Returns: Pandas.DataFrame A DataFrame containing the formatted results table with specified syntax. Examples # Create a .csv file. >>> sample_estimation_model . format_regression_table ( format = 'csv' , path = \"c: \\f older\\saved_results.csv\" ) # Create a LaTeX .tex table without fixed effects (with prefix 'imp_fe_' and 'exp_fe_') >>> sample_estimation_model . format_regression_table ( format = 'tex' , ... path = \"c: \\f older\\saved_results.tex\" , ... omit_fe_prefix = [ 'imp_fe_' , 'exp_fe_' ], ... note = 'Estimation run on July 7, 2020 by Peter Herman.' )","title":"format_regression_table"},{"location":"api_docs/format_regression_table/#function","text":"gme. format_regression_table ( results_dict:dict = None, variable_list:List[str] = [], format:str = 'txt', se_below:bool = True, significance_levels:List[float] = [0.1,0.05,0.10], round_values:int = 3, omit_fe_prefix:List[str] = [], table_columns:list = [], path:str = None, include_index:bool = False, latex_syntax:bool = False, r_squared:bool = False ):","title":"Function"},{"location":"api_docs/format_regression_table/#description","text":"Format estimation results into a standard table format with options for significance stars, LaTeX syntax, standard error positioning, rounding, fixed effect omission, and others options.","title":"Description"},{"location":"api_docs/format_regression_table/#arguments","text":"results_dict : Dict[statsmodels.genmod.generalized_linear_model.GLMResultsWrapper] A dictionary of GLM fit objects from statsmodels variable_list : (optional) List[str] A list of variables to include in the results table. If none are provided, all variables are included. The default is an empty list, which results in the inclusion of all estimated variables. format : str Determines the file formatting of text. Accepts 'tex' for LaTeX, 'txt' for plain text, or 'csv' for a csv table. Default is 'txt'. se_below : bool If True, standard errors are presented below estimates. If False, they are presented in a column to the right. The default is True. significance_levels : List[float] A list specifying the three percentages, from lowest to highest, on which to base significance stars. The default value is [0.01, 0.05, 0.10]. round_values : int The number of decimal points to include in the reported figures. The default is 3. omit_fe_prefix : (optional) List[str] A list of strings such that any variable starting with that string are omitted from the created table. The value is an empty list that omits no variables. table_columns : (optional) List[str] A list of keys from the results_dict to be included in the created table. The default is an empty list, which results in all values being created path : (optional) str A system path and file name to write the created table to. File extensions of .txt (format = 'txt'), .tex or .txt (format = 'tex'), or .csv (format = 'csv') are recommended. include_index : bool/ If true, the outputed .csv file will contain row numbers. Default is False. latex_syntax : bool If true, the table will include LaTeX syntax, regardless of the chosen format. Default is False. variable_order : (optional) List[str] If supplied, provides an specific ordering in which to list the variables in the table. r_squared : bool If True, it includes R^2 values in the table. This is primarily useful if OLS regression results are supplied. Default is False. note : (optional) str Adds an optional note to the bottom of a table written via the path argument.","title":"Arguments"},{"location":"api_docs/format_regression_table/#returns","text":"Returns: Pandas.DataFrame A DataFrame containing the formatted results table with specified syntax.","title":"Returns"},{"location":"api_docs/format_regression_table/#examples","text":"# Create a .csv file. >>> sample_estimation_model . format_regression_table ( format = 'csv' , path = \"c: \\f older\\saved_results.csv\" ) # Create a LaTeX .tex table without fixed effects (with prefix 'imp_fe_' and 'exp_fe_') >>> sample_estimation_model . format_regression_table ( format = 'tex' , ... path = \"c: \\f older\\saved_results.tex\" , ... omit_fe_prefix = [ 'imp_fe_' , 'exp_fe_' ], ... note = 'Estimation run on July 7, 2020 by Peter Herman.' )","title":"Examples"},{"location":"api_docs/load_estimation/","text":"Function load_estimation ( filename: str ) Description Load a saved estimation model Arguments filename : str \u2003 The path and file name under which an object was saved. Returns Returns : object \u2003 A loaded instance of the specified object. Examples >>> loaded_model = load_estimation ( \"c: \\\\ Documents \\\\ saved_object.p\" )","title":"load_estimation"},{"location":"api_docs/load_estimation/#function","text":"load_estimation ( filename: str )","title":"Function"},{"location":"api_docs/load_estimation/#description","text":"Load a saved estimation model","title":"Description"},{"location":"api_docs/load_estimation/#arguments","text":"filename : str \u2003 The path and file name under which an object was saved.","title":"Arguments"},{"location":"api_docs/load_estimation/#returns","text":"Returns : object \u2003 A loaded instance of the specified object.","title":"Returns"},{"location":"api_docs/load_estimation/#examples","text":">>> loaded_model = load_estimation ( \"c: \\\\ Documents \\\\ saved_object.p\" )","title":"Examples"},{"location":"api_docs/make_data_square/","text":"Function gme. make_data_square ( trade_dataframe=None, imp_var_name: str = \"importer\", exp_var_name: str = \"exporter\", multiple_sectors: bool = False, sector_var_name: str = None, year_var_name: str = \"year\", drop_intratrade: bool = True, year_by_year: bool = True ): Description Expand countries and sectors by adding zero-valued trade flows to make the panel square. Arguments trade_dataframe : pd.DataFrame A DataFrame containing trade values and observation identifiers (importer, exporter, year, and (optionally) sector). imp_var_name : str The name of the column containing importer identifiers. exp_var_name : str The name of the column containing exporter identifiers. multiple_sectors : bool If true, the data is squared for each sector as well. Default is False. sector_var_name : (optional) str The name of the column containing sector identifiers. Must be supplied if squaring for each section (i.e. if multiple_sectors = True). year_var_name : (optional) str The name of the column containing year identifiers. drop_intratrade : bool If True, the returned data will not contain intranational observations in which the importer and exporter are the same country. Default is True. year_by_year : bool If true, the data is squared on a year by year basis, which allows the dimensions to differ each year. For example, it lets the set of countries or sectors around which the data is squared to differ each year. Doing so reduces the likelihood of including countries or sectors with zero trade activity in certain years. ## Returns Returns : Pandas.DataFrame A square dataset. ## Examples >>> raw_data = pd . read_csv ( 'https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv' ) >>> trade_data = raw_data [[ 'importer' , 'exporter' , 'year' , 'trade_value' ]] >>> square_data = make_data_square ( trade_data , imp_var_name = 'importer' , exp_var_name = 'exporter' , multiple_sectors = False , year_var_name = 'year' , drop_intratrade = False , year_by_year = True )","title":"make_data_square"},{"location":"api_docs/make_data_square/#function","text":"gme. make_data_square ( trade_dataframe=None, imp_var_name: str = \"importer\", exp_var_name: str = \"exporter\", multiple_sectors: bool = False, sector_var_name: str = None, year_var_name: str = \"year\", drop_intratrade: bool = True, year_by_year: bool = True ):","title":"Function"},{"location":"api_docs/make_data_square/#description","text":"Expand countries and sectors by adding zero-valued trade flows to make the panel square.","title":"Description"},{"location":"api_docs/make_data_square/#arguments","text":"trade_dataframe : pd.DataFrame A DataFrame containing trade values and observation identifiers (importer, exporter, year, and (optionally) sector). imp_var_name : str The name of the column containing importer identifiers. exp_var_name : str The name of the column containing exporter identifiers. multiple_sectors : bool If true, the data is squared for each sector as well. Default is False. sector_var_name : (optional) str The name of the column containing sector identifiers. Must be supplied if squaring for each section (i.e. if multiple_sectors = True). year_var_name : (optional) str The name of the column containing year identifiers. drop_intratrade : bool If True, the returned data will not contain intranational observations in which the importer and exporter are the same country. Default is True. year_by_year : bool If true, the data is squared on a year by year basis, which allows the dimensions to differ each year. For example, it lets the set of countries or sectors around which the data is squared to differ each year. Doing so reduces the likelihood of including countries or sectors with zero trade activity in certain years. ## Returns Returns : Pandas.DataFrame A square dataset. ## Examples >>> raw_data = pd . read_csv ( 'https://www.usitc.gov/data/gravity/example_trade_and_grav_data_small.csv' ) >>> trade_data = raw_data [[ 'importer' , 'exporter' , 'year' , 'trade_value' ]] >>> square_data = make_data_square ( trade_data , imp_var_name = 'importer' , exp_var_name = 'exporter' , multiple_sectors = False , year_var_name = 'year' , drop_intratrade = False , year_by_year = True )","title":"Arguments"},{"location":"api_docs/save_estimation/","text":"Function save_estimation ( estimation_model, filename: str ) Description Save a serialized copy of an EstimationModel or other object at the specified path. Arguments estimation_model : object The object to be saved. filename : str The path and file name under which to save the object. Returns Returns : None Examples >>> save_estimation ( sample_model , \"c: \\\\ Documents \\\\ saved_object.p\" )","title":"save_estimation"},{"location":"api_docs/save_estimation/#function","text":"save_estimation ( estimation_model, filename: str )","title":"Function"},{"location":"api_docs/save_estimation/#description","text":"Save a serialized copy of an EstimationModel or other object at the specified path.","title":"Description"},{"location":"api_docs/save_estimation/#arguments","text":"estimation_model : object The object to be saved. filename : str The path and file name under which to save the object.","title":"Arguments"},{"location":"api_docs/save_estimation/#returns","text":"Returns : None","title":"Returns"},{"location":"api_docs/save_estimation/#examples","text":">>> save_estimation ( sample_model , \"c: \\\\ Documents \\\\ saved_object.p\" )","title":"Examples"}]}